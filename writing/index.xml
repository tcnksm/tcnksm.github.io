<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Writings on Taichi Nakashima</title>
    <link>https://deeeet.com/writing/</link>
    <description>Recent content in Writings on Taichi Nakashima</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <copyright>Copyright © 2013-2020 Taichi Nakashima</copyright>
    <lastBuildDate>Wed, 28 Jul 2021 08:44:39 +0900</lastBuildDate><atom:link href="https://deeeet.com/writing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Move Fast: How Facebook Builds Softwareを読んだ</title>
      <link>https://deeeet.com/writing/2021/07/28/move-fast/</link>
      <pubDate>Wed, 28 Jul 2021 08:44:39 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2021/07/28/move-fast/</guid>
      <description>Software Engineering DailyのJeff Meyersonが書いた Move Fast: How Facebook Builds Software を読んだ．というかJeff Meyerson本人によって録音されたAudiobookが公開されているのでそちらで聴いた．Podcastのホストをやっていることもあり音声はとても聴きやすく長さも3時間くらい (紙の本だと123ページ) でさっと聴くことができた．
内容としてはFacebookのエンジニア組織について「Product」と「Culture」「Technology」という観点からCase study的に様々な話がまとめられている．「Product」ではWebからMobileへのシフトの話やGoogle+が出てきた時の話が，「Culture」ではマネージメントやBootcampと呼ばれるオンボーディングプログラムついて，「Technology」ではインフラからフロントエンド，リリースエンジニアリングの話やOSSのプロダクトに関して書かれている．
ちなみに作者のJeff Meyerson自身はFacebookの社員であったことはなく，過去にSoftware Engineering Dailyに出演したFacebookのエンジニア達のインタビューなどが基になっており，当事者として書かれた本ではない．
Facebookのエンジニア組織について断片的には見聞きしてきたが，例えば (ボリュームはぜんぜん違うけど) Googleの Software Engineering at Google: Lessons Learned from Programming Over Time のように体系的にまとまっているものを自分は読んだことはなかった．他の同様の本と同じように，ここに書かれてるものをそのまま自分の組織に持ち込めるかというとそんなことは全くないが，考え方やStrategyなどは参考になりそうなものもあり普通に楽しめた．
Spotifyの組織について書いたCompeting With Unicornsを読むといかにSpotifyがAutonomous (自律性) を重視して組織を作っているのかがわかるが，本書を読むといかにFacebookという組織がMove fastを中心に成り立っているのかがわかる．それはプロダクト開発の意思決定だけではなく，GraphQLやReactなど彼らが世の中に出してきたTechnologyや開発プラクティスにも根ざしているように感じた．(もう辞めたけど) TDDのKent BeckがFacebookに入社してテストが全然書かれてなくて考え方そのものが全然違うことに驚いた話などFacebookならではのユニークさに関する話は面白かった．
Move Fast with Jeff Meyerson ではJeff自身がSoftware Enginner Dailyのゲストになって本書の内容や現状のFacebookについて話してるので，内容の補完として聴くのも良いと思う (これは中のひとに聴いたほうがいいんじゃないかと思う質問も多かったが&amp;hellip;)．</description>
    </item>
    
    <item>
      <title>Waypointとは何か</title>
      <link>https://deeeet.com/writing/2020/10/16/waypoint/</link>
      <pubDate>Fri, 16 Oct 2020 12:16:20 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/10/16/waypoint/</guid>
      <description>Hashicorpの2020年冬の新作 Waypoint (リリースブログ)に関してドキュメントなどをざっと眺めてみたので最初の印象をちょっと書いてみる．ちゃんとしたレビューは @copyconstruct の記事 Waypoint とか読むのが良い．毎度のことながらドキュメントやガイドはかなりちゃんとしたのがあるので使い方とかはそっちを読んだ方がいい．以下に書くのはざっくりした個人の感想（ちなみにもう一つのBoundaryに関してはZero Touch Productionとは何か に軽く書いた）．
What is Waypoint Waypointは，KubernetesやNomad，Amazon ECS，Google Cloud RunといったPlatformの上にBuild，DeployとReleaseの一貫したWorkflowを実現するツール．使ってる言語やそのパッケージ方法や，配下のPlatformへのデプロイ方法が何であろうが，共通の設定ファイルwaypoint.hclとwaypointというコマンドで同じWorkflowを提供する．PaaSというよりは配下のインフラをラップしてそれらに関係なくPaaSのUI/UXを実現しようという感じかな．
Why Waypoint Developers just want to deploy Herokuを始めとするPaaSはアプリケーションのデプロイ体験としてはとても良いものを提供していた．しかし，世の中はKubernetesを始めとするCaaSが出てきたりで複雑な方向に向かった．もちろんそれで得られたメリットは多く，PaaSにはない柔軟性をもたらしたが，一方でシンプルにアプリケーションをデプロイするという体験はなくなってしまった (e.g., Kubernetes YAMLの壁)．
またPackerでVMでやってたのが，DockerでKubernetesになり，また最近はCloud RunでServerlessで専用のCLIが出てきてと，インフラやアプリケーションのデプロイのトレンドはどんどん変わっていく．がその度に新しいCLIや設定ファイルが出てきてそれに合わせてWorkflowをアップデートして〜とやってく必要がある．今はKubernetesとかServerlessとか言ってるけどどうせ5年もたったら違うことを言ってるはずだしね (Kubernetes自体は残っていても抽象化されてそれ自体は見えなくなってるいるはず)．
Waypointはこの辺りの課題を解決することを目的にしてる．DockerfileやらYAMLやらを書くのではなくて必要最低限のHCLでそれらを生成して，どの基盤であってもwaypoint CLIで同じWorkflowでってのをできるようにする．waypointで統一しておけば次の時代が来ても対応できるという狙い．配下が変わってもBuild，Deploy，ReleaseというWorkflow自体は大きくは変わらないという前提があるけど（それは正しいし，それ自体も変えていける感じするし）．
Otto この辺のコンセプトを聞くと数年前にリリースされて死んだOttoを思い出す（懐かしい）．Ottoはまさに同じような問題を解こうとしてたから．HNでmitchellh自身が回答してるけど以下のような違いがある:
 Waypiontは配下のインフラ (e.g., Kubernetes) 自体は管理しない．Ottoはその辺もTerraform使って担おうとしてた WaypiontはBuild，Deploy，ReleaseというWorkflowをPluginを使って拡張できるようにしている．OttoはHashicorpの生態系であるVagrant，Terraform，Vault，Consulをつなぎ合わせるという目的が強かった  Workflow Waypointが言ってるBuild，Deploy，ReleaseというWorkflowがそれぞれ何かというと:
 Build: アプリケーションのソースコードからBuildpackなりDockerなりでデプロイ単位になるArtifactを作ること Deploy: Buildで作ったArtifactをデプロイすること Release: デプロイした環境に実際にトラフィックを流し始めること  このWorkflowで良いなあと思ったのはDeployとReleaseをちゃんと分けていること．Deploy != Release にも書かれているけど今はDeployとReleaseは分けて考えるのは当たり前になってきており，これはOttoの時代には一般的ではなかった．現状はないけど今後PluginなどでCanaryやBlue-greenといったRelease戦略が実装されていくんじゃないかと思う（Service meshを抽象化してね）．
Next step 今後どうなりそうかに関してはRoadmap にいろいろ書いてある．良さそうと思ったのはService Brokerの仕組みを使ってDBやQueueなども対応していくこと，App promotionで異なる環境に対応していくことなどなど．
Twitterで直接mitchellhにも聞いたけどKubernetes manifestのカスタマイズをどうしてくのかは気になるところでもある．現状の方法はHelmと同じTemplate方式になっているので，変更した値に対して毎回変数を定義していく必要がある．つまりコアのPlugin自体を変更しないといけない．これはHelmでも起こったが，各社の要望に全て答えるのは無理で最終的にはForkするか，新たなPluginを自分たちで作るしかなくなる（それを期待してるとは思うが）．抽象化で一番むずかしいのはこの辺をどう対応していくかなのでその辺は引き続き要チェックではある．</description>
    </item>
    
    <item>
      <title>Zero Touch Productionとは何か</title>
      <link>https://deeeet.com/writing/2020/10/15/zero-touch-production/</link>
      <pubDate>Thu, 15 Oct 2020 09:14:25 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/10/15/zero-touch-production/</guid>
      <description>GoogleのSREとSecurityによるBuilding Secure Reliable Systems という本の中で「Zero Touch Production (ZTP) 」という考え方が紹介されていた．これはインフラの権限管理やインフラの構築そのものの指針となる概念であり，自分がそうあるべきだとずっと思ってきた考え方でもある．これはどのような考え方なのか?をこれまでの歴史を踏まえて具体的なツールや事例とともにまとめておく．
Zero Touch Production Building Secure Reliable Systems においてZero Touch Production (ZTP) は以下のように定義されている．
The SRE organization at Google is working to build upon the concept of least privilege through automation, with the goal of moving to what we call Zero Touch interfaces. The specific goal of these interfaces—like Zero Touch Production (ZTP), described in Chapter 3, and Zero Touch Networking (ZTN)—is to make Google safer and reduce outages by removing direct human access to production roles.</description>
    </item>
    
    <item>
      <title>社内PlatformチームのProduct Management</title>
      <link>https://deeeet.com/writing/2020/10/07/internal-platform-product-management/</link>
      <pubDate>Wed, 07 Oct 2020 08:43:26 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/10/07/internal-platform-product-management/</guid>
      <description>現職においてPlatform チーム（社内基盤チーム）として働き始めて2年近くがたった．このチームにおいて自分はTech Leadをメインに努めてきたが，同時にPlatformの「どのような機能を」「どのような優先度で」作るか? を決めるProduct Manager的な役割も果たしてきた（ちなみにTech Leadに関してはメルカリのテックリードが学んだ、HowよりWhyを重視することが大切なわけ で少し話した）．これは何度も失敗しながら悪戦苦闘しつつやってきたが自分たちなりのフレームワークをつくり実際に回すことができている．
未だに試行錯誤しているのでここで書いていることが正解だとは思っていないが，今後同じようにPlatformチーム的なことを始めるひとに向けて現状自分たちがどのようにやっているのかについて簡単にまとめておく（他の会社がどのようにやってるのかも聞きたいのでもし同じようなことをやってるひとがいたら会話しましょう!）．
本来なら会社のTech Blogとかに書くべき内容だが，技術的なことであれば自信を持って書けてもProduct Managementに関しては正直自信を持って書くことができない．上手くいったこともあるけどそうじゃないことのほうが多いと思う．動き始めていたプロジェクトを止めて大きく方向転換をすることになりチームに迷惑をかけてしまったこともある．今でもこれで良いのか?を試行錯誤している．そういうこともあり個人ブログで書くことにした．
以下ではまず大まかなアイディアについて簡単に紹介しその後「何を」「どのような優先度で」を具体的にどのように決めているのか?についてまとめる．
Platform as Product これはすでに各所で言われていることだがInternal Platformをやるにあたって一番大切なマインドセットはPaltformを単なる「ツール」としてではなくて「プロダクト」として考えることだと思う．つまりPlatformを利用する社内の開発者をCustomerとしてみて，Customerに対してPlatformという「プロダクト」を提供していると考える．そしてそのCustomerのためにより良い機能の追加や改善を行う．例えば以下の記事が参考になると思う．
 Applying product management to internal platforms Product for Internal Platforms Product management in infrastructure engineering. Code less, engineer more - Increment: Teams  もちろんInternal PlatformはCloud Providerのように会社として外部に提供しているプロダクトではない．会社としてはメインのビジネスがある．その場合にPlatform Teamはどのような立ち位置になるかというと，直接的に会社のCustomerに価値を提供するのではなく，自分たちにとってのCustomerである開発者に価値を提供することで，開発者がよりよいサービスを開発できるようになり，間接的に会社のCustomerに価値を提供すると考える（もちろんインフラのReliabilityといった直接的な価値もあるが）．
こう考えると一般的なProduct managementとやるべきことの差は大きくはない．社内が対象であることによるコミュニケーションの違いだったり，対象がPlatformという技術に特化した領域であるくらいの違いしかない．サービスを開発するにあたって開発者がどのような課題を抱えているのかを見つけそれらに優先度をつけて順番に実行していくというのが大まかな流れになる．
Framework Product managementの1つのゴールは具体的なRelease Sprintとして具体的にタスクにまで落とし込むことだと思う．ここではその大まかな流れについて紹介する．以下はそれを簡単に図にしたものである．
まずチームとしてのMissionを持つ．次にMissionをもとに長期的なRoadmapを作る．そしてRoadmapをもとにRelease Sprintのタスクに落とし込む．以下ではこれらを具体的に紹介する．
Vision &amp;amp; Mission まず最初はPlatformチームとしてのVisionとMissionを決める．What is company vision? A picture of a better place の定義に従うと，Visionとはどこへ向かうのか（Where）を示し，Missionはそこへ向かうために何をするか（What）を示す．会社の中のチームの場合はもちろん会社のVisionにアラインしつつそこに向かうためにチームとして何ができるのかを考える．特にMissionの定義は責任の範囲を明確にしたり，無限にある問題空間を限定することにもつながる．
VisionとMissionを決めるにあたっては以下が参考になった．
 re:Work - Guide: Set and communicate a team vision Writing strategies and visions.</description>
    </item>
    
    <item>
      <title>Infrastructure as Dataとは何か</title>
      <link>https://deeeet.com/writing/2020/05/11/infrastructure-as-data/</link>
      <pubDate>Mon, 11 May 2020 06:18:53 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/05/11/infrastructure-as-data/</guid>
      <description>最近GCPから登場したKubernetes YAMLのPackage managerであるKptは「Infrastructure as Data（Configuration as Data）」という考えかたを基礎としてそれを推し進めようとしている．それ以外にもKubernetesのEcosystemには（明示はされていなくても）この考え方が中心にある．Infrastructure as Codeとは何が違うのかなど歴史を振り返りつつまとめてみる．
（指針はBorg, Omega, and Kubernetesという論文にあるが「Infrastrcuture as Data（Configuration as Data）」という言葉を明確に定義した文章はない．この記事はReferencesに挙げるいくつかのPodcastにおける@kelseyhightowerの発言や，それに反応する@bgrant0607のTweetなどを中心に自分なりに考えをまとめているだけで，概念は変わらずとも名前などは今後変わるかもしれない）
Infrastructure as Script Infrastructure as Codeという言葉が登場する前はインフラのセットアップは手順書に基づくManual operation行われていた．今でこそCloudが中心となり数百のMachineを扱うのは当たり前になってきたが，オンプレ時代では今ほど大規模でもなく1つのPhysical machineを使い続けることも多く（長く使うほどコストパフォーマンスが高い）インフラはStaticなものだった．そのためManual operationでも十分といえば十分だったことも多かったと思う．もちろん専用のSoftwareの登場を待つことなくShell scriptなどで自動化を早くから進めてInfrastructure as a scriptはやられていたと思う．
Infrastructure as Code Hardware virtualizationの登場により（Virtual）Machineを立てたり消したりすることが容易になり，またそれをサービスとして提供するAWSやGCPといったCloud providerが使った分だけ課金するPay-as-you-goモデルを採用したことで（Auto-scalingなどにより）必要に応じて必要なMachineをProvisioningするようになり，インフラはよりDynamicなものになった．DynamicになったことによりインフラのセットアップやアプリケーションのDeployのReproducibility（再現性）も求められるようになった．また扱うトラフィックも大規模になりそれをさばくためのインフラも大規模になった．
これらの背景から登場したのがPuppetやChef，AnsibleといったSoftwareを中心としたInfrastructure as Code（IaC）である．IaCはコード（e.g., DSL）によってインフラのセットアップを自動化する方法である．IaCにより，Error-proneなManual operationはなくなり，インフラのReproducibilityも高くなった．これだけではなくIaCはソフトウェア開発の方法論をインフラ管理に持ち込んだ．つまり，GitによりVersion管理をし，Github上でPull Requestによる変更を行い，CI/CDを可能にした．今でこそ当たり前だがこれらを可能にしたことは大きい．
TerraformもIaCに属するがTerrafromはVMやManaged DBのセットアップといったCloud providerの提供するResourceの管理に特化しており上に挙げたSoftwareとはフォーカスしてるエリアが異なる（Chefとかでもできるとは思うが&amp;hellip;）．ChefやPuppetは立ち上げられたVMをProvisioningするが，Terraformはその前段であるMachineの立ち上げることにフォーカスしてるといった感じ．この後のコンテナ時代により自分はChefやAnsibleは使わなくなり（Packerとかでコンテナでも利用できないかとか考えてた時期もあったがw），自分の中ではIaC = Terraformになっている．
また従来のShell scriptによる自動化との大きな違いはDeclarativeであることだろう．ChefやTerraformのDSLでの記述としては「こうあるべき」というDesiredな状態を書くようになり，一連の動作をにImperativeに記述するShell scriptとは大きく異なる．ここで登場したDeclarative configurationは今日も使われておりInfrastructure as Dataにも繋がる．
Immutable Infrastructure VMの立ち上げが容易になったとは言え，当時はセットアップが完了しアプリケーションが動いているMachineに新たにChefのRecipeを流し込んでSoftwareの更新を行うことは普通だった．そのためChef・Ansible時代によく言われたのは「Idemponence（冪等性）を満たせ」だった．つまりChefのRecipeを書く場合は何度も実行しても結果が同じようになるように書けという意味だ．巨大なChefのCookbookを運用した人はわかると思うが正直それは難しかった．またChefやAnsibleは1つMachineのセットアップには強いが複数のMachineのセットアップには弱く，継続的な実行が行われなとConfiguration Driftが避けられないという問題があった（Continuous Deliveryをしてないと流し漏れは発生する…自動化してても人がManualで何かを変えてしまうことはある）．
アプリケーションのDeployに関しても動いているMachineに直接変更を流し込むのは普通であり，そこにConfiguration Driftが加わることで当時のDeployはDeterministic（決定論的）ではなかった．つまり同じで同じコードをデプロイしてるのにMachineによって結果が異なることがあった．デプロイが失敗したときに確実にそれをRollbackできる保証もなかった．
この問題を解決するために出てきたのがImmutable Infrastructureである．Immutable infrastructureとは動いてるMachineには変更を加えず設定を変更したり，新しいバージョンのアプリケーションをDeployするときはVMごと作り直すという考え方である．これによりDeployはDeterministicになった．Deterministicになることで，Rollbackが容易になる = 失敗してもすぐに戻せる = 失敗を恐れずリリースを速く行うことができる，ということが可能になった．今ではImmutableになってないと怖くてデプロイはできない．
ここで登場してきたのがDockerを中心としたContainer技術である．もちろんPackerなどを使うことでアプリケーションコードごとEC2やGCEのVMイメージをつくりそれをDeployすることでImmutable infrastructureを実現することは可能である．しかし，VMと比較して起動の早さ，Registryによる配布の容易さ，そして何よりもUtilizationの高さという利点によって，Immutable DeployとしてContainerが利用されることが多くなってきた（その後のServerlessまで考えるとインフラの進化はUtilizationの改善と紐付けて考えられる）．</description>
    </item>
    
    <item>
      <title>Competing With Unicornsを読んだ</title>
      <link>https://deeeet.com/writing/2020/03/05/competing-with-unicorns/</link>
      <pubDate>Thu, 05 Mar 2020 10:53:25 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/03/05/competing-with-unicorns/</guid>
      <description>Competing with Unicorns: How the World’s Best Companies Ship Software and Work Differently
The Agile Samuraiの作者でありSpotifyにおいてAgile CoachとEngineerを努めたJonathan Rasmussonによる本．本書はUnicornもしくはTech companyがどのようにチームをつくり，組織をスケールさせ，文化を作っているのかについて書いている．タイトルにUnicornとあり複数の企業を扱ってるように見えるが，基本的には作者のSpotifyにおける体験が基になっておりSpotifyの話が中心になっている．
なぜMicroservicesか?ではMicroservicesの最終ゴールは組織にあると書いた．これは共通の見解（のはず）である一方で，Microservicesにおいてどのような組織構造・チーム構成を作っていくのが良いのかについて具体的な例を基に書かれたものはあまり見たことがない．自分は組織作りにまで関われているわけではないし，専門でもないが，これまでいくかの記事，発表を見てきた中でもSpotifyはこれを非常にうまくやっているように感じていた．
Spotifyがどのようなチームや組織を作っているかについてはScaling Agile @ Spotifyという2012年に公開されたブログが一番有名であると思う．そこではメインのコンセプトであるSquadやTribe，Guildという概念が紹介されている．また2019年にはその組織やエンジニアリング文化について紹介する20分の動画も公開されており（Spotify Engineering Culture part1・part2）2012年からのアップデートがわかる．もう一つ自分が感銘を受けたのがBreaking Hierarchy - How Spotify Enables Engineer Decision MakingというQCon New York 2019の発表で，そこではいかに組織のヒエラルキーをぶっ壊してエンジニアやチームに意思決定を促しているかについて紹介されている．
Competing with Unicornはこれらをより詳細にまとめた本になる．SquadやTribeによる組織構造や，各SquadのAutonomous（自律性）を保ちつつCompany betsによるAlignmentの方法，Productivityへの投資やこのような組織におけるLeadershipのあり方などについて詳しく解説されている．
Squad and Tribe SpotifyのようなUnicornではいかに組織を拡大していくかが大きな課題になる．スケールしつつもStartupのような俊敏さを失わないようにするのはとても難しい．Spotifyはこの課題を解決するためにSquad，Tribe，Chapter，Guildという組織構造を取り入れている．
まず一番基礎となる単位がSquadである．Squadは8人以下のメンバーで構成され，Mission vs. Projectで詳しく紹介するようにそれぞれにMissionが与えられMini-startupのように動けるように設計されている．Squadは自己組織化されており，何をどのようにつくるかという意思決定や，開発からリリース，運用までなるべく自分たちで完結できるようになっている．Squadが最も重要な単位でありその他のTribeやChapterはこれを補助するためだけに存在してる（この辺はTeam topologiesのTeam first thinkingと同じ）．SquadはCVとかにも書かれているし結構一般的に使われているっぽい．
同様のMissionをもったSquadをまとめたのがTribeである．例えば，App Integration Squad（Facebookなどのアプリケーションとの連携を行なう）やHome Consumer Electronics Squad（家電との連携を行なう）などをまとめてPartner and Platform Eeperience Tribeを構成する．TribeはDunbar’s Numberを基に40人から150人で構成され，Squadと同様にMissionを持つ．Tribeの利点は同様の課題を持ったSquadをまとめることでアイディアやコードなどを共有しやすくなることにある．自律性を保つためにSquadは協力はし合うが依存は少なくしている，Tribe間は更に依存はなくしている．
Tribe内部において特定の技術領域などでまとまったのがChapterである．例えばQA ChapterやWeb Engineer Chapterなどがある．それぞれのChapterにはChapter leadがおり採用から給与の決定，キャリアの開発などを担う．Chapterの利点はコミュニケーションを形成して最新の技術やよりよいプラクティスなどをやり取りできるようになる部分である．
Chapterを複数Tribe間にまで拡大したのがGuildである．例えばiOS Guildなどがある．ChapterとGuildの違いはGuildが基本的には任意のコミュニティであることである．iOS Guildに入るのにiOS開発者である必要はないしGuildの集まりに毎回必ず出席する必要もない．</description>
    </item>
    
    <item>
      <title>Team Topologiesを読んだ</title>
      <link>https://deeeet.com/writing/2020/02/06/team-topologies/</link>
      <pubDate>Thu, 06 Feb 2020 10:05:05 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/02/06/team-topologies/</guid>
      <description>https://teamtopologies.com/
DevOps consultantとして技術と組織の両面からDevOpsの支援を行なってるMatthew SkeltonとManuel Paisによる本．Consultant本は大体中身が薄く感じることが多くなり手に取ることは少なくなってきたが，各所で見かけたり，2人によるDevOpsにおけるチームのあり方のパターンをまとめたWhat Team Structure is Right for DevOps to Flourish?が良かったので読んでみた．
本書はDevOpsの視点から高速なDeliveryを実現するためにどのようなチームや組織を作るべきかについてまとめている．個人ではなくチームをDeliveryの最も重要な単位と考え（Team first-thinking），チームが最大限にパフォーマンスを発揮するために，チームの人数やその責任の範囲のデザインの仕方（Team API）から，基本的なチームタイプ（Fundamental team topology）やそのチーム間のコミュニケーションパターン（Team interaction mode）とそれをどのようの変化させていくか（Organizational sensing・Topology evolution）が紹介されている．また理論だけではなくてCase studyとして各社の事例も各章で紹介されている．
本書は大きく3つのPartからなる．Part1ではConwayの法則を再考しつつ現実の組織がいかにアーキテクチャやコミュニケーションパターンが考慮されていないか?という本書で解こうとしている問題がまとめられている．以降のPartはその解法としてPart2は基本的なチームタイプについてPart3はそのチームのコミュニケーションのパターンとそれをいかに進化させていくかについて紹介される．以下ではこれらを簡単にまとめておく．
Team-first Thinking 本書に限らず多くのところで述べられているように「小さく長期的に安定した」なチームを作ることは非常に大切である（例えば How Twilio scaled its engineering structureやHow to build a startup engineering teamなど）
「小さな」は本書では具体的には5-9人である．この人数の根拠としてDunbar&amp;rsquo;s Numberを使っている．Dunbar&amp;rsquo;s NumberはDunbarが提唱した人間が安定的な社会関係を維持できるとされる人数の認知的な上限である．簡単に言うと何人までは互いに信頼でき，何人までは覚えていられるか？という人数のラインを示している．これをMicroservicesの組織の形態でよく使われてるSpotifyのチームの形（Scaling Agile @ Spotify）に落とし込むとSquadが8-10人，Tribeが50-150人，Divisionが150-人となりチームの粒度やそのグルーピングの限度の指標に利用できる．またコミュニケーションのリンクの数からも「小さな」が良いことは理解できる（以下はチームの人数とそこでのコミュニケーションのパスの数 ．12人でも既に66もある．Two-Pizza Teams: The Science Behind Jeff Bezos&#39; Rule）
「長期的に安定」するべきはのは，チームとは互いに信頼し合い働き方や考え方を一致させて初めて高いパフォーマンスを出せる（Tuckman&amp;rsquo;s stages of group development）ものであり，それには時間がかかるためである．プロジェクトの度にチームがコロコロ変わっていてはチームとしてのゲル化は進まずパフォーマンスも上がらない．逆に高いパフォーマンスのチームを分けたりするとProductivityは一気に落ちる．
「長期で安定した」チームを持つことで初めてOwnershipについて考えることができるようになる．Ownershipとは「Continuity of care」を持つことであり，チームはそれによって段階的に長期にものを考えることができるようになる．目の前の問題を解決する段階から，数ヶ月先のこと考えて実験を行う段階へ向かうことができるようになる．NetflixがどのようにOwnershipを育てているかについて語っているMistakes and Discoveries While Cultivating OwnershipではOwnershipのLevelを定義しており，目指すべきところ（かつLeadershipがメンバーに持たせるのは）は「Vision」であるとしている．「Vision」はまさに長期で目指すべきところを考えることに他ならない（これをさらに組織全体でTribeレベルでもできてるのがSpotify Breaking Hierarchy - How Spotify Enables Engineer Decision Making ）．長期でVisionを持つことは直前の問題解決の意思決定にも直結する（例えば3ヶ月後にxをする予定だから今はyに時間をかけずに単純なzという手法を使おうと思うことができる）．</description>
    </item>
    
    <item>
      <title>2019年振り返り</title>
      <link>https://deeeet.com/writing/2019/12/31/2019/</link>
      <pubDate>Tue, 31 Dec 2019 23:22:13 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2019/12/31/2019/</guid>
      <description>2019年のアウトプットとインプットを簡単に振り返っておく．
Working 業務でのチームとしてのアウトプットはMercari Microservices Platformの進捗（2019年）にまとめた．前年に引き続きPlatformの開発と運用を続けている．
昨年はAPI gatewayの開発など自分で手を動かすことが多かったが，今年は自分が具体的なプロジェクトを持ち自ら手を動かすことは意識的に少なくし，Tech leadとしてチームのアウトプットをどのように最大化にするか?ということを常に考えていた．技術的な視点や意思決定も時間的に影響範囲的により広く見るように意識し始めた（インプットも組織やチームに関連するものが多くなった）．見えやすいアウトプットは少ないが，プロジェクトを進めつつ，これまで曖昧だったPlatformのMissionは何かを明確に定義し，チームが拡大しても皆が同じ方向を向けるようにPlatformとしてどうなるべきか?というVisionを定義するなどした（まだうまく書けてないがチームのPrincipleとPracticeも書き始めている）．
社内基盤であってもそれをProductとして見てそれをいかに成長させるかを考えるProduct manager的な動きもしてきた．具体的にはPlatformとしてどのような機能を提供するべきかを調査し（社内向けのDeveloper surveyを行ったり，SREやSecurity，Architectチームからの意見を聞き入れたり），それらの意見と自分らがやりたいことから優先度を決め，次の半年何ができているべきか? 抽象度を上げて1-3年後にはどうあるべきか?を考えてロードマップをつくり，さらにそれをSprintベースのEpicに落とし込みそれを実行する，といったことをやってきた．うまく回りつつあるがまだ課題もあるので引き続き改善していきたい．
PlatformのMissionは定義したがチームのResponsibilityはより大きくなり1チームとしては抱える問題が大きくなりすぎてきた．そのためチームのCognitive loadが高くなりタスクの優先度を決めるのが難しくなり開発の速度も遅くなりつつある&amp;hellip;まだベストの解はないが来年は専門性に特化した形でチームを分割する，またTech leadというRoleを育てかつ移譲していくことを来年は考えていきたいと思っている．
Output Speaking 今年は以下の対外発表をした．
 開発者向けの基盤をつくる at Hackers Champloo How We Structure Our Work At Mercari Microservices Platform Team Why Microservices? at Mercari Bold Challenge  Hackers Champlooはいつか行ってみたいと思っていたイベントなのでそこで登壇できて良かった．内容も当時自分が考えていたことを詰め込むことができて良かった．
対外発表はやりすぎると業務や私生活に影響が出る（昨年はやりすぎてしんどかった）ので今年くらいの頻度（年3回）が理想的でこれを継続していきたい．今年は会社のイベントの登壇がメインだったのでもっと社外のイベントに積極的に出ていきたい．今年は海外Conferenceでの登壇はできなかったので引き続き挑戦していく．
Writing 今年はどちらかというとインプットに集中していたこともありブログはなぜMicroservicesか?くらいしか書けなかった．来年はもう少しバランスを取れるようにしていきたい（四半期に1-2本のペースで書いていく）．
Learning Conferences 今年は以下の海外カンファレンスに行かせてもらった．
 Google Cloud Next 19 SRECon 19 Asia/Pacific  SREConは初めて行ったがとても良かった．Kubernetesといった特定の技術やクラウドによらない話が多く，組織やコミュニケーションといったテーマに結構な時間が割かれているのが良かった（結局一番大切な話でもあるので）．Conferenceの規模も500人程度でそこまで大きくなく参加者の顔が見え議論しやすいのも良かった（チームのメンバーがSRECon EUに行っていたがそちらもこれまで行ったか中でも最高だったと言っていた）．
Books 今年読んだ技術書籍は別途まとめた．その中でも良かったのはWill Larsonの「An Elegant Puzzle: Systems of Engineering Management」 ．Will LarsonはInfrastructure TeamのEMであり自分の業務とも近くチームに関することから組織の話までとても影響を受けたと思う．BasecampのProduct StrategyのRyan SingerがProduct managementに関してまとめた本であるShape Upもとても面白かった．より技術的な本だとBrendan Greggの新作BPF Performance Toolsも良かった．</description>
    </item>
    
    <item>
      <title>今年読んだ技術書籍（2019年）</title>
      <link>https://deeeet.com/writing/2019/12/05/2019-book/</link>
      <pubDate>Thu, 05 Dec 2019 07:38:22 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2019/12/05/2019-book/</guid>
      <description>今年読んだ技術書籍やレポートなどをざっくりまとめてる．Infrastructure Engineer・Platfomerとして日々の業務に直結するものから1年くらいかけてやっていきたいと思っていることなどを中心に．
Kubernetes 業務ではメインにKubernetesを使っているのでKubernetesに関わる書籍は発売されれば大体目を通すようにしている．
今年発売されたので良かったのはProgramming Kubernetes．この本はCRDやOperatorによってKubernetes nativeなアプリケーションを構築することにフォーカスしている．昨年のJapanContainerDaysでのMicroservices Platform on Kubernetes at Mercariでも話したようにKubernetesを使う大きな理由の1つはその拡張性にある．Kubebuilderなど便利なフレームワークはあれどやはり中身をちゃんと理解するのが重要なのでそれを体系的に扱ってる本は重要．
Brendan BurnsのDesigning Distributed SystemsはContainerを使った分散システムの様々なBuilding BlockやPattern（例えばSidecarパターンやAmbassadorsパターンなど）を紹介している本．Kubernetes上で作るサービスのArchitectをやっていきたい人とかは一度読む良いと思う．分散システムはデザインして構築してデバッグするのが複雑で難しいが「適切にStructuredすればよりReliableであり正しくArchitectedすればよりScalableな組織に導くことができる」というパンチラインとても好き．自分がPlatfomerとしてやりたいことを端的に表している．
先日の登壇でも話したように今自分がメインに作っている基盤にはメルカリだけではなくメルペイも動いておりセキュリティはとても重要な要素になっている．Kubernetes界隈はコンテナという新しいパラダイムのためにセキュリティの進化もめざましくて新しいツールなどを追うのはなかなか大変でHotな分野でもある．AquaのLiz RiceらによるKubernetes SecurityはKubernetesのセキュリティに特化した本（Report）．若干古くなってしまったが基礎的な部分が抑えられているのでざっと目を通すのはおすすめ．
Kubernetesの本で読むべき本を聞かれたときには自分はDeveloperなら[Kubernetes: Up and Running](Kubernetes: Up and Running)でCluster AdminならManaging Kubernetesを勧める．でその[Kubernetes: Up and Running](Kubernetes: Up and Running)の2nd Editionが今年出た！1st Editionからの差分はResourceの解説としてRBACやIngressが，PracticeとしてYAMLをGitでどのように管理するかといった話が，Extending KubernetesとしてCRDやAdmission webhooの紹介が追加されていてよりよい感じになっていた．
SRE 自分は今はSREではないがインフラを扱うPlatformerとしてGoogle SREの考え方やプラクティスは常に参考にしている（その辺りの話をSRE NEXTで話すので皆来てくれ!!）．
Site Reliability EngineeringとThe Site Reliability Workbookに続いてGoogleのSREとSecurity TeamからBuilding Secure Reliable Systemsという本が出た．SecurityとReliabilityは両方とても重要な要素だが同時に満たすのは難しい．それぞれに想定しなければならないリスクが異なる（自然に起こる悪意のないマシン障害 vs. 悪意のある攻撃者）．Reliabilityを優先すると脆弱になるしセキュリティを厳しくしすぎるとReliabilityに支障をきたす．じゃあそれをGoogleはどうしてるのか扱ったのが本書．まだEarly Releaseで3章までしか出てない（しかも無料で公開されてる!!）がめちゃくちゃ面白かったので全部出るのが楽しみ．
もう一つ読んだのがCase Studies in Infrastructure Change Managementという本（Report）．GFSからColossusへの移行（2年）とDisklessへの移行（6年!）というGoogles社内の大規模なインフラ移行の解説とそこから得られた学びについてそれに関わったSREのTPM（Technical Program Manager）が書いたReport．業務でも今年半年くらいかけてKuberbetes Clusterの移行という大きなインフラ移行プロジェクトをやっていたのでとてもタイムリーな内容だった．自分たちが上手くやれてるところもあればもっとうまくやれたかなーということもありこのReportに書かれてる事は規模は違えど分かるわーっての多かったし次に活かせそうな学びあって良かった．
他にSREというかインフラ関連だとCindy SridharanのDistributed Systems ObservabilityとかDaniel Stenberg（Curlの作者）によるHTTP/3の解説本であるHTTP/3 explainedとかを読んだ．</description>
    </item>
    
    <item>
      <title>なぜMicroservicesか?</title>
      <link>https://deeeet.com/writing/2019/05/20/why-microservices/</link>
      <pubDate>Mon, 20 May 2019 13:10:19 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2019/05/20/why-microservices/</guid>
      <description>現職においてMonolithアーキテクチャからMicroservicesアーキテクチャへの移行とその基盤の構築に関わって2年近くが経った．未だ道半ばであるがこれまでの経験や日々のインプットをもとにいろいろ書いておこうという気持ちになった．本記事ではそもそもMicroservicesアーキテクチャとは何かを整理し，なぜやるべきか?・なぜ避けるべきかを整理する．
Microservices? Microservicesアーキテクチャとは「Single purpose，High cohesion，そしてLoosly Couploedなサービスを組み合わせてシステムを構築する」アーキテクチャ手法である．それぞれの原則をまとめると以下のようになる．
 Single purpose: 一つのことに集中しておりそれをうまくやること Loose coupling: サービスは依存するサービスについて最小限のことを知っていること．あるサービスの変更に他のサービスの変更が必要でないこと． High cohesion: それぞれのサービスが関連する振る舞いやデータをカプセル化していること. ある機能を作るときに全ての変更が一つのサービスにまとまっていること．  Microservice Architecture at Medium
Microservicesアーキテクチャをモデル化するときはこれら3つを「全て」満たす必要がある．これによってMicroservicesアーキテクチャの利点を最大限に活かすことができる．一つでも欠けると崩壊する．
 Single purposeを満たさないとそれぞれのサービスは多くのことをやることになる．つまり複数のMonolithが存在することになる Loose couplingを満たさないと一つのサービスの変更が他のサービスに影響を与えることになる．そのため（Microservices化のメリットである）素早く安全にリリースをすることができなくなる．密結合するとデータの不整合やデータロストなどが起こる High cohesionを満たさないと分散Monolithになる．つまり一つの機能の開発のために複数のサービスを変更しないといけなくなる  コードの行数が少ないから・細かなタスクを扱うからMicroserviceではない．Microservicesアーキテクチャのゴールはできる限り多くの小さなサービスを持たないことである．また新しいテクノロジーを使っているからMicroserviceではない．Kubernetes上のコンテナとして動いているからMicroserviceではない．
Why Microservices? 「Microservicesは組織論」と言われるようにMicroservicesアーキテクチャの究極的な成果物は新たな組織図である．新たなアーキテクチャに基づく新たなチームの編成，組織の再構成を狙うのが大きな目的である（逆コンウェイの戦略，Inverse Conway Maneuverなどと呼ばれる）．
What We Got Wrong: Lessons from the Birth of Microservices
組織を再編する大きなモチベーションはサービス成長に伴う組織の拡大（エンジニアの増加）に起因することが多い．組織の拡大はそのパフォーマンスの低下を引き起こす可能性がある．Accelerateは2013年から2017年の4年間を通してスタートアップを含む2000以上の企業から「いかに組織のパフォーマンスを加速させるか」という視点で聞き取り調査を行った本である．この調査結果の一つに以下のグラフがある．
このグラフはエンジニアの数とそのパフォーマンスを組織の違いによってマッピングしたものである．グラフの縦軸はここではパフォーマンスの指標として一日あたりのエンジニアあたりのデプロイ数を示している．この調査結果から，組織を拡大しても（エンジニアが増えても）パフォーマンスは必ずしも高まるわけではなくむしろその低下をもたらすことがあることがわかる．一方で指数関数的にそのパフォーマンスが高まった組織があることもわかる．パフォーマンスに起因する要素は様々だがアーキテクチャとチーム編成が与える影響は大きい．このアーキテクチャとして近年デファクトになりつつあるのがMicroservicesアーキテクチャである．
以下はMicroservicesアーキテクチャが可能にすることを端的に表した図である．
The microservice architecture is a means to an end: enabling continuous delivery/deployment
Microservicesアーキテクチャによって可能になるのは小さく自立・独立したCross functionalなチームを各サービスに配置することである．そしてそのチームに対して適切な権限を与えて構成する「組織」とMicroservices「アーキテクチャ」が可能にするのはContinuous Deliveryという「プロセス」である．この「プロセス」が「組織としてのパフォーマンスを最大化すること」を可能にする．以下のAirbnbのKubeCon 2018 North AmericaでのKeynote Developing Kubernetes Services at Airbnb Scaleのスライドがとてもシンプルにこれを伝えていた．</description>
    </item>
    
    <item>
      <title>2018年振り返り</title>
      <link>https://deeeet.com/writing/2018/12/29/2018/</link>
      <pubDate>Sat, 29 Dec 2018 23:55:16 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/12/29/2018/</guid>
      <description>2018年のアウトプットとインプットを簡単に振り返っておく．
Work 仕事で取り組んだことは全て以下のMercari Tech Conference 2018で発表させてもらった．前年から引き続きMercariのMicroservices化に向けた基盤の構築をしている．
MTC2018 - Microservices Platform at Mercari
大きかったのは&amp;ldquo;API GatewayによるMicroservices化&amp;rdquo;で紹介したAPI gatewayのリリースそして@terryの&amp;ldquo;Mercari API: from Monolithic to Microservices&amp;rdquo;や@morikuniさんの&amp;ldquo;Listing Service: From Monolith to Microservices&amp;rdquo;で紹介されている「出品」というMercariの中でもとても重要な機能をMicroservicesとして切り出し始めたこと．
Microservices化とその基盤の整備は来年以降もさらにコミットしていく．
Output 今年は対外発表がとても多かった．上記以外だと以下のような発表をした．
 Microservices on GKE at Mercari Continuous Delivery for Microservices with Spinnaker at Mercari Microservices Platform on Kubernetes at Mercari 今学ぶべき技術  英語で登壇はできなかったが&amp;ldquo;Interview: Taichi Nakashima from Mercari&amp;rdquo;で初めて英語でインタビューを受けたり，&amp;ldquo;Mercari with Taichi Nakashima and Tonghui (Terry) Li&amp;rdquo;でGCP Podcastに日本企業として初めて出演するという経験もした（写真）．
ブログは6記事書いた．特に読まれたのは&amp;ldquo;Service meshとは何か&amp;rdquo;だった．来年は月1記事くらいは書きたい．
またAPI Gatewayを開発するなかで書いた小さなGo PackageをOSSにすることもした（mercari/go-dnscache）．長期的にはAPI GatewayそのものをOSSにしていきたい．
Conference 会社で以下の海外Conferenceに参加させてもらった．</description>
    </item>
    
    <item>
      <title>Kubernetesがいかに自動化の考え方を変えたか?</title>
      <link>https://deeeet.com/writing/2018/12/13/how-kubernetes-change-our-way-of-automation/</link>
      <pubDate>Thu, 13 Dec 2018 00:43:34 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/12/13/how-kubernetes-change-our-way-of-automation/</guid>
      <description>先日Japan Container Days v18.12の基調講演で話をさせていただく機会があった．内容としてはMercari のMicroservices Platformの基盤として「なぜ」Kubernetesを選択したか？ついて現状や今後の展望を踏まえて紹介をした．
Microservices Platform on Kubernetes at Mercari
「なぜ」の回答としては，CRDやAdmission webhookといった拡張機構を使うことで今後起こりうる様々なWorkloadに特化したPaaSや抽象化レイヤーを書いていけるExtensibilityの高さとそのBuilding BlockとしてのEcosystemの強さを挙げた．
このトークのExtensibilityの文脈で話したくて時間がなかったのが「Kubernetesがいかに我々の自動化に対する考え方を変えたか？」だ．本記事ではその話せなかった部分をは吐き出しておく．
Preface 「Custom Controller書くぞ！」はMercari のMicroservices Platform チーム内で自動化について議論していると必ず出てくる発言だ．
Kubernetes以前の自動化ではコマンドラインツールを書くバッチスクリプトを書くもしくはAnsibleのplaybookやChefのRecipeを書くといった手法が使われてきた．Kubernetesが当たり前になってからは長らくそれらをやっていないし問題の解法として頭に浮かばなくなった（むしろ避けている）．コマンドラインツールを書くのは好きだったが最近はめっきり書かなくなった．
Kubernetesを使ってるから当然だと思われるかも知れないがもっと深い部分で考え方が変わった．つまりKubernetesでなくてもこの考え方は通用する．「Custom Controller書くぞ！」という発言はKubernetesの思想や内部機構がもたらした新しい自動化の考え方の１つだ．
How Kubernetes works Kubernetesの大きな特徴のひとつは「Declarative Configuration」だ．KubernetesユーザはKubernetes APIに対してあるべきDesiredな状態を宣言（Declare）しKubernetesはその状態になるように「自律的に」動き続ける．例えばユーザが「Podを5つ動かす」という状態を宣言するとKubernetesはそれを受け「Podが5つ動いている状態」を維持するように動く．
これを実現しているのがReconciliation loopである．Kubernetesでは大量のControllerが動いておりそれぞれが独立したReconciliation loopを実行している．個々のControllerはシステムの一部の小さな機能を担っており他のシステムの状態に関しては感知しない．それぞれがそれぞれの問題のみを解決する．UNIX哲学的に作られた独立したControllerの集合こそがKubernetesである．
このReconciliation loopは以下の4つを繰り返しているだけである．
 Desired stateを知る 現在のstateをObserveする Desired stateとObserveされたstateの差を見つける Desiredな状態になるような処理を実行する  Managing Kubernetes
Controllerの根底にある重要な概念が「Level Triggering」と「Edge Triggering」である．以下の図のようにこれらはシステムがあるSignal（もしくはEvent）に対していつ反応するかに違いがある．「Edge Triggering」はSignalの変化に対して反応し，「Level Trigger」は状態を検知して反応する．
Level Triggering and Reconciliation in Kubernetes
抽象的にみれば結果は同じだがSampling Rateを考えると結果は変わる．例えば以下の図を考える．Signalの上昇を「on」降下を「off」としてシステムとしては「on」で1を足し「off」で1を引くとする．このとき「Edge Triggering」が何らかの理由で「off」のTriggerに失敗するもしくは逃すと最終的な状態は「Edge triggering」は2になり「Level Triggering」は1になる．
Level Triggering and Reconciliation in Kubernetes
「Edge Triggering」は障害に弱い．人手によるオペレーションが前提になる．失敗が前提のDistributed systemではこれは非常にCriticalになる．Kubernetesがシステムとしてもしくはその上で動くアプリケーションがStableなのはReconciliation loopによる「Level Triggering」を採用しているからに他ならない（もちろんシステムそのものが複雑であるという対価は払っている）．</description>
    </item>
    
    <item>
      <title>KustomizeでKubernetes YAMLを管理する</title>
      <link>https://deeeet.com/writing/2018/07/10/kustomize/</link>
      <pubDate>Tue, 10 Jul 2018 09:34:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/07/10/kustomize/</guid>
      <description>Kubernetes YAMLの壁で述べたようにKubernetesのYAML管理はKubernetesユーザにとって長年の課題だ．コミュニティでは様々なツールが議論されてきた．先日SIG-CLIから登場したkustomizeは将来的にkubectlに統合される前提で開発されている+他のツールと比べても非常に筋が良い（と感じている）．本記事ではkustomizeが登場した背景とKustomizeを使って何ができるのかをまとめる．
Declarativeであること Declarative ConfigurationはKubernetesの重要な機能の一つだ．KubernetesユーザはKubernetes APIに対してあるべきDesiredな状態を宣言（Declare）することでKubernetesはその状態になるように動き続ける．例えばユーザが「Podを5つ動かす」という状態を宣言するとKubernetesはそれを受け「Podが5つ動いている状態」を維持するように動く．
Declarative configurationの逆のアプローチがImperative configurationだ．ユーザは一連の動作を全て指示する．例えばPodを5つ立てたいならその状態になるために必要な動作を1つ1つ指示する．Imperative configurationは理解しやすい，「これをして，これをして&amp;hellip;」と書くだけでありDeclarativeの複雑なSyntaxを理解する必要はない．Declarative configurationが強力なのは「あるべき状態」を伝えられることだ．Kubernetesはそのあるべき状態を理解できるのでユーザのインタラクションと独立してその状態へ「自律的に」動くことができる．つまり問題や障害があっても自分でそれを直すことができる（Self-healing）．より詳しくはLevel Triggering and Reconciliation in Kubernetesを読むと良い．
GitOps Declarative Configurationの大きな利点の一つはGitでバージョン管理できるところだ．つまり変更をPull Requesでレビューし変更の履歴を残すことができる．そしてGitを「Source of truth」としてCI/CD workflowを構築することができる（もともとあったものに名前がついただけだが最近はこれをGitOpsと呼ぶ）．
kubectlの問題 既存のkubectlコマンド「のみ」ではこのGitOpsを実現するのは難しい．例えばSecretリソースをBinaryファイルから作成するには，まずBinaryファイルをBase64でエンコードしそこからSecret用のYAMLを作成する必要がある．この場合Source of truthはBinaryファイルでありYAMLファイルではないため別途スクリプトを準備して2つのファイルを関連させなければならない．これはConfigMapリソースの管理においても同様である．
もちろんkubectl createコマンドとそのオプションを使うこともできるがそれはImperativeなワークフローである．
YAML管理の問題 近年の多くのOSSツールはKubernetesにデプロイするためのYAMLファイルが一緒に提供されていることが多い．試すだけならそのまま利用すれば良いことが多いが会社などで実際に導入する場合は環境に合わせたカスタマイズが必要である．例えばCPUやメモリを使いすぎないように適切なResource Limit/Requestを設定したり内部ツールのためにLabelやAnnotationを別途付与する必要がある．
また本番環境だけではなく開発環境用のYAMLファイルも準備するのも普通であるが，多くの場合それらの設定は同じにはならない．例えばResource limitは開発環境では少なめに設定するのが普通だと思う．
既存のkubectlコマンドのみを使うのであれば愚直に共通の設定を含んだ複数のYAMLファイルを管理するしかない．共通部分の設定変更に漏れが生じることは避けられないしUpstreamのYAMLファイルの変更の追従も難しい．
Kubernetes YAMLの壁で紹介したHelmなどを使えばこの問題をある程度解決できる．しかしHelmはデファクトではないのでそもそもHelm Chartが存在していない場合は自分でそれを書かないといけない．またHelmのTemplate機構では変更したいYAMLのフィールドが変数として公開されていないといけない．そのためChartが公開されていてもForkが必要な場合がある&amp;hellip;
Kustomize これらの問題を解決するために登場したのがkustomizeである．kustomizeはSIG-CLIのサブプロジェクトであり将来的にはkubectlに統合される前提で開発されている（Goにおけるvgoのような開発スタイル）．より詳細な背景や既存の問題点を理解するにはKEPや公式ブログを読むのが良い．
KustomizeはYAMLファイルのDeclarative管理を推し進めReusabilityとCustomizabilityを高めるツールである．
Kustomizeの使い方 基本はGithubのREADMEやExampleを読むのが一番良い．
kustomization ファイルを使う まずkustomization.yamlを準備しapplicationをつくる．applicationにより複数のYAMLリソースをGroupingする．例えば以下のように書ける．以下ではDeploymentとService，ConfigMapリソースからapplicationを構成している．
これらに対してkustomize buildコマンドを実行することでkubectl apply可能な1つのYAMLファイルを生成できる．
これにより上述したSecretリソースとBinaryファイル，ConfigMapリソースと設定ファイルの紐づけ問題を解決しDeclarative管理を行えるようになる．例えばSecretリソースは以下のようにkustomization.yamlに記述できるためbuild時にファイルのDecryptコマンドの実行が行える．
secretGenerator:- name:app-tlscommands:tls.crt:&amp;#34;cat secret/tls.cert&amp;#34;tls.key:&amp;#34;cat secret/tls.key&amp;#34;type:&amp;#34;kubernetes.io/tls&amp;#34;ConfigMapリソースに関してはbuild時に設定ファイルの内容からhash値を計算しmetadataとnameのsuffixにそれを自動で付与してくれるので，設定ファイルを変更する度に名前がユニークになる．つまり設定ファイルを変えると新しくデプロイが走るようになる（+ロールバックも可能になる）．
さらにkustomization.yamlには共通のnamespaceやcommonLabelsを書けbuild時に各リソースにそれを差し込むこともできる．
Overlaysを使う さらに1つのKustomizationファイルをbaseとしてoverlayにより複数のvariantを生成することができる．つまり共通のYAMLファイルから一部の設定のみが異なるYAMLファイルを作成できる．例えばBase YAMLファイルから本番環境用のYAMLと開発環境用のYAMLを生成できる．
以下のように書ける．以下からはBaseのKustomizationファイルからReplica数とCPU limitのみを変えた本番用のYAMLファイルを生成する．
この機能により外部のYAMLファイルの管理問題と複数環境問題を解決できる．OSSとして提供されるYAMLファイル（off-the-shelf configuration）をcloneしてきてOverlayによりInternal用の環境に合わせてカスタマイズすれば良い．Template方式とは違い上書きするだけなのでBaseの設定ファイルの書き方に影響を受けない．
まとめ Kustomizeの登場した背景と何ができるかについて簡単にまとめた．
Helm Chartが準備されていないOSSツールのため，複数の環境用のYAMLファイルの管理のためにわざわざChartを準備するのはめんどくさいなあと感じていたし，今後kubectlにも統合されていくことを考えてもKustomizeは今後良い選択になっていくと感じている（実際に本番で動かしているアプリケーションも移行してみていて良さそうと感じている）．
またMercariではCDにSpinnakerを利用しているのでkustomize buildを行うPipeline stageを準備できれば良いかなーと思っている．
参考  kubernetes-sigs/kustomize SIG CLI KEP for Kustomize subproject GitOps - Operations by Pull Request Helm vs Kapitan vs Kustomize Kubernetes YAMLの壁  </description>
    </item>
    
    <item>
      <title>Service meshとは何か</title>
      <link>https://deeeet.com/writing/2018/05/22/service-mesh/</link>
      <pubDate>Tue, 22 May 2018 21:50:11 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/05/22/service-mesh/</guid>
      <description>Microservicesの世界においてService meshは大きなキーワードになった．KubeCon 2017やKubeCon 2018 EUにおいても多くのセッションをService mesh（もしくはその代表格であるIstio）が占めており注目の高さも伺える．もちろんMicroservicesを進めるMercariにおいても導入を検討しており今後重要なコンポーネントの1つになると考えている．本記事ではそもそもなぜService meshという考え方が登場したのか，なぜ重要なのか? その実装としてのIstioとは何で何ができるのか? について簡単にまとめてみる．
参考文献 Service meshを一番理想的な形でサービスに使い始めその考え方を広めたのはLyftだ（と思う）．LyftはIstioのコアのコンポーネントであるEnvoyを開発しそれを用いてService meshを構築し自社のMicroservices化の課題を解決してきた．Service meshの初期衝動や真価を知るにはLyftの事例を見るのが良い．Envoyの作者であるMatt KleinによるKubeCon2017での発表&amp;ldquo;The mechanics of deploying Envoy at Lyft&amp;rdquo;や彼が寄稿しているSeeking SREの13章&amp;quot;The Service Mesh: Wrangler of Your Microservices?&amp;ldquo;などがとても参考になる．
またService meshを広めるきっかけとなったオープンソースのプロジェクトはIstioである．Istioはまだ登場したばかりであるが既に書籍がある．RedHatの開発者によるIntroducing Istio Service Mesh for Microservices（無料!）を読むとIstioの大まかな概要を掴めると思う．
さらに（これはまだ自分が読み途中だが）Zero Trust NetworksもService meshを知る上で重要な考え方の１つだと思う．
Microservicesの現状と課題 最初にMicroservicesの世界の現状と課題について簡単にまとめる
言語 MicroservicesおいてPolyglotは普通だ．そもそも適切なサービスで適切な技術を採用できるようにすることはMicroservicesの大きな目的の1つであり，その選択にはプログラミング言語も含まれる．特に現状では普通のAPIに関してはGoが選択されることが多いが，Machine learningのモデルのServingにはPythonをフロントエンド系にはNodeをという選択は普通に有り得る．正直そのサービスオーナーが運用や採用を含めて責任を持てるならHaskell使おうとRustを使おうと問題はない（はず）．
また言語によってはフレームワークも多彩であり，例えばPythonであればFlaskを使うこともあればDjangoを使うこともあるだろう．
Protocol Microservicesにおいてはサービス間はネットワーク越しにコミュニケーションを行う．そのためそのコミュニケーションに利用するProtocolも多彩になる．MercariのようにgRPCを共通のProtocolとして採用することもあれば，HTTP/1.1もしくはHTTP/2でRESTを使うこともある．MessagingとしてKafkaやCloud PubSubを使ったり，CacheとしてRedisやMemcacheを使ったり，DatabaseとしてMySQLやMongoを使うこともありそれぞれProtocolは異なる．
分散システム ネットワーク越しのリクエストが前提となるMicroservicesは分散システムである．Fallacies of distributed computing（分散コンピューティングの落とし穴）にあるように「ネットワークは信頼できる」と思ってはいけない．この分野では，リクエストが失敗したときにback-offつきでRetryを行うこと，Timeoutを設定すること，適切なRate-limitをつけ異常なリクエストをブロックすること，対象のサービスが何らかの障害で死んでしまってもCircuit breakingでそれを回避することなど多くのBest practiceが養われてきた．MicroservicesはこれらのPracticeを使う必要がある．
Observability 上で紹介したSeeking SREにおいてMatt Kleinが述べているようにMicroservicesでは可視化が全てだ．ログやメトリクス，分散Tracingを駆使してあるリクエストがどこで何が起こったのかを理解できるようにしなければならない．これをちゃんとやるにはログやメトリクスなどは一貫性のあるフォーマットに揃っている必要もある．
（KubeConとかで話しているとこれができてないところは多いらしい&amp;hellip;）
認証認可 Microservicesでは各チームは自分のサービスに対して責任を持ち他のサービスについて気にしないことが理想だ．このためにはZero Trust（もしくはDon&amp;rsquo;t trust each other）を前提とし，サービス間はデフォルトでmTLSもしくはRole-Based Access Control (RBAC)などによるAuthN/AuthZを行う必要がある．
Service meshの登場 上述した機能や課題を全てアプリケーションレイヤーで実装するのは現実的ではない．アプリケーション開発者はビジネスロジックを書き最高のサービスを書くことに集中しそれ以外のMicroservices固有の問題からは開放されないければならない．古くからMicroservicesを実践してきた大企業，例えばNetflixやGoogle，Twitterなどは各言語向けのSDKを実装することでこれらを解決してきた．NetflixがOSSとして公開しているHystrixなどは有名だ．しかしこの方法には以下のような課題がある．</description>
    </item>
    
    <item>
      <title>Kubernetes上でgRPCサービスを動かす</title>
      <link>https://deeeet.com/writing/2018/03/30/kubernetes-grpc/</link>
      <pubDate>Fri, 30 Mar 2018 01:21:28 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/03/30/kubernetes-grpc/</guid>
      <description>Kubernetes上でgRPCサービスを動かすことが多くなってきている．が適切にロードバランスをする，リクエストを落とさずサービスをデプロイするためにいくつか注意することがあるので簡単にまとめておく．
以下の2つを意識する．
 Kubernetes ServiceはL4のLoad balancer（LB）であること gRPCはコネクションを使いまわすこと  KubernetesのPodは死んだり作られたりを繰り返す．KubernetesのPodにはそれぞれ内部IPがアサインされるが，このIPはPodが新しく作成される度に変わる．IPが変わってもPodにアクセスするためにKubernetesではServiceをつくる．ServiceはPodを抽象化しVirtual IP（VIP）を提供する．VIPを使うことでPodのIPが変わってもPodにアクセスすることができる．
VIPはNetwork interfaceに接続された実際のIPではない．VIPの目的は単純にTrafficを対象のPodにforwardすることであり実体はiptablesである（ちなみにVIPとPodsのIPのMappingはkube-proxyが担っており継続的にKubernetes API経由でServiceの変更を検知しiptablesの設定を更新する）．これよりKubernetesのSewrviceはTCP/IPベースの，つまりL4のLoad balancer（LB）であることがわかる．
Kubernetesは内部DNSも提供しServiceに対してもDNSレコードが作られる（e.g., my-svc.my-namespace.svc.cluster.local）．このDNSが返すレコードは上述のVIPでありPodのIPは返さない（理由としてはTTLを無視した古いDNSライブラリなどの影響を避けるためである）．
HTTP 1.1 / RESTの場合は都度コネクションが貼られるためこの内部DNSとServiceへのリクエストは問題なくロードバランスされる．がコネクションを使いまわすHTTP2の上にのるgRPCでは問題になる．単純にやると複数のServerがあろうと全てのリクエストは1つのServerにのみ到達する（L4のLBではgRPCは適切にロードバランスできない．詳しくはIntroduction to modern network load balancing and proxyingを読むと良い）．Clientを増やしても「運が良ければ」別のサービスにLBされるのみである．Clientの数がServerよりも少ないと幾つかのServerはidleになってしまう．
Client-side LB 現状の解法としてはHeadless Service（内部DNS経由で各PodのIPが取得できる）+ gRPC Client Side LBを使うのが良い．Goの場合はv1.6でDNS Resolverが入ったのでそれを使う（今までは自分で書かないといけなかった&amp;hellip;）．以下のようにDialする．
resolver, _ := naming.NewDNSResolverWithFreq(1 * time.Second) balancer := grpc.RoundRobin(resolver) conn, _ := grpc.DialContext(context.Background(), grpcHost, grpc.WithInsecure(), grpc.WithBalancer(balancer)) これによりgRPCはDNS経由でPodのIPを取得し全てのgRPC serverにコネクションを張りMethod call毎にロードバランスを行うようになる．また定期的にDNS resolveをしPodのIPに更新があればコネクションを張り直す．Freqはなるべく短くしておく必要がある．PodのIPは頻繁に変わるのですぐに更新される必要がある（デフォルトだと30分なので完全に死ぬ）．
MicroservicesのようにPolyglotを意識しないといけない場合はClient-side LBを言語毎に実装するのは現実的ではない（例えばPythonの場合はDNS cacheが問題になった&amp;hellip;）ためSidecarパターンを考えるのが良い．
Sidecar gRPC/HTTP2を扱える+Kubernetesで動かすProxyとしてはEnvoyがデファクトになりつつある．EnvoyはSidecarコンテナとしてClient Pod内にデプロイするようにデザインされている．ClientからServerへのリクエストをSidecar Envoy経由にすることでEnvoyが適切にgRPCリクエストのロードバランスを担ってくれるようになる．詳しくはUsing Envoy to Load Balance gRPC Trafficが詳しい．
Client-side LBと比較したSidecarの利点はアプリケーションコードに変更を加える必要がない点である．ただ毎回Sidecarを意識してデプロイするのは煩雑であるのでIstioなどを使いService Meshを構築してしまうのが今後の方向性だろう（Istioの利点はLB以外にもある）．</description>
    </item>
    
    <item>
      <title>Kubernetes YAMLの壁</title>
      <link>https://deeeet.com/writing/2018/01/10/kubernetes-yaml/</link>
      <pubDate>Wed, 10 Jan 2018 11:42:29 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/01/10/kubernetes-yaml/</guid>
      <description>Kubernetes に入門しようする人を躊躇させる原因のひとつは間違いなくYAMLによる設定ファイルだろう．Kubernetesにアプリケーションをデプロイするとき，例えそれがシンプルなサーバーアプリケーションであっても，多くのYAMLファイルを手で記述する必要がある．初心者を慄かせるその大量のYAMLはよくwall of YAML（YAMLの壁）などと揶揄される．
初心者でなくてもKubernetesのYAMLは煩わしい．YAML自体は単なるKubernetes APIへのリクエストボディであり慣れてしまえば実はそんなに難しくない．しかし記述する内容のほとんどがBoilerplateであり何度も書いていると飽き飽きする（実際にはほとんどがコピペだが）．あるアプリケーションの開発環境と本番環境のYAMLファイルをいかに効率的に管理するかについて決定的な方法もない．
そもそもKubernetesの開発者はこのYAMLを利用者に書かせるつもりはなかったらしい（参考）．しかしKubernetesの誕生から3年が経ち未だにYAMLで設定ファイルを記述するスタイルは変わらない．今後もおそらく簡単には変わらないだろう．
ただこの状況を改善しようとするプロジェクトは多く存在する．それらのプロジェクトは煩雑なYAMLの記述を避けKubernetesへのアプリケーションのデプロイの敷居を下げることを目標としている．そして今どのプロジェクトがデファクトになっていくかが注目されている．
本記事では現時点（2018年1月）においてどのようなプロジェクトがあるのかその状況および問題を簡単にまとめる．私見や自分がどのように使っているかは書くがあくまで現状の整理を行う．
Helm https://helm.sh
まず現時点で一番有名なのはHelmだろう．Helmは Kubernetes Package Managerである．
Kubernetes Package Managerとは何か? KubernetesをKernelとみなすと複数のNodeインスタンスを束ねたClusterを１つのコンピューター，その上で動くコンテナはプロセスとみなすことができる．このような視点になるとHelmはCentOSにおけるYUM，DebianにおけるAPTと同様の役割を果たす．例えばGrafanaをインストールしたい場合は以下のようにできる．
$ helm install stable/grafana このようにHelmはKubernetes上で動くアプリケーション（コンテナ）のパッケージング及び配布の機構を提供する．
HelmのパッケージングのフォーマットはChartと呼ばれる．Chartの中身は単純にYAMLのTemplateである（より具体的にはGo Templateである）．デプロイ時にこのTemplateの変数に対して具体的な値を渡すことでYAMLを作りそれをクラスタに実行する．例えばDockerイメージのバージョンだけを切り替えたい場合はTemplateは以下のようになる．
containers: - name: example image: gcr.io/deeeetlab/example:{{ .Values.version }} .... HelmはWall of YAMLに対するシンプルな解法だ．Boilerplateを共通Templateとしてしまい変更が必要な部分のみを外部から与えられるようにする．Helmはコミュニティにも受け入れられており既に多くのChartが公式のレポジトリに存在する．
メルカリでもHelmは使っている．具体的にはSREが提供する共通のツールはHelmでパッケージングしている．chartmuseumを使ってインターナル向けのレポジトリも準備しているところだ．
Helmの課題 Helmが完璧かと言われるとそうでもない．特にセキュリティに関してはExploring The Security Of Helmで指摘されているように少しザルすぎるように思える．
はっきり言ってHelmでmicroservicesのパッケージングはやりたくはない．HelmのTemplateは結局YAMLでありChartを書くには結局YAMLを書く必要がある．さらにGo templateを駆使した方法はスマートには見えないところが多い．YUMやAPTでAPIアプリケーションの配布をしたいか?と言われたら答えは「No」だろう．Helmはあくまで共通系のツールの配布にしか向いていないと感じている．
共通系のツールの配布としても完璧とは言えない．公式で提供されているChartに対するカスタマイズは公開されている一部の変数のみにしか許されない．カスタマイズ性は著しく低くその作者の力量にも大きく依存する．ツールの設定ファイルなどをYAMLの中に直接書いてあったり気持ち悪い部分も多い．
Helmでインストールしたパッケージの設定ファイルをいかに宣言的に管理するか？に対する解も存在しない．APTやYUMに対してChefやAnsibleが登場したようにHelmに対してももう一段ハイレベルな管理ツールが求められるだろう．
とはいえHelmは広く使われておりデファクトになりつつある．今年3.0もリリースされる予定になっている．今後の進展は要チェックである．
ksonnet https://ksonnet.io
Helmの後発として2017年に登場したのがksonnetである．まだ登場したばかりだが既にkubeflowなどでの採用実績がある．
ksonnetはKubernetesの設定ファイルの記述にjsonnetを採用している．jsonnetはGoogleで開発されたJSONを定義するためのDSLである．変数やオブジェクトの連結などが可能なJSONのサブセットだと思えば良い．またKubernetesのAPIオブジェクトは予めksonnet-libとして共通化して提供されているためBoilerplateを避けることができる．例えばNginxコンテナのDeploymentの記述は以下のようになる．
これによりYAML問題は避けられている．
ksonnetはさらにもう一段進んだ機能を持つ．まずPrototypeというコンセプトを持つ．これはjsonnetによるTemplateである．デフォルトでいくつかの共通パターンが提供されている．例えばDeployment resourceとService resourceを使ったアプリケーションのデプロイはよく使うパターンだがこれはdeployed-serviceという名前のprototypeで提供されている．ユーザーはこのprototypeに具体的な値を与えて実際のデプロイの構成する．Helmと同様にPrototypeを自分で書き配布することもできる．
次にksonnetはEnvironmentというコンセプトを持つ．これはkubernetes本家にもHelmにもない概念だ．ksonnetはどの環境，つまりどのクラスタに，設定ファイルを適用するかを強制する．各環境ごとの設定値の変更もスマートに行える．これにより間違ったクラスタに設定ファイルを適用してしまった&amp;hellip;という問題を避けることができる．
ksonnetのコンセプトと全体像は以下の図でまとめられる．
ksonnetは流行るか コンセプト自体はとても好きだが何とも言えない．自分の中ではjsonnetがどこまで書けるようになるかが懐疑的だ．皆がこれを普通に扱えるようになるようになるのかもわからない．VS CodeのExtensionが提供されていたりするのでIDE前提にはなりそうだが&amp;hellip;
kubeflowによる採用事例はあるがまだ少ない．生態系が整っていくと未来はあるかもしれない．Helmでサポートも考えられているらしいので（参考）より広く使われる機運はありそうと感じている．
興味がある人はTutorialとTourをひと通り見てみると良い．
kubecfg https://github.com/ksonnet/kubecfg
ksonnetはかなりopinionatedなプロジェクトだ．jsonnetの採用だけでなくPrototypeやEnvironmentなど独自の概念も多い．ksonnetからopinionをなくしたのがkubecfgである（開発者にも言われた）．
kubecfgはシンプルにKubernetesの設定ファイルをjsonnetで記述可能にするプロジェクトだ．シンプルにYAMLによる手書きを避けることを目的としている．まだ採用事例は見ないがこれはこれでシンプルに良い解になりそうだなあと感じている．
まとめ 本記事ではKubernetes YAML問題を解決しようとするいくつかのプロジェクトを紹介した．他にもkomposeやkedgeなどもあるが全てを見れてはいない．Helmが広く認知されている一方で問題も多い，ksonnetのコンセプト自体は良いがまだ認知は少なく流行るかわからないというのが現状だ．</description>
    </item>
    
    <item>
      <title>2017年振り返り</title>
      <link>https://deeeet.com/writing/2017/12/31/2017/</link>
      <pubDate>Sun, 31 Dec 2017 02:10:01 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/12/31/2017/</guid>
      <description>今年は激動の一年だった．
1月．SREとしてMercariに転職した．試用期間の間は既存のGolangミドルウェアの改善の手伝いをしたり，Botを書いて業務の自動化をしたり（GolangでSlack Interactive Messageを使ったBotを書く），デプロイの高速化などをやっていた．
4月．試用期間が終わった直後くらいにUSへの長期出張が決まった．ちょうどMercari USが日本に先行してMicroservices化に舵を取り始めた時期でその土台作りなどを手伝った．Spinnakerを使ってCD環境を整えたり（SpinnakerによるContinuous Delivery），CI環境を整えたり，gRPCを使ったサービス自体を1から書いたり，Cloud PubSubを使ったイベント連携を仕込んだりなどやれることは全部やったと思う．あとgo-httpdocというGoパッケージを書いてOSSとして出すなどもした．
他にも出張中はCoreOS Fest 2017やGopherfest 2017，GoSF，Spinnaker 1.0 Launch Partyに参加したりもした．このようなイベントが近場で開催されていてさっと行ける，使ってるツールの作者や大規模に運用している企業のひとが普通にいて直接話を聞いたりできる，というのはとにかく良くてここで働きたいなあという気持ちが強く芽生えた．
7月．日本に帰ってきてからはUSでの知見を活かし日本でもMicorservices化の推進を始めた．SREの中に専門のチームもでき良い体制ができつつある．加えてMachine Learningのプロダクトを安全かつ高速にリリースするためのプラットフォームの構築の手伝いも始めた（具体的な成果はメルカリの今年1年間の機械学習の取り組みとこれからで紹介されている）．
8月．長期出張などもありずっと休めていなかったので長めの休みをとってノルウェーに行き友人の結婚式に参加した．海外の結婚式に出席するのは初めてでとても良い経験だった．
9月．第 1 回 Google Cloud INSIDE Games &amp;amp; AppsにてMicroservices化の取り組みについて発表した（Microservices at Mercari）．ここでの発表は反響も大きく良いフィードバックをたくさんもらえて良かった．
10月．結婚した．日本に帰ってきてからは結婚の準備と仕事との両立でとにかく大変だった&amp;hellip;（例のリストです）．
11月．Go Conference 2017 Autumnで発表した（Go+Microservices at Mercari）．Go+gRPCで書かれたMicroserviceをいかにk8s上で動かすかについての知見を紹介した．
12月．KubeCon + CloudNativeCon North America 2017に参加した．KubernetesはMercariのMicroservicesにおいて重要な基盤になっておりどのセッションも学びしかなかった（具体的な感想はKubeCon2017感想: Kubernetes in 2018に書いた）試したいことやアイディアがたくさん溜まっているので冬休み中にいろいろ検証したいと思っている．
2018年 2018年も引き続きMicroservicesとMachine Learning Opsが自分の中で大きな2軸になると思う．
今年のうちにMicroservices化をさらに加速させるための仕組みを作れたと思う．が来年多くのサービスが作られる中で解決するべき課題はまだまだたくさんあり挑戦は終わらない．
Google and Uber’s Best Practices for Deep LearningにあるようにMachine Learningの分野においてもSREやDevOpsエンジニアはますます重要な職種になっている．これまで培ってきたTDDやCI/CD，Immutable Infrastructureといったプラクティスをいかに応用してくかが大切だと思う．まだとりあえずCloudに投げれば良いってのがやりにくい分野でもあるのでやりがいがある．
転職していきなりMicroservices化という大きな仕事に抜擢してもらえてかなりやりがいのある一年だった．かなり楽しいし刺激的だけどその一方でプレッシャーも大きく感じていてそれを振り払うように働きまくってしまったように思う．来年は仕事外にもちゃんと時間を割きリズムを崩しすぎないようにしたいと思う．
References  2016年振り返り 2015年振り返り  </description>
    </item>
    
    <item>
      <title>Golangのcontext.Valueの使い方</title>
      <link>https://deeeet.com/writing/2017/02/23/go-context-value/</link>
      <pubDate>Thu, 23 Feb 2017 09:20:07 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/02/23/go-context-value/</guid>
      <description>Go1.7でcontextパッケージが標準パッケージに入りしいろいろなところで使われるようになってきた．先日リリースされたGo1.8においてもdatabase/sqlパッケージなどでcontextのサポートが入るなどますます重要なパッケージになっている．
&amp;ldquo;Go1.7のcontextパッケージ&amp;rdquo;で書いたようにcontextは「キャンセルのためのシグナルの受け渡しの標準的なインターフェース」として主に使われる．ある関数やメソッドの第1引数にcontext.Contextが渡せるようになっていればキャンセルを実行したときにその関数は適切に処理を中断しリソースを解放することを期待する．これはパッケージの作者とその利用者との間のある種の契約のようになっている（パッケージ側でgoroutine作るなというパターンもここで効いてくる）．
これだけではなくcontext.ContextインターフェースにはValueというメソッドも定義されている．これを使うと任意の値を受け渡すことができる（contextと言われるとこちらを想像する人も多い）．これは便利だが注意して使わないと崩壊するのでどう使うべきかをまとめておく（contextも分かりやすい）．
なぜ注意が必要か? context.ValueのSetとGetは以下のように定義されている．
WithValue(parent Context, key, val interface{}) Context Value(key interface{}) interface{} WithValueで値をセットしValueで値を取り出す．注意するべきなのは型を見ればわかるようにtype-unsafeでコンパイラでチェックができないからである．要するにmap[interface{}]interface{}である．つまり避けれるなら避けた方が良い．
例えばチームでAPIサーバーを開発していてあらゆる値が様々なHandlerで無防備にSetされたりGetされたりするようになると崩壊する．
どのようなときに使えるか? ではどのようなときにValueは有用になるか? ある特定のリクエストスコープ内で限定的な値を渡すのに便利に使える．例えば以下のようなものが考えられる．
 ユーザID 認証情報（Token） Distributed TraceのID  どのような値を渡すべきでないか? あるいは適していないか．例えば，DB ClientやAPI Client，loggerなどである．これらはスコープに限定的ではないしそもそもテストがしにくくなる．これらはサーバーが依存として持つべきである．以下のようにmiddlewareで渡すかhandlerに持たせる（ジョブワーカーを書いている場合もStructを定義してそこに渡すべきである）．
func MyMiddleware(db Database, next http.Handler) http.Handler { return http.HandlerFunc(func (w http.ResponseWriter, r *http.Request) { // Use db here.  next.ServeHTTP(w, r) }) } type MyHandler struct { db Database } func (h *MyHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {...} 使い方 context.Valueを使う上で注意するのは値へのアクセスを制限する，ちゃんと型を持たせることである．以下のやり方はどちらかというとstrictでパッケージ作者寄りのやりかただが，チームで何か書いている場合であってもむやみにいろいろな値がSetされてカオスになるよりは初めから厳しくやるのが良いと思う．
まずkeyは以下のようにunexportedな型をもったunexportedなconstとして定義する．こうしておけば意図しないところ（少なくともpackage外で）で値がSetされたりGetされることがなくなる．</description>
    </item>
    
    <item>
      <title>SREとしてMercariに入社した</title>
      <link>https://deeeet.com/writing/2017/02/13/mercari/</link>
      <pubDate>Mon, 13 Feb 2017 09:47:24 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/02/13/mercari/</guid>
      <description>1月16日よりMercariにてSRE/BSE（Backend System Engineer）として働いてる．
これまではとある会社で社内向けのPaaSエンジニアとして働いてきた（ref. PaaSエンジニアになった）．PaaSの目標である「アプリケーション開発者の効率を最大化」を突き詰めながら少人数のチームでいかにScalableなプラットフォームを構築するかに注力してきた．Cloud FoundryやDockerといったインフラの最前線とも言える技術やアーキテクチャに触れ，かつその中で自分の技術的な柱である自動化に取り組むことができたのは非常に刺激的で自分に大きなプラスになった．
その一方でPaaSというプラットフォームはその性質上サービスそのものからは中立的になることが避けられない（だからこそScalabilityを実現できるのだが）．よりサービスに近い部分，サービスの成長に直結する部分で開発がしたい，それを支えるインフラに関わりたいと思うようになった．
Mercariという選択は，サービスやその可能性/成長，働いているひと，使っている技術といった視点から行った．が一番の大きな理由は日本発のサービスとしてグローバルな市場を本気で獲りに行っているところだ．UberやAirbnb，Netflixを見ていても今後Webサービスはグローバルで闘えることが必須になるだろうし技術的に一番チャレンジングかつ面白いものそこにあると思う．自分が身に付けたいと思う技術もそこにある（実際早くもそれに関わる仕事ができた）．
Site Reliability Engineering (SRE)はGoogleが提唱し多くの企業で採用され始めている職種である．その初期衝動はソフトウェアエンジニアリング的にいかにインフラを設計するかにあり，Error Budgedという現実的な指標をもってInnovationの促進とReliabilityの担保というトレードオフを解決しているところが大きな特徴であると言える．これらの思想をもとに各社が独自の実装をしているのが現状だと思う．
Mercariは早くからこのSREという名前を採用している（ref. インフラチーム改め Site Reliability Engineering (SRE) チームになりました）．メンバーの各人がそれぞれの得意分野をもちAvailabilityやPerformance，Security，Deploy Automationに取り組んでる．まだチームに入って1ヶ月程度だが皆がどんどん問題を解決してるのをみて圧倒されている．自分に足りない部分は吸収しつつPaaSの世界で培ってきた考え方や技術で貢献していきたいと思う．また自分なりにSREとは何ぞやというもの考え行動していきたい．
Backend System Engineer (BSE)という職種はMercariのSREの中でもソフトウェア開発に重点を置いている職種である．開発言語にGolangを使いアプリのPush基盤やAPI gateway，Proxyサーバーといったミドルウェアを開発している．近年一番自分が使い込んできたGolangという技術を一番理想的な形で使えるのも非常に楽しみだ（とりあえずGo1.8対応はしたgaurun#57，widebullet#10）．
これからよろしくお願いします🙇
（とりあえず席は@kazeburoさんと@bokkoさんに挟まれていて緊張感がある&amp;hellip;！）</description>
    </item>
    
    <item>
      <title>Writing An Interpreter In Goを読んだ</title>
      <link>https://deeeet.com/writing/2017/01/12/go-interpreter/</link>
      <pubDate>Thu, 12 Jan 2017 17:06:29 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/01/12/go-interpreter/</guid>
      <description>Thorsten Ballによる&amp;ldquo;Writing An Interpreter In Go&amp;rdquo;を読んだ．
技術界隈のブログを見ているとたまにSteve Yeggeの「If you don’t know how compilers work, then you don’t know how computers work」という言葉に出会う．その度に学生のときにコンパイラの授業を受けなかったこと後悔し，社会人になって挑戦しようとして挫折したことを思い出して悲しい気持ちになる．@rui314さんのCコンパイラをスクラッチから開発してみたを読んではかっこいいなと思いつつ僕には無理だなあと心が折れていた．
どの言語を書いていてもコンパイラ（もしくはInterpreter）は切っても離せないものであり内部の動きがどうなっているかを知っておきたいという欲求はプログラマーなら誰しもあると思う（少なくとも僕にはある）．では他にも学ぶことがたくさんあるという時間制約の中でベストな学習リソースは何かと言われると自分の観測範囲ではなかなか良いものに出会うことはなかった．Dragon Bookは重すぎるしLispの処理系をx行のRubyで書いて見ました系ブログは軽すぎる．
本書&amp;ldquo;Writing An Interpreter In Go&amp;rdquo;はその1000ページのコンパイラ本と個人ブログのギャップを埋めるために書かれた本である．紙の本にした200ページ程度でさっと読める．なんちゃって言語を実装するのではなくMonkeyという本書のためにデザインされたある程度まともな言語（C言語っぽい）を実装する．スクラッチから初めてLexer，Parser， Evaluationを実装していく（HostにGolangを使うのでアセンブラなどまでは踏み込まない）．テストまでちゃんと書くとだいたい2000行程度で実装できた（時間にするとだいたい1週間程度）．
本書の利点を挙げると，
 サンプルコードがGolangで書かれている（かつ標準パッケージのみが使われている）．Golangはとにかく言語仕様のシンプルであるため本書のサンプルコードを読むのはとても簡単である．また自分の好きな言語に移植するのも容易であると思う． サンプルコードを動かしながら読み進めることができる．最新のGolangのruntimeで動かすことができるので環境を準備するのはたやすい．Lexerを書けばここまでできて，Parserを書けばここまでできて&amp;hellip;と読むことができて理解度が高い． テスト駆動で書かれている．本書に登場するコードはすべてテストもセットになっている．テストのおかげで何を期待するのかをすぐに理解することができた．またテストはGolangのベストプラクティスであるTable Driven Testsが採用されているため読みやすい（ただし途中でテストも写経するのはめんどくなった&amp;hellip;）  これらの利点以上に感動したのが本書の書き方である．そもそも作者はコンパイラを職業にしているひとではない．個人的なpassionでコンパイラについて知識を深めてきたひとである（詳しくは作者が出演したGo time #28 を参考）．そのために前提がとても優しい．僕のようなゴリゴリのCS出身ではないプログラマが疑問に思うことを一つ一つちゃんと拾ってくれる（書き方も柔らかくて「これ疑問に思ったっしょ？次にちゃんと説明するから！」的に書かれていて良い）．
表層的な部分だけでなくて内容に少し触れておくと，本書で一番面白かった・感動したのはParserの実装である．Paserは実装するのではなくyaccやbisonなどのParser generatorを使うのが一般的らしいが本書ではそれらのツールを使わない．すべて自分で1から実装する．特にExpressionをParseするためのPratt parser（JSLintで使われている）は他のParserを知らないため比較はできないがとにかくシンプルで感動した（デバッグとかしんどいしPaser generator使わなくてもシンプルにできるよという話もあるHandwritten Parsers &amp;amp; Lexers in Go．実際自分で書いたりGolangの実装を覗いたりしてみるとシンプルなものなら自分で書いても良いのでは?という気持ちにはなった）．
あとGolangを書いているひとにオススメなのはGolangのコンパイラ実装であるhttps://golang.org/pkg/go/と合わせて読むこと．名前のつけ方が似てるので比較しながらコードを追うことができとても勉強になる．
まとめ コンパイラのことがわかったのかと言われるとまだまだ自信を持ってYesとは答えられない．が全く知らないという状況は抜け出したと思う．前よりも恐れがなくなったというか身近に感じている．これを入り口にしてさらに専門的な本を読んでみようという意欲も湧いている．とにかく読んでよかった．
少しでもコンパイラに興味があり入り口が見つけられない人は是非手にとると良いと思う．オススメです！</description>
    </item>
    
    <item>
      <title>2016年振り返り</title>
      <link>https://deeeet.com/writing/2016/12/31/2016/</link>
      <pubDate>Sat, 31 Dec 2016 13:16:08 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/12/31/2016/</guid>
      <description>最初に今年やったことなどをつらつらと書いてみる．
2月．Dave Cheneyが中心となりGo1.6のRelease Partyが世界各地で開催されることになった（Go 1.6 release party）．東京でもやりたかったのでOrganizerとなりHatenaのオフィスを借りて開催した．8月のGo1.7のリリース時は特に世界規模でやる流れはなかったがフォーマットだけは借りて再びHatenaのオフィスで開催した．この時リリースの概要をまとめたスライドを作ったが@bradfitzがそれを改変して別のMeetupの発表資料に使ってくれた嬉しかった．2017年2月にリリース予定のGo1.8のリリース時は再び世界規模でやるかってのをDaveと話したのでぜひ参加してください．
5月．みんなのGo言語の執筆を主に行っていた．自分はコマンドラインツールに関する章を担当した．今まで発表やブログ記事の集大成的に書けたので良かった．（村上春樹の手法を真似て）朝5時に起きて午前中のみ/決められた分量のみに集中するという方法で書き進めたがうまくいった気がする．9月に販売したが概ね高評価でとても嬉しい（ブログなどは基本見つけたものは目を通してます！ありがとうございます）．日本語だけどDaveやGo teamの@rakyllにも手に取ってもらえた．
6月．ドイツのBambergにある支社で2週間ほど働いた．本社のメインチームと離れてリモート側になった時の問題を身にしみて感じた．Bambergはかなり田舎でみながゆっくりと生活していた．同僚とひたすらビールを飲んでいた．Berlinも休日を使って訪問したが町中からエネルギーを感じて良い街だった．いつか住んでみたい．
7月．Devenverで開催されたGopherConに参加した．有名な（プロジェクトの）開発者が普通にうろうろしている状況でとても刺激的だった．自分もLTの舞台で発表した（詳しくはGopherCon 2016でLTしたに書いた．動画もある）．次は通常セッションで参加したい．
12月．golang.tokyoでテストしやすいGoコードのデザインという発表をした．自分が今のチームでコードを書く時に/レビューをするときに意識してきたことをまとめて発表する良い機会だった．こちらも良いフィードバックをもらえたので嬉しかった．
そして12月31日をもってRakutenを退職した．新天地でも引き続きやっていきます！
書いた記事 今年はアウトプットよりもインプットを重視したため去年の半分の14本だった．年間PVは229,231 viewsだった．特に読まれたのは以下の記事．順番はPV順．
 Golangのエラー処理とpkg/errors GolangのGCを追う GolangでAPI Clientを実装する Go1.7のcontextパッケージ Golangにおけるinterfaceをつかったテスト技法  英語で書いた記事はTracing HTTP request latency in golangが多く読まれた．
来年も引き続きインプット重視の予定なので今年同様に月1本程度書けると良い．
公開したOSS OSSとしては以下を書いた．
 https://github.com/tcnksm/gotests (★254) https://github.com/tcnksm/go-input (★147) https://github.com/tcnksm/go-httpstat (★115) https://github.com/tcnksm/dutyme (★17) https://github.com/tcnksm/go-irkit (★6)  上の3つはGo Newsletterでも取り上げてもらえた．
良かった記事 良いと思った記事や発表はブクマで記録してるが今年は1000ほどあった．その中でも特に印象に残ったのは以下（Golangに関しては多かったので別途まとめた 良かったGolangの記事/発表（2016年））．
 Unikernels are unfit for production - Blog - Joyent The Netflix Tech Blog: Evolution of the Netflix Data Pipeline JavaScriptの文化とleftpadの話とpadStartについて - from scratch Your Life Is Tetris.</description>
    </item>
    
    <item>
      <title>Systems Performanceを読んだ</title>
      <link>https://deeeet.com/writing/2016/11/07/systems-performance/</link>
      <pubDate>Mon, 07 Nov 2016 09:25:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/11/07/systems-performance/</guid>
      <description>Brendan Greggによる&amp;ldquo;Systems Performance: Enterprise and the Cloud&amp;rdquo;を読んだ．
Linux（Solaris）のパフォーマンスの分野でBrendan Greggという名前を聞いたことがあるひとは多いと思う．名前を知らなくてもが書いているブログやカンファレンスでの発表資料を見かけたことはあると思う．また彼が開発したFlame Graphにお世話になってるひともいるのではないか（ref. GolangでFlame Graphを描く）．とにかくパフォーマンスに関して常に先端にいるひとである．
そんな彼がSystems（ここでいうSystemsとはCPUやメモリといったハードウェアとKernelやOSといったソフトウェアを指す）のパフォーマンスについて内部のアーキテクチャーを含め徹底的に解説したのが本書である．面白いに決まってる．
本書の根底にある考え方は前書きに書かれているknown-knowns，known-unknownsそしてunknown-unknownsという考え方である（詳しくはKnown unknownsに書いた）．パフォーマンスのチューニングやボトルネックについて考えるとき僕らは何がknownで何がunknownであるかに意識的にならないといけない．このような前提があり本書は各コンポーネントやソフトウェアを低位なレベルから詳細に紐解いていく．
詳細であるから良いというわけではない．僕はLinuxについてより深い理解を得たいと思ったときにO&amp;rsquo;ReillyのLinuxシステムプログラミングや詳解Linuxカーネルといった本に助けを求めた．これらは学びはたくさんあったが淡々と事実が並べられるだけで素直に面白いとは思えなかった（特に詳解&amp;hellip;は全てを読むことはやめてリファレンス的にしか使っていない．それゆえ辞書と呼ばれるのだろう）．
本書が面白いのは「パフォーマンス」という明確なゴールをもった上で低位な部分を解説しているところだと思う．例えばCPUについてアーキテクチャーはこうでスケジューラーはこうなっていてと説明をしながらここがパフォーマンスで問題になるといった説明をする．なぜこれが良いか？を考えた時に結局そこが日々の業務に直結するからだろうと思った．闇雲に深いレベルことを学んでも知識にはなっても業務には生かせない（もちろん大切なことだと思う）．
目次は以下のようになっている．
 Intro Methodology Operating Systems Observability Tools Applications CPUs Memory File Systems Disks Network Cloud Computing Benchmarking Case Study  本書はまずMethodology（方法論）から始まる．これが良い．パフォーマンスの何が難しいって「どこから始めるか？」である．もちろん熟練したひとたちにはそれなりのスタイルなどはあると思う．例えばNetflix（これはBrendanが書いてるが）の Linux Performance Analysis in 60,000 Millisecondsや@y_uuk1くんのLinuxサーバにログインしたらいつもやっているオペレーションがある（これは本書だとTool Methodになるかな）．これらは会社ごとに特化したものであって，よりGeneralなものは明確に言語化されてるとは言いがたいのではないかと思う．だから適当にググってヒットしたコマンドをとりあえず試してみる，といったことになる．方法論は僕のようにパフォーマンスに苦手意識を持っている人間に道筋を与えてくれるものだった．例えばUSE Methodはすぐに役に立っている．
それ以降はOSやCPU，メモリそしてネットワークに関しての詳細な説明が続く．各章の枠組みは一致していて基本的な用語の解説から始まり，コンセプト，アーキテクチャー，そしてそのコンポーネントで使える方法論（具体的なコマンドなど）が説明される．どのコンポーネントも徹底的に解説され非常に勉強になる．本書で特徴的なのはLinuxだけではなくSolarisについても言及されるところだろう．別のOSについての理解/比較は今自分が使っているOSへの深い理解にもつながる（Solarisについては流す程度で読んだけどあのTracabilityは良いなあと思ってしまったよ！）．
そしてCloud Computingの解説もある．これは近年避けられないテーマだ．OS virtualizationとHardware virtulizationそれぞれについて解説し比較が行われる．ホスト側とゲスト側の両方の視点がある．もちろんパフォーマスに関して留意するべきことも紹介される．
Benchmarkingの章は「There are lies, damn lies and then there are performance measures」という言葉から始まる．ベンチマークを行う側，そして読む側が何に気をつければ良いかについて理解できる．「ベンチマークの結果についてその結果の分析に1週間かけていなければそれはきっと間違っている」とまで言い切っている．
最後のCase Studyもとても面白かった．この章は前章で解説してきたことの集大成的な章になっている．ここではBrendannが実際に対応したパフォーマンスの問題について，彼が何を考え，どのような方法論でそれに立ち向かい，誤り，そしてそこからどのように解決に向かったが語られる．ここは結構彼の人となりが出ていて笑いながら読んだ（どんなけDtrace使いたくなってるんだとかね）．熟練のひとが何を考えているのか知れるのは方法論とは違った道筋を与えてくれる．また初心者はこの章を先に読んでも良いと勧められている．本書で解説されるものごとの意義がより伝わりやすくなるからだろう．
まとめ 紙の本で700ページ以上もあり他の本に浮気しつつ読むのに半年以上もかかってしまった．それでも上でべた褒めしたようにどのページをめくっても学びしかなかった．僕はこの本を何度も読み直すと思うし多くのひとに勧めたいと思う．少し高いがそれ以上の価値はあると思う．今後インフラ界隈（DevOpsやSRE）の必読書になっていくでしょう．オススメです．
もともとは@y_uuk1が読んでいたのを見て読み始めた．本当に良い本を紹介してくれてありがとう</description>
    </item>
    
    <item>
      <title>GolangでAPI Clientを実装する</title>
      <link>https://deeeet.com/writing/2016/11/01/go-api-client/</link>
      <pubDate>Tue, 01 Nov 2016 11:42:19 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/11/01/go-api-client/</guid>
      <description>特定のAPIを利用するコマンドラインツールやサービスを書く場合はClientパッケージ（SDKと呼ばれることも多いが本記事ではClientと呼ぶ）を使うことが多いと思う．広く使われているサービスのAPIであれば大抵はオフィシャルにClientパッケージが提供されている．例えば以下のようなものが挙げられる．
 https://github.com/aws/aws-sdk-go https://github.com/Azure/azure-sdk-for-go https://github.com/PagerDuty/go-pagerduty https://github.com/hashicorp/atlas-go  特別使いにくい場合を除けば再実装は避けオフィシャルに提供されているものを使ってしまえばよいと思う（まともなものなら互換性などをちゃんと考慮してくれるはずなので）．一方で小さなサービスや社内のサービスの場合はClientは提供されておらず自分で実装する必要がある．
自分はこれまでいくつかのAPI client パッケージを実装してきた．本記事ではその実装の自分なりの実装パターン（各人にやりかたはあると思う）といくつかのテクニックを紹介する．
Clientとは何か? API ClientとはAPIのHTTPリクエストを（言語の）メソッドの形に抽象化したものである．例えば https://api.example.com/users というエンドポイントからユーザ一覧を取得できるとする．API Clientは具体的なHTTPリクエスト（メソッドやヘッダの設定，認証など）を抽象化し ListUsers()のようなメソッドに落とし込んでその機能を提供する．
なぜ Client を書くべきか? そもそも共通化できることが多いため．それぞれのリクエストは独立していても例えばユーザ名やパスワード，Tokenなどは基本は同じものを使うし，ヘッダの設定なども共通して行える．またテストも書きやすくなる．
いつClientを書くべきか? 複数のエンドポイントに対してリクエストを投げる必要がある場合はClientを書いてしまえばいいと思う．例えば，単一のエンドポイントに決まったリクエストを投げるだけであればClientをわざわざ書く必要はない．自分の場合は3つ以上エンドポイントがあればClientをさっと書いていると思う．
基本的な実装パターン 以下では https://api.example.com （存在しない）のAPI Client パッケージを実装するとする．このAPIでは/usersというパスでユーザの作成と取得，削除が可能であるとする．また各リクエストにはBasic認証が必要であるとする．
パッケージの名前をつける https://golang.org/doc/effective_go.html#package-names
上のEffective Goにも書かれているようにパッケージ名は shortかつconciseかつevocativeのものを選択する．API Clientであればそのサービス名がそのままパッケージ名になると思う．例えば PagerDutyであれば pagerdutyがパッケージ名になる．
名前については以下でもいくつか述べる．
Client（struct）を定義する まずはClient structを実装する．Clientのフィールドにはリクエスト毎に共通に利用する値を持たせるようにする．HTTP APIの場合は例えば以下のようなものが考えられる:
 url.URL - リクエスト毎にパスは異なるがベースのドメインは基本的には共通になる．例えば今回の場合は https://api.example.com は共通である http.Client - 各HTTP リクエストにはnet/httpパッケージのClientを用いる．これは同じものを使い回す 認証情報 - 認証に利用する情報も基本的には同じになる．例えば今回の場合はBasic認証に必要なユーザ名とパスワードは共通である．他にもTokenなどが考えられる log.Logger - デバッグの出力も共通である．自分はグローバルなlogを使うよりも明示的に指定するのを好む  今回の場合は以下のように実装できる．
type Client struct { URL *url.URL HTTPClient *http.Client Username, Password string Logger *log.</description>
    </item>
    
    <item>
      <title>Golangにおけるinterfaceをつかったテスト技法</title>
      <link>https://deeeet.com/writing/2016/10/25/go-interface-testing/</link>
      <pubDate>Tue, 25 Oct 2016 09:26:51 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/10/25/go-interface-testing/</guid>
      <description>最近何度か聞かれたので自分がGolangでCLIツールやAPIサーバーを書くときに実践してるinterfaceを使ったテスト技法について簡単に書いておく．まずはinterfaceを使ったテストの基本について説明し次に自分が実践している簡単なテクニックをいくつか紹介する．
なおGolangのテストの基本については @suzuken さんによる「みんなのGo言語」 の6章が最高なので今すぐ買ってくれ！
前提 自分はテストフレームワークや外部ツールは全く使わない．標準のtestingパッケージのみを使う．https://golang.org/doc/faq#Packages_Testing にも書かれているようにテストのためのフレームワークを使うことは新たなMini language（DSL）を導入することと変わらない．最初にそれを書く人は楽になるかもしれないが新しくプロジェクトに参入してきたひとにはコストにしかならない（Golang以外も学ぶ必要がある）．例えば自分があるプロジェクトにContributeしようとして見たこともないテストフレームワークが使われているとがっくりする．
とにかくGolangだけで書くのが気持ちがいい，に尽きる．
テストとinterface テストという観点からみた場合のinterfaceの利点は何か？ interfaceを使えば「実際の実装」を気にしないで「振る舞い」を渡すことができる．つまり実装の切り替えが可能になる．interfaceを使うことでいわゆるモックが実現できる．
どこをinterfaceにするのか？ interfaceはモックポイントと思えば良い．外界とやりとりを行う境界をinterfaceにする，が基本．外界との境界とは例えばDBとやりとりを行う部分や外部APIにリクエストを投げるClientである．他にも考えられるがとりあえずここをinterfaceにする．
実例 以下では簡単なAPIサーバーを書くとしてinterfaceによるテスト手法の説明を行う．このAPサーバーはDBとしてRedisを使うとする．なおコードは全てpseudoである．
まずはDBのinterfaceを定義する．
type DB interface { Get(key string) string Set(key, value string) error } 次に実際のRedisを使った実装を書く．例えば以下のように書ける．
type Redis struct { // 接続情報など } func (r *Redis) Get(key string) string func (r *Redis) Set(key, value string) error main関数から呼び出すときのことを考えてコンストラクタを実装すると良い（必要な接続情報などが与えられなかった時，もしくは必要な初期化処理に失敗した時にエラーを返せる）．
func NewRedis() (DB, error) ここで重要なのは実際の実装であるRedisを返すのではなくinterfaceのDBを返すこと．サーバー側ではこのinterfaceを使う．
サーバーの実装は以下のようにする．
type Server struct { DB DB } func (s *Server) Start() error ServerはinterfaceのDBを持ち内部の実装（例えばhandlerなど）ではこのinterfaceを利用する．</description>
    </item>
    
    <item>
      <title>sync.ErrGroupで複数のgoroutineを制御する</title>
      <link>https://deeeet.com/writing/2016/10/12/errgroup/</link>
      <pubDate>Wed, 12 Oct 2016 09:36:20 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/10/12/errgroup/</guid>
      <description>Golangの並行処理は強力である一方で同期処理を慎重に実装する必要がある．&amp;ldquo;Go 言語における並行処理の構築部材&amp;rdquo;にまとめられているようにGolangは様々な方法でそれを実装することができる．実現したいタスクに合わせてこれらを適切に選択する必要がある．
この同期処理の機構として新たにgolang.org/x/sync/errgroupというパッケージが登場した．実際に自分のツールで使ってみて便利だったので簡単に紹介する．
使いどころ 時間のかかる1つのタスクを複数のサブタスクとして並行実行しそれらが全て終了するのを待ち合わせる処理（Latch）を書きたい場合にerrgroupは使える．その中でも「1つでもサブタスクでエラーが発生した場合に他のサブタスクを全てを終了しエラーを返したい」（複数のサブタスクが全て正常に終了して初めて1つの処理として完結する）場合が主な使いどころである．
実例 ここでは例として複数のworkerサブタスクをgoroutineで並行実行しそれらすべての終了を待ち合わせるという処理を考える．最初に今までのやりかたとしてsync.WaitGroupを使った実装を，次にerrgroupを使った実装を紹介する．
sync.WaitGroup goroutineの待機処理としてよく使われるのがsync.WaitGroupである．その名前の通り指定した数の処理（goroutine）の実行の待ち合わせに利用する．例えば以下のように書くことができる．
var wg sync.WaitGroup errCh := make(chan error, 1) for i := 0; i &amp;lt; 10; i++ { wg.Add(1) go func(i int) { defer wg.Done() worker(i) }(i) } wg.Wait() 新たなgoroutineを生成する度にAddでWaitGroupをインクリメントし処理が終了したときにDoneを呼ぶ．そして全てのworkerの処理が終了するまでWaitで処理をブロックする．これはchannelを使っても実装できるがsync.WaitGroupを使ったほうが読みやすいことも多い．
ではworkerでのエラーを処理をしたい場合にはどうするのが良いだろうか? sliceでエラーをため終了後にそれを取り出す，errorのchannelを作り外部でそれを受け取るといったパターンが考えられる．何にせよ別途自分で処理を実装する必要がある．
sync.ErrGroup errgroupパッケージを使う以下のように書くことができる．
eg := errgroup.Group{} for i := 0; i &amp;lt; 10; i++ { i := i eg.Go(func() error { return worker(i) }) } if err := eg.Wait(); err != nil { log.</description>
    </item>
    
    <item>
      <title>GolangでAmazon EchoのSmart Home Skillを書く</title>
      <link>https://deeeet.com/writing/2016/08/30/alexa-irkit-ac/</link>
      <pubDate>Tue, 30 Aug 2016 09:56:39 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/08/30/alexa-irkit-ac/</guid>
      <description>Amazon Echo（以下Alexa）はAmazonが開発・販売している音声アシスタント+Bluetoothスマートスピーカーである．音楽を流す，今日の天気やニュースを聞く，Googleカレンダーの予定を聞く，TODOを追加する，家電を操作するなどなど&amp;hellip; といった多くのことを全て音声を通じて実行することができる（こちらの動画がわかりやすい）．
現時点（2016年8月）では音声認識は英語のみで対応地域もUSのみとなっている（例えば天気を聞くと地域を指定しない限りUSの天気が返ってくる）．また連携できるサービスも日本で使えるものは少ない．ただ発表当時から「これは完全に買いだ」と思っており先日GopherCon2016で渡米したときにいきおいで購入した（自分は音声アシスタントはSiriなどのスマートフォンに搭載されているものよりも据え置き型のものに未来を感じている．実は大学院では会話ロボットの研究をしていたのでこの分野には思うことはたくさんある．がそれは別途書く）．
Alexaをしばらく使った感想としてはとにかく音声認識がすごい！ Living roomに置いているがどこから話しても認識してくれる（最悪玄関から「Turn-off AC」と叫んでも認識してくれる）．音声認識の研究をしていた身からして一番驚いたのはAlexaのスピーカーから音楽を流していても認識がまともに働くこと．音楽のシグナルと言葉のシグナルを分離する的な研究はあったがここまで実用的になっているのは正直驚いた．また自分は対話システムにおけるバージイン（割り込み）を研究テーマにしていたことがあるがそちらも完璧に実用的である．Alexaの上に乗るアプリケーションにはもちろん感動するが音声対話システムとしての基礎がものすごくしっかりしていることにとても感動した．
さてAlexaは音楽を流す，天気を聞くといったBuilt-inのSkillに加えてサードパーティが提供するSkillを有効にして機能拡張することができる．そしてSkillは自分で開発することもできる．SkillはAWS LambdaのFunctionとして実装するので現状はLambdaが対応するPython，Node.jsもしくはJavaでの開発が前提となる．がGolangの場合はシングルバイナリをデプロイしてNode.jsから実行するという方法が使えるためGolangも開発の選択肢になる．
今回Golangを使い実用的なAlexa Skillを書いた．本記事ではその実装方法を簡単に紹介する．なおコードは全て https://github.com/tcnksm/alexa-irkit-ac に公開している．
デモ 以下は今回作成したAlexa Skillのデモ動画．自宅のエアコンのON/OFFを行う．ON/OFFのシグナルの送信にはIRKitを使っている．
実装の概要 Alexaの独自Skillの開発にはAlexa Skill Kitを用いる．Skill Kitには以下の2種類がある．
 Custom Skill Smart Home Skill  Custom SkillはよりGeneralなリクエストを受けるのに利用する．例えばWeb Seriviceに情報を問い合わせるやピザを注文するなど．若干冗長な言い回しをしないといけないが自由なワードを認識させる事ができる．Smart Home Skillは家電操作に特化したリクエストを受けるのに利用する．受け付ける言い回しは限定されているがより自然な命令ができる．今回はエアコンの操作なのでSmart Home Skillを利用した（Custom Skillを使ったNode.jsの実装はIRKitの作者の@maaashさんの&amp;ldquo;Amazon Alexaにエアコンをつけてもらう&amp;rdquo;が参考になる）．
Smart Home Skillのリクエストの流れは以下のようになる．
自分で書く必要があるのは4のLambda Functionである．Alexa Serviceからリクエストを受け家電を操作するためのAPI（この場合はIRKitのInternet HTTP API）にリクエストを投げる．
これに加えてSmart Home Skillの場合はOAuth 2.0 Authorization Frameworkを使ったAccount Linkingが必須になる．今回作成したSkillは完全に個人用途なのでAlexa Serviceからのリクエストを最小限でハンドルするシンプルなOAuthサーバーをGoで書いてIBM BluemixにPushして済ませた（実はここが一番めんどくさかった．ドキュメントが不足していたので自分でRFCを読まないといけなかった）．
コードは&amp;ldquo;Amazon Alexa Simple Account Linking Server by Golang&amp;rdquo;に置いた．
GolangでLambda Functionを書く GolangによるLambda FunctionのデプロイとNode.jsとの連携にはapexを利用した．この場合は以下のようなHandleFuncを実装すればよい．
func main() { apex.</description>
    </item>
    
    <item>
      <title>Go1.7のSubtestsとSub-benchmarks</title>
      <link>https://deeeet.com/writing/2016/08/02/go1_7-subtest/</link>
      <pubDate>Tue, 02 Aug 2016 09:00:00 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/08/02/go1_7-subtest/</guid>
      <description>Go1.7ではSubtestsとSub-benchmarksという機能がtestingパッケージに導入される．これを使うとテスト関数/ベンチマーク関数の中にテスト/ベンチマークを定義できるようになる．テストの場合はテストに階層を持たせることができ，ベンチマークの場合はTable Driven的にベンチマークを記述することができるようになる．さらに一連のテスト/ベンチマークに対して共通のsetupとtear-downを持たせることもできる．
テストの場合はTable Driven Testsで十分なことも多く恩恵は少ないかもしれない．それよりもベンチーマークで効果を発揮することが多い．
例えば以下のように異なる設定値を使ってFooのベンチマークをとるとする．今までであればそれぞれ設定値ごとにベンチマーク関数を準備する必要があった．
func BenchmarkFoo1(b *testing.B) { benchFoo(b, 1) } func BenchmarkFoo10(b *testing.B) { benchFoo(b, 10) } func BenchmarkFoo100(b *testing.B) { benchFoo(b, 100) } func benchFoo(b *testing.B, base int) { for i := 0; i &amp;lt; b.N; i++ { Foo(base) } } Go1.7のSub-benchmarkを使うと以下のように書ける．
func BenchmarkFoo(b *testing.B) { cases := []struct { Base int }{ {Base: 1}, {Base: 10}, {Base: 100}, } for _, bc := range cases { b.</description>
    </item>
    
    <item>
      <title>Go1.7のcontextパッケージ</title>
      <link>https://deeeet.com/writing/2016/07/22/context/</link>
      <pubDate>Fri, 22 Jul 2016 09:12:28 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/07/22/context/</guid>
      <description>Go1.7ではgolang.org/x/net/contextがcontextパッケージとして標準パッケージに仲間入りする．そしていくつかの標準パッケージではcontextパッケージを使ったメソッド/関数も新たに登場する．contextパッケージは今後さらに重要な，Gopherは普通に扱うべき，パッケージになると考えられる．本記事ではそもそもcontextパッケージとは何か？なぜ登場したのか？なぜ重要なのか？どのように使うべきか？についてまとめる．
contextパッケージが初めて紹介されたのは2014年のThe Go Blogの記事 &amp;ldquo;Go Concurrency Patterns: Context&amp;rdquo;である．この記事ではなぜGoogleがcontextパッケージを開発したのか，どのように使うのか具体的な検索タスクを例に解説されている．まだ読んだことがない人はそちらを先に読むと良い．
contextパッケージとは何か ここでは具体的な利用例からcontextとは何かを説明する．
例えばGoの典型的な利用例であるWebアプリケーションを考える．Goのサーバにおいてリクエストはそれぞれ個別のgoroutineで処理される．そしてリクエストHandlerは新たなgoroutineを生成しバックエンドのDBや別のサーバにリクエストを投げ結果を得てユーザに対してレスポンスを返す．
このような別サーバへのリクエストのように時間のかかる処理をgoroutineで実行する場合どのようなことに注意する必要があるだろうか．まず最初に注意するべきはその処理に適切なTimeoutやDeadlineを設定して処理が停滞するのを防ぐことである．例えば別のサーバにリクエストを投げる場合にネットワークの問題でリクエストに時間がかかってしまうことは大いに考えられる．リクエストにTimeoutを設定して早めにレスポンスを返しリトライを促すべきである．
次に注意するべきは生成したgoroutineを適切にキャンセルしリソースを解放することである．例えば別のサーバにリクエストを投げる場合に適切なキャンセル処理を行わないとTimeout後もネットワークリソースが使われ続けることになる（CPUやメモリを使い続けるかもしれない）．この場合net/httpパッケージレベルでリクエストをキャンセルするべきである．
さらにそのgoroutineは別のgoroutineを呼び出しそれがまた別の&amp;hellip;と呼び出しの連鎖は深くなることが考えられる．その場合も親のTimeoutに合わせてその子は全て適切にキャンセルされリソースは解放されるべきである．．
このようにキャンセル処理は重要である．contextパッケージはこのキャンセルのためのシグナルをAPIの境界を超えて受け渡すための仕組みである．ある関数から別の関数へと，親から子へと，キャンセルを伝搬させる．
これはcontextを使わなくても実現できる．しかし標準パッケージになったことでcontextは「キャンセルのためのシグナルの受け渡しの標準的なインターフェース」として使える．この流れは別の標準パッケージに新たに追加された関数に見ることができる．
（後述するがcontextパッケージは限定されたスコープの値，例えば認証情報など，の受け渡しとしても利用できる．しかし筆者はこれは付随的な機能でありキャンセル機構としてのcontextの方が重要であると考えている）
コードで追うcontextパッケージ 言葉のみでは伝わりにくいので具体的なサンプルコードを使ってcontextパッケージの使いどころを説明する．
以下のような単純なリクエストHandlerを考える．このHandlerはユーザからのリクエストを受けバックエンドのサービスにリクエストを投げる．そして得た結果をユーザに返す（具体的なレスポンスの書き込みなどは省略している）．リクエストは別のgoroutineで投げ，エラーをchannelで受け取る．このコードを改善していく．
func handler(w http.ResponseWriter, r *http.Request) { // 新たにgoroutineを生成してバックエンドにリクエストを投げる  // 結果をerror channelに入れる  errCh := make(chan error, 1) go func() { errCh &amp;lt;- request() }() // error channelにリクエストの結果が返ってくるのを待つ  select { case err := &amp;lt;-errCh: if err != nil { log.Println(&amp;#34;failed:&amp;#34;, err) return } } log.Println(&amp;#34;success&amp;#34;) } まず現状のコードはネットワークの問題などでrequest()に時間がかかりユーザへのレスポンスが停止してしまう可能性がある．これを防ぐためにはTimeoutを設定するべきである．timeパッケージのtime.Afterを使うと以下のようにTimeoutを設定することができる．
func handler(w http.</description>
    </item>
    
    <item>
      <title>GopherCon 2016でLTした</title>
      <link>https://deeeet.com/writing/2016/07/12/gophercon2016-lt/</link>
      <pubDate>Tue, 12 Jul 2016 19:04:47 -0600</pubDate>
      
      <guid>https://deeeet.com/writing/2016/07/12/gophercon2016-lt/</guid>
      <description>GopherCon 2016でLTをした．@tenntennさんがやった通常トーク（50分）はなかなかハードルが高いがLTは初めの一歩として良いと思う．来年もDenverで再び開催されることがアナウンスされているので来年以降に発表するひとのためにどんな感じだったかを簡単に書いておく．
モチベーション 発表スライドを見てもらえばわかるが特に新しい話をしたわけではない．日本のミートアップなどで話したこと，ブログに書いたことを英語にしただけにすぎない（ただ実演デモをするという挑戦はした）．
「大御所たちと同じステージで喋る機会を逃すのはもったいない」（ちなみに当日のLTは僕の次がRobert Griesemer氏でその次がBrad Fitzpatrick氏だった！），「日本のGo界隈にこんなやつおるでってのを知ってもらいたい」というモチベーションで発表した．あとなんとなく自分の中でここでぶっ込まないと一生逃げると思ったのもある（通常セッションにしろやって話だが50分喋る良いネタがなかった..）．
流れ まずLTセッションの募集は会議開催の10日前ほどにアナウンスされた（&amp;ldquo;GopherCon 2016 - Lightning Talk Annoucement&amp;rdquo;）．逃さないためにはtwitterの@GopherConやGophers slackをちゃんとウォッチしておくと良い．
CFPはPaperCallで行われた．タイトルや発表内容をちゃんと書く．
結果の発表は開催前日に，発表日は通常会議の初日に，発表順は当日その場で発表された．そのため資料の準備と練習の時間はほとんどない．飛行機での移動中などに形だけ資料を完成させ会議の合間に練習するしかない．また事前にディスプレイの接続チェックなどはできないので特殊なことはしないほうがよい．
発表場所は上の写真のメインルーム．発表時間は6分で，質疑応答は次の発表者の準備が完了するまで行われた．
LTセッションは通常会議が行われた2日間両方で行われ，発表人数はそれぞれ12名だった．他の発表や自分の発表を考えると採択率は高いと感じた（直前なので申し込む人が少なかったのかもしれない）．
この流れは来年変わるかもしれないし変わらないかもしれない．もし来年移行挑戦する人がいればぜひ参考にしてください．
まとめ 偉そうに書いたが採択されてからひたすら緊張し「聴衆として普通に楽しむだけにすればよかった」と何度も思った．ただ終わってみればやってよかったという気持ちしかない（こういうリプライもらえたり，終わった後に議論できて良かった）．機会があるひとはどんどん挑戦しましょう．
ちなみに通常セッションのトークは以下が最高だったのでビデオが公開されたら全部観ましょう．
 &amp;ldquo;Understanding nil&amp;rdquo; &amp;ldquo;Navigating Unfamiliar Code with the Go Guru&amp;rdquo; &amp;ldquo;Go for Data Science&amp;rdquo; &amp;ldquo;Visualizing Concurrency in Go&amp;rdquo; &amp;ldquo;Go for Crypto Developers&amp;rdquo; &amp;ldquo;Inside the Map Implementation&amp;rdquo; &amp;ldquo;Go Without the Operating System&amp;rdquo; &amp;ldquo;The Design of the Go Assembler&amp;rdquo; &amp;ldquo;cgo: Safely Taming the Beast&amp;rdquo;  </description>
    </item>
    
    <item>
      <title>Golangの新しいGCアルゴリズム Transaction Oriented Collector（TOC）</title>
      <link>https://deeeet.com/writing/2016/06/29/toc/</link>
      <pubDate>Wed, 29 Jun 2016 10:31:00 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/06/29/toc/</guid>
      <description>http://golang.org/s/gctoc
Goの新しいGCのProposalが出た．まだProposal段階であり具体的な実装はないが簡単にどのようなものであるかをまとめておく．
GoのGCはGo1.5において単純なStop The World（STW）からConcurrent Mark &amp;amp; Sweepへと変更され大きな改善があった（詳しくは&amp;ldquo;GolangのGCを追う&amp;rdquo;に書いた）．先の記事に書いたようにGo1.5におけるGCの改善は主にレイテンシ（最大停止時間）に重きが置かれいた．数値目標として10msが掲げられGo1.6においては大きなヒープサイズ（500GB）においてそれを達成していた．
GCの評価項目はレイテンシのみではない．スループットやヒープの使用効率（断片化の対処）なども重要である．Go1.6までのGCではそれらについて大きく言及されていなかった（と思う）．例えばスループットに関してはハードウェアの進化がそれを改善するはずであるという前提が置かれていた（&amp;ldquo;Go GC: Prioritizing low latency and simplicity&amp;rdquo;）．
今回提案されたTransaction Oriented Collector（TOC）アルゴリズムはGCのスループットを改善するものである．
TOCアルゴリズムの経験則 Transaction Oriented Collector（TOC）アルゴリズムは「あるTransactionで生成されたオブジェクトはTransactionが終了すると同時にすぐ死ぬことが多い」という経験則に基づくアルゴリズムである．ここでいうTransactionとはいわゆるACIDにおける不可分な処理単位ではなく，Webサービスなどでリクエスト受けてレスポンスを返すまでの一連の処理を示す．
この仮定はGenerational GC（世代別GC）が利用している「多くのオブジェクトは生成されてすぐにゴミとなりわずかなオブジェクトだけが長く生き残る」という経験則に似ている．TOCアルゴリズムはこの経験則のGoなりの再解釈のようにも見える．
このTOCアルゴリズムの経験則はどこから来たか? Goが多くサポートしているCloudアプリケーションである．このようなアプリーションは，他のネットワークや他のGoroutineからメッセージを受け，それをUnmarshalし，それを使い計算をし，結果をMarshalし，それを他のネットワークやGoroutineに投げる．そしてそのGoroutineは死ぬか他のリクエストを受けるために停止状態になる．
リクエスト中での計算では大きなヒープからデータを読み込むことはあるかもしれないが典型的には書き込みは滅多に起きずヒープはTransaction間で一定になる．そしてGoroutine内で新たにアロケートしたオブジェクトは他のGoroutineに共有される（publish）かもしれないし共有されない（local）かもしれない．TOCアルゴリズムはこの共有されない場合の観測結果を使う，つまり「もしGoroutineがその中でアロケートしたオブジェクトを共有しない場合，そのオブジェクトはGC時に到達不可能になり関連するメモリ領域はすぐにアロケートできる」である．
TOCアルゴリズムの恩恵を受けるのはnet/http#Serverやnet/rpc#Serverを使ったアプリケーションであると想像できる．
TOCアルゴリズムの実装の提案 TOCアルゴリズムの実装はProposalのExamplesをみるとわかりやすい．
（まず前提としてGoのGCのMarkはBitmapで管理されている．BitmapはオブジェクトのヘッダにMarkbitを持たせるのではなく関連するメモリ領域をBitのテーブルとして別で集中管理する手法である．これはCopy-On-Writeとの相性が良いなどがある）．
TOCアルゴリズムでは各Goroutineは2つのPointerをもつ．1つはCurrent Sweep Pointerである．このPointerはどこまでSweepを行ったか（Allocateしたか）を示す．もう1つはInitial Sweep Pointerである．これはそのGoroutine開始時のSweep Pointerを示す．この2つのPointerの間のオブジェクトはMarkされていようがMarkされていまいが「そのGoroutineで新たにアロケートされたオブジェクト」となる．そしてMarkされていないオブジェクトは共有されていない（Publishされていない）オブジェクトであるとする．
これをどのように実現するか? ライトバリア（Write barrier）を使う．このライトバリアはそのGoroutine内で新たにアロケートされたオブジェクトがInvariantであることを保証する．つまりそのオブジェクトが他に共有されればMarkをつける．
10011110010100101010100001001011010010110100101001011101010111101 ^ &amp;lt;- before ^ after -&amp;gt; Initial Sweep Pointer Current Sweep Pointer （Proposalの図を拝借させてもらった．1は前回のGCで到達可能であったオブジェクト，もしくはGoroutineで新たにアロケートされそしてPublishされたオブジェクトである．BeforeとInitialの間にある0はアロケートされたがPublishされていないオブジェクトである．Afterにある0はまだアロケートされていないオブジェクトである）
あとはGoroutine終了時にCurrent Sweep PointerをInitial Sweep Pointerへと戻せば良い．新たにオブジェクトが生成されていようとそれが共有されていなければMarkは立っていないので，次回のGCサイクルを待たずに次回のSweepにおいてアロケートの対象になる．
まとめ 簡単にGoの新たなGCのProposalを追ってみた．今後の実装とそれによる効果がどうなるかが楽しみである．</description>
    </item>
    
    <item>
      <title>GolangでFlame Graphを描く</title>
      <link>https://deeeet.com/writing/2016/05/29/go-flame-graph/</link>
      <pubDate>Sun, 29 May 2016 14:22:17 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/05/29/go-flame-graph/</guid>
      <description>アプリケーションのパフォーマンス問題の解決やチューニングで大切なのは問題のコアやボトルネックに最短パスで到達することである．
基本的なパフォーマンス分析の入り口はアプリケーションのスレッドがon-CPUで時間を消費しているかoff-CPUで時間を消費しているかを理解するところから始まる．on-CPUの場合はそれがuserモードかkernelモードかを特定し，さらにCPUプロファイリングによってどのcode pathがCPUを消費しているのかの分析に向かう．off-CPUの場合はI/OやLock，pagingといった問題の分析に向かう．
Flame Graphはon-CPUでのパフォーマンスの問題が発覚した時に行うCPUプロファイリングを助ける．どのcode pathがボトルネックになっているのかを1つのグラフ上で理解できる．本記事ではFlame Graphとは何か? なぜ必要なのか? を解説しGoのアプリケーションでそれを用いるために方法を解説する．
Flame Graphとは何か? Flame GraphはCPUプロファイリング結果をvisualizeしたグラフである．元Joyent，現NetflixのBrendan Gregg氏によって開発された．例えば以下はMySQLのCPUプロファイリング結果をFlame Graphで描画したものである．
CPU プロファイリング CPUプロファイリングの共通のテクニックはStack traceのサンプリングである．Stack traceというのは関数コールのリストで，code pathの先祖を追うことができる．例えば，以下はGolangのstack traceで子から親へStackと辿ることができる．
syscall.Syscall syscall.write syscall.Write os.(*File).write os.(*File).Write log.(*Logger).Output log.Printf Flame Graphの初期衝動 CPUプロファイリングの出力は往々にしてverboseである．例えば，Brendan Gregg氏がFlame GraphをつくるきっかけとなったプロダクションのMySQLのプロファイリングの出力は500,000行近くもあったという（参考画像&amp;hellip;やばいw）．
Flame Graphはそのような膨大なCPUプロファイリングを一つのグラフ上で直感的かつ簡単に理解するために開発された．
Flame Graphの読み方 以下はFlame Graphを単純化したものである．
 Stack traceは長方形のボックスの列で表現される．1つ1つのボックスは関数（Stack frame）を示す y軸はStackの深さを示す．一番上のボックスはStack traceが収集された時にon-CPUであった関数であり，その下にあるボックスはすべて先祖になる．あるボックスの下にあるボックスはその関数の親である（高いほど悪いわけではない） x軸はその関数のSampleの割合を示す．時間ではない．それぞれの関数はアルファベット順にソートされているだけ それぞれのボックスの幅はその関数の出現頻度を示す（長いほどStack trace中に多く登場したこと意味する） 色には特に意味はない  ではこのFlame Graphからどのようなことがわかるか?
 Q. 最もon-CPUだったのはどの関数か?  A. g()（グラフの一番上を見れば良い）   Q. なぜg()はon-CPUなのか?  A. a() -&amp;gt; b() -&amp;gt; c() -&amp;gt; e() -&amp;gt; f() -&amp;gt; g()（y軸を見る）   Q.</description>
    </item>
    
    <item>
      <title>Known unknowns</title>
      <link>https://deeeet.com/writing/2016/05/24/known-unknowns/</link>
      <pubDate>Tue, 24 May 2016 08:07:56 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/05/24/known-unknowns/</guid>
      <description>&amp;ldquo;Systems Performance: Enterprise and the Cloud&amp;rdquo; をずっと読んでいる．この本はNetflixのBrendan Gregg氏がJoyent時代に書いた本である．その名の通りLinux（とSolaris）のシステムのパフォーマンスの本である（とにかく一つ一つが丁寧かつ深く解説されておりページをめくるごとに学びしかないのでパフォーマンスに関わるひとは今すぐ読むと良い）．
この本で一貫して現れてくる，通底するのが，known-knowns，known-unknownsそしてunknown-unknownsという概念である．元ネタはDonald Rumsfeld 氏の会見でのコメントだが（cf. There are known knowns），複雑なシステムのパフォーマンスの重要な原則を集約している．良い概念なので簡単に紹介する．
それぞれをパフォーマンスの観点から説明すると以下のようになる．
 known-knowns - 知っていること．そのパフォーマンスのメトリクスをチェックするべきことを知っているし，現在の値も知っている．例えば，CPUの利用率をチェックするべきことを知っているし，その平均的な値が10%であることも知っている known-unknowns - 「知らないこと」を知っていること．そのパフォーマンスのメトリクスをチェックできること，そのようなサブシステムが存在してることを知っているが，まだそれらを観測したことがない（知らない）．例えば，profilingによって何がCPUを使いまくっているのかチェックできるのを知っているけどまだそれを実施してない． unknown-unknowns - 「知らないこと」を知らないこと．例えば，デバイス割り込みがCPUを多く消費することを知らず，そのためチェックしてないかもしれない．  パフォーマンスというのは「知れば知るほど知らないことが増える」という分野である．システムについて学べば学ぶほど，unknown-unknownsに気づき，それはknown-unknownになり，次回からはそれをチェックできるようになる．
そしてこれはパフォーマンスに限った話ではない．</description>
    </item>
    
    <item>
      <title>GolangのGCを追う</title>
      <link>https://deeeet.com/writing/2016/05/08/gogc-2016/</link>
      <pubDate>Sun, 08 May 2016 23:01:06 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/05/08/gogc-2016/</guid>
      <description>Go1.5とGo1.6でGoのGCのレイテンシが大きく改善された．この変更について「ちゃんと」理解するため，アルゴリズムレベルでGoのGCについて追ってみた．
まずGoのGCの現状をパフォーマンス（レイテンシ）の観点からまとめる．次に具体的なアルゴリズムについて，そして最後に実際の現場でのチューニングはどうすれば良いのかについて解説する．
GoのGCの今 最初にGoのGCの最近の流れ（2016年5月まで）をまとめる．
Go1.4までは単純なStop The World（STW）GCが実装されていたがGo1.5からは新たなGCアルゴリズムが導入された．導入の際に設定された数値目標は大きなヒープサイズにおいてもレイテンシを10ms以下に抑えることであった．Go1.5で新たなアルゴリムが実装されGo1.6で最適化が行われた．
以下は公開されているベンチマーク．まずはGo1.5を見る．
GopherCon 2015: Rick Hudson - Go GC: Solving the Latency Problem
グラフの横軸はヒープサイズで縦軸はレイテンシである（小さいほどよい）．以前のバージョンと比較するとヒープの増加に伴ってレイテンシが3.0sを超えていたのがほぼ0sに抑えらているのがわかる．コミュニティからも以下のようなベンチマークが公開されている．
 https://twitter.com/brianhatfield/status/634166123605331968 Billions of request per day meet Go 1.5 (The new version of Go reduces our 95-percentile garbage collector from 279 milliseconds down to just 10 ms)  次に以下はGo1.6のベンチマーク．
QCon: Go GC: Prioritizing Low Latency and Simplicity
縦軸と横軸はGo1.5と同じ．まずGo1.5のグラフと比べると10倍のヒープサイズでベンチマークが行われているのがわかる．Go1.5が50GBに達する前にレイテンシが増大しているのに対してGo1.6は250GBのヒープに対しても10msのレイテンシで抑えらているのが確認できる．
Go1.7のリリースが近いが，既に今までと同じくTwitterの@brianhatfield氏がCanaryテストを行い，さらにGCのレイテンシが改善されたことが報告されている．
 Go 1.7 observed performance changes (production canary@eeca3ba)  これらのアップデートからGoにおいてGCのレイテンシは大規模プロダクション環境においても全く問題にならないレベルになっていることがわかる．つまりパフォーマンスに問題があったときに疑うべき場所としては優先度は低いと言える．</description>
    </item>
    
    <item>
      <title>Golangのエラー処理とpkg/errors</title>
      <link>https://deeeet.com/writing/2016/04/25/go-pkg-errors/</link>
      <pubDate>Mon, 25 Apr 2016 09:00:22 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/04/25/go-pkg-errors/</guid>
      <description>GoConでは毎回エラー処理について面白い知見が得られる．Go Conference 2014 autumn においては（実際のトークではないが）居酒屋にて@JxckさんがRob Pike氏から以下のようなテクニックを紹介してもらっていた．
 Errors are values - The Go Blog Golang Error Handling lesson by Rob Pike  これはWrite（やRead）のエラー処理が複数続く場合にerrWriter を定義して複数のエラー処理を一箇所にまとめてコードをすっきりとさせるテクニックであった．
そして今回の Go Conference 2016 spring のkeynoteにおいてもDave Cheney氏から（僕にとっては）新たなエラー処理テクニックが紹介された．
 Gocon Spring 2016  実際に使ってみて/コードを読んでみて（飲み会でもコードとともにいろいろ教えてもらった）自分の抱えている問題を解決できそうで使ってみたいと思えた．
本記事では現在のエラー処理の問題と発表で紹介されたpkg/errorsについてまとめる．なお上記のスライドにはトークノートも書かれているので具体的な内容はそちらを見るのが良い．
問題 @Jxckさんのケースは1つの関数において複数のエラーハンドリングが煩雑になる，言わば縦方向のエラー処理の問題であった．Dave氏のトークで語られているのは深さ方向のエラー処理の問題である．大きく分けて2つの問題がある．
 最終的に表示されるエラーメッセージ 特定のエラーに対する分岐処理  以下ではそれらを具体的に説明する．
エラーメッセージ まずはエラーメッセージについて．以下は基本的なGoのエラー処理である．
func Foo() error { conf, err := ReadConf() if err != nil { return err } ... return nil } Foo()がReadConf()を呼び，ReadConf()がエラーを返せばそれをerrとして返し，そうでなければconfをつかった処理を続行し問題がなければnilを返す．
大きなパッケージやツールになるとこの定型的な処理はどんどん連なり深くなる．例えばこの例の場合はReadConf()がさらにWrite()といった標準パッケージの関数を呼びそのエラーを返すかもしれないし，Foo()は別の関数から呼ばれその中でエラーが処理されるかもしれない．
これらの一連のエラーは最終的にどうなるか? コマンドラインツールやWebサーバーのmain()に戻り以下のようにfmt.</description>
    </item>
    
    <item>
      <title>2015年振り返り</title>
      <link>https://deeeet.com/writing/2015/12/31/2015/</link>
      <pubDate>Thu, 31 Dec 2015 23:19:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/31/2015/</guid>
      <description>2015年の振り返りとして自分が好きだったもの，影響を受けたものを雑多にまとめる．それに合わせて自分の活動についても振り返り，2016年の展望を書く（fogus: Send More Paramedicsの形式が良かったのでそれを真似ている）．
Blog posts read 今年読んで印象に残った，影響を受けたブログ記事．順不同．
Japanese  コードを書くことは無限の可能性を捨てて一つのやり方を選ぶということ 7年働いた時点での私の仕事の極意 志低く ソフトウェアエンジニアだけでサービス運用できる環境を作って失業した話 食べログの口コミに見る人間心理 -麻薬と性とトラウマと- 運用を楽にするためのアプリケーションコードを書くということ Webオペレーションエンジニアのアウトプットと開発力 我慢の期間 2015年Webサーバアーキテクチャ序論 A Million Hello Worlds 技術者が研究者のように論文を書くメリットはあるか OSS開発の活発さの維持と良いソフトウェア設計の間には緊張関係があるのだろうか? 持続的なプラットフォームのための難しい決断 翻訳は/誰がやっても/間違える 下から目線のコードレビュー  English  CoreOS Co-Founder Alex Polvi Talks Containers, Rocket vs. Docker, and More HTTP/2 is Done Software engineers should write After Docker: Unikernels and Immutable Infrastructure My Philosophy on Alerting Mitchell Hashimoto: Containers or No Containers? That is One Question for 2015 What makes a cluster a cluster?</description>
    </item>
    
    <item>
      <title>Go言語でファジング</title>
      <link>https://deeeet.com/writing/2015/12/21/go-fuzz/</link>
      <pubDate>Mon, 21 Dec 2015 00:25:30 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/21/go-fuzz/</guid>
      <description>この記事はGo Advent Calendar 2015の21日目の記事です．
今年もGoコミュニティーから多くのツールが登場した．その中でも異彩を放っていたのがGoogleのDynamic testing toolsチームの@dvyukov氏によるgo-fuzzである．
go-fuzzはGo関数のファジングを行うツールである．このツールはとても強力で標準パッケージで100以上，golang.org/x/パッケージで40以上，その他を含めると300以上のバグを発見するという実績を残している（cf. Trophies）．
本記事ではこのgo-fuzzの紹介を行う．
ファジングとは?  Fuzz testing - Wikipedia, the free encyclopedia ソフトウェアの脆弱性検出におけるファジングの活用  「ファジング」とはソフトウェアのテスト手法である．テスト対象となるソフトウェアにランダムなデータを大量に入力し意図しない挙動を検出する．
普通のソフトウェアは予期しないデータを受けても適切な処理，例えばエラーを返すなど，を行う．そしてそれはテストされる．しかし予期しない入力をすべてテストすることは難しい．適切に処理しているつもりであっても予期しないデータによりソフトウェアがクラッシュしてしまうことはありうる．このようなテストでファジングは光る．大量のランダムデータを入力し予期しないクラッシュを見つける．
ファジングの利点に以下が挙げられる．
 チープである バイアスがない  まずファジングは単純にランダムなデータを放り込むだけなので非常にチープな手法である．使うだけなら特別な知識は必要ない．次にランダムであるためテスターのバイアスがない．そのソフトウェアをつくっているひとほど思い込みが強くなってしまう（と思う）が，そのようなバイアスを排除することができる．
ファジングで入力となるデータは「ファズ」と呼ばれる．コマンドラインツールであれば引数や環境変数，ウェブサーバーであればHTTPリクエストである．ファジングではこのファズをいかに生成するのかが重要になる．完全にランダムにする，指定の範囲内で連続に値を変化させる，正常なデータの一部を変更させる．ある特定の制御文字列を対象にするといった手法がある．
go-fuzzとは?  dvyukov/go-fuzz GopherCon 2015: Dmitry Vyukov - Go Dynamic Tools (Slide)  Go言語の関数に対してファジングを行うために開発されたのがgo-fuzzである．go-fuzzはC/C++のafl-fuzzがベースになっている．
go-fuzzは完全にランダムなデータを入力するのではなく，正常なデータの一部を変更させランダムなデータを生成する．これにより単純にランダムな値で盲目的にテストをするのではなく，ある程度「ありそうな」データでテストを行うことができる．このためのデータセットをcorpusと呼び，go-fuzzはテストを繰り返しながらこのcorpusを成長させていく．
corpusはどのように成長するのか? go-fuzzはASTを使い対象関数のテストのカバレッジ情報を取得する．カバレッジを上げるような入力が得られればそれをcorpusに登録する．これによりテストはより網羅的になる．
corpusは事前に与えることもできる．例えば対象とする関数の入力が画像データである場合は事前に幾つかの画像データを与えることができる．もしくはユニットテストなどで既にテストしている値を使うこともできる．
入力を繰り返し意図しない挙動が得られる（例えばpanicが起こる）とgo-fuzzはそれを引き起こした入力とスタックトレースをファイルとして保存する．開発者はその結果をもとに新たにユニットテストを追加しコードを修正していく．
使いかた go-fuzzによるファジングには以下の2つが必要である
 Fuzz()関数の準備 go-fuzz-buidとgo-fuzzの実行  まずFuzz()関数は以下のような関数である．
func Fuzz(data []byte) int dataはgo-fuzzによって与えられるランダムな値である（ほとんどはinvalidな値である）．そしてこの値をテストしたい関数に入力として与える．go-fuzzはこの入力で関数がpanicしたりクラッシュしたり，メモリを割り当てすぎてhangしないかを監視する．
Fuzz()の返り値はcorpusの作成に使われる．以下の3つの値のうちどれかを返す．
 1- 入力がふさわしいデータであると考えられる場合（例えば関数がエラーを返さずに正常に処理された場合その入力はその関数にとってふさわしい入力であると考えることができる．ここから新たなランダムな値を生成すれば新たなエラーを発見できる可能性が高い） -1 - 入力がカバレッジを上げるようなふさわしい入力であると考えられてもcorpusには追加したくない場合 0 - 上記以外の場合（例えばエラーが返った場合）  関数が書けたら以下で専用のバイナリをつくる．zip形式で出力される．</description>
    </item>
    
    <item>
      <title>自宅で美味いコーヒーを淹れる</title>
      <link>https://deeeet.com/writing/2015/12/17/coffee-2015/</link>
      <pubDate>Thu, 17 Dec 2015 00:35:03 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/17/coffee-2015/</guid>
      <description>この記事はコーヒー Advent Calendar 2015の17日目の記事です．
コーヒーを淹れること，豆を買いに行くこと，コーヒー器具を集めること，コーヒー関連の本を読むことが好きだ．コーヒーは趣味といっても過言でなない．自宅で美味しいコーヒーを淹れるために今までいろいろ試行錯誤してきたが，最近ある程度固まってきたのでその環境についてまとめてみる．
過去 最初に自分とコーヒーとの馴れ初めをつらつらと．
親がコーヒー好きなので実家では当たり前のように毎日コーヒーが淹れられていた．そのため家で自分でコーヒーを淹れて飲むのは当たり前のものとして育った．実家ではドリップマシンが使われていた．特にこだわりはなく出されるものをそのまま飲んでいたと思う．
自分でコーヒーを淹れるようになったのは大学生で一人暮らしを始めてから．最初は実家にあった使われていないドリッパー（確かHARIO）と近所のスーパーの安い豆でドリップを始めた．見よう見まねでなんとなくやっていたと思う．大学生にもなるとカフェなどでまともなコーヒーを飲むようになり，自分で淹れるコーヒーがあまり美味しくないと感じ始めた．
どうやら自分で豆を挽くと美味いということを聞きつけポーレックスのコーヒーミルを買い，近所のKALDIで豆を買い，手挽きによる豆とドリップを始めた．
こうなってくると良いドリッパーも欲しくなる．いろいろ探してChemexを買った．Chemexは未だに使っているので5年以上の付き合いになる（よくおしゃれインテリア的な感じで使ってるやついるけどああいうやつはフィルターが買えなくなって最終的に花瓶として使い始める．映画&amp;ldquo;インターステラー&amp;rdquo;では水飲み用のデキャンタとして使われていて映画の評価に響いた）．
手挽きはとにかく失敗した．特にグラインドが粗すぎて青臭くなってしまうことが多かった．また手挽きは時間がかかるため平日は厳しくて週末しかできないとう問題があった．そのため在学中は平日はお店で挽いてもらった豆でドリップし，週末に手挽きでドリップをした．ドリップは自己流でやっていて日によってばらつきがあるもののある程度まともなものが淹れられるようにはなった
現在 過去のコーヒー環境には以下の問題があった．
 手挽きをしていたため平日に自分で豆を挽くことができないこと 自己流ドリップで同じ豆でも味が固定されないこと 良質な豆を使っていないこと  まず1つ目は社会人になり財力で解決した．必要なのは良質な電動グライダー．コーヒーは一生付き合うと思いKalitaのナイスカットミルを購入した．これは最高でグラインドは綺麗に均等になるしあらゆる淹れ方（ドリップ，エスプレッソ，フレンチプレス）にあったグラインドに簡単に調整できる．何より速い．忙しい朝でも新鮮な豆を挽くことができる．
2つ目の再現性の問題．これは本を読み，またサードウェーブの流れに触れることで多くを学び解決した．これについては以下で詳しく書く．
3つ目の問題はそもそも問題と認識できていなかった．これもサードウェーブの文化に触れることで学んだ．東京にもサードウェーブの流れを汲んだカフェはたくさんある（cf. 東京サードウェーブコーヒー）．KALDIの豆は今でも買う（特にリッチブレンドが大好き）が時間があればロースターに行き新鮮な豆を買うようになった．サードウェーブの原点であるオークランドが対岸にあるサンフランシスコを訪れたときは時間があればロースターに行き，コーヒーを飲み，豆を買うなどした（cf. サンフランシスコでたくさんコーヒー飲んだ）．
良い豆? ちょっと脱線するが，良い豆って何? あるいはサードウェーブと今までの豆の違いって何? という話を．
昔ながらのコーヒーに深煎りのものが多い．それは昔は豆が熟度など不完全なものしか入ってこなくてその欠点を焙煎で消そうとしていたため．
近年はクオリティの高い豆が流通するようになった．特にサードウェーブってのは豆そのもの味を楽しもうって流れで浅煎が多い．そして素材そのものを楽しむのをよしとする．同じ農園の豆の味でも年ごとに違うし，同じ品種でもテロワール（生産地の地理/地勢/気候/土壌などの特徴）で異なってくる．今まではそれを隠そうとしたが，今はそれを楽しもうとしている．
農家に対する配慮に関してもよく語られる．これはコーヒーの美味しさは実が果実として完熟しているかどうかで決まるため．かつそれは木になっているときにしか判断できないから．良好な関係を築くことでより良い栽培/収穫方法のループを回せるようにする．スタバとかが批判されるのは同じ味を世界中で実現するために農家無視で大量生産してるから．
もちろん今でも昔ながらの深煎りも好きで全然飲むけど．
以下では最近はどのような器具を使っているのか? 再現性を高めるにはどうしているのか? などについて書く．
器具 もともとドリップ至上主義でコーヒーはドリップ，ドリップ以外は認めない，ドリップ以外は飲むに値しないという過激な立場を取ってきた．しかし豆にはその豆にあった淹れ方があることを知り，今ではこの考えを改めていろいろな抽出方法を使うようになった（メインはドリップだが）．
簡単に現在使っている器具を紹介する．
左から順番に
 Chemex - Chemexを愛して5年．これからも使い続けると思う．Chemexは専用のフィルターが必要でそれによってChemexっぽい味が出る．KALDIの豆によく使う．最近は人に淹れることもあるので6カップの購入も考えている． Donuts Dripper - Chemex以外のドリップもしようと2年ほど前に購入．しっかり濃いのに重くないスッキリした飲み心地になるようにデザインされており実際そんな感じになる．市販の安いフィルターが使えるもの良い． BIALETTI モカポット - エスプレッソマシンなど買えない．圧力はマシンに及ばないがそれに近しいものは淹れることができる．ど濃いコーヒーを飲みたい時に良い．イタリアにはどの家庭にもあるらしい． AeroPress - 素早く淹れられて洗うのも簡単．使い方次第でいろいろな淹れ方ができて面白い（大会もあるとか）．これは買ったばかりでまだうまく使えていないのでこれからもっとうまくなりたい． HARIO フレンチプレス - なかなか難しいが面白い味のコーヒーを作れる．友人からプレゼントしてもらった．  ちなみにカップはFour Barrel Coffeeのもの．ここで飲んだコーヒーが今まで飲んだなかで一番美味しかった．
再現性 過去の自分のコーヒーの淹れ方には同じ豆を使っているのに再現性がないという問題があった．それもそもはずで計測などしないで感覚でやっていたのが原因．再現性への回答は計測すること．
抽出の味のファクターは豆の量，グラインドの細かさ，抽出時間，お湯の量，お湯の温度である．HARIOのドリップスケールにははかりとタイマーが付いている．これは最高のツールで豆の量とお湯の量，抽出時間を計測しながらコーヒーを抽出することができる．これを導入したことで再現性は飛躍的に高まった．
使い方を簡単に説明する．まず以下のように豆の量を測る．
次に抽出．以下はChemexを使った例．抽出しながら，時間とお湯の量を測ることができる（温度計は別途購入した）．
豆の量や抽出時間はどこから知るのか? サードウェーブ系のロースターでは豆を買う時に聞けば大抵どこでも教えてくれる．場所によっては豆のグラインドのサンプルもくれる．簡単な説明書みたなのを準備してるところもある．新鮮な豆とそれを淹れるためのレシピを聞けば自宅でそれを再現することは容易い（しかもロースターで飲むと1杯500円くらいする．豆は100gあたり800円-1000円の価格帯になる．1杯あたりは10-15gの豆を使うので自分で淹れれば1杯100-150円程度になる．つまり自分で淹れたほうが断然良い）．
この手法でドリップであれば5分程度で最高の味を再現できるようになった．</description>
    </item>
    
    <item>
      <title>Go言語でLet&#39;s EncryptのACMEを理解する</title>
      <link>https://deeeet.com/writing/2015/12/01/go-letsencrypt-acme/</link>
      <pubDate>Tue, 01 Dec 2015 23:18:42 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/01/go-letsencrypt-acme/</guid>
      <description>Let&amp;rsquo;s Encrypt
TL;DR Let&amp;rsquo;s EncryptのベースのプロトコルであるACMEを理解する．
まずACMEをベースとしたCAであるboulderをローカルで動かす．次にACMEのGo言語クライアントライブラリであるericchiang/letsencrypt（非公式）を使い実際にboulderと喋りながら証明書発行を行い，コードとともにACMEが具体的にどのようなものなのかを追う．
はじめに 証明書というのは面倒なもの，少なくともカジュアルなものではない，というイメージが強い．それは有料であることや自動化しにくいなどといったことに起因している（と思う）．そのようなイメージに反して近年登場する最新の技術/プロトコルはTLSを前提にしているものが少なくない（e.g., HTTP2）．
このような背景の中で登場したのがLet&amp;rsquo;s Encryptと呼ばれるCAである．Let&amp;rsquo;s Encryptは上で挙げたような問題（煩雑さ）を解決しようとしており，無料・自動・オープンを掲げている（cf. &amp;ldquo;Let&amp;rsquo;s Encrypt を支える ACME プロトコル&amp;rdquo;）．最近（2015年12月3日）Public Betaがアナウンスされすでに1日に70kの証明証が発行され始めており（cf. Let&amp;rsquo;s Encrypt Stats）大きな期待が寄せられている．特に自分は仕事で多くのドメインを扱うのでLet&amp;rsquo;s Encryptは使ってくぞ！という意識がある．
Let&amp;rsquo;s EncryptはDV証明書を発行することができるCAである．DV証明書とはドメインの所有を確認して発行されるタイプの証明書である．Let&amp;rsquo;s Encryptの大きな特徴の1つに自動化が挙げられる．申請からドメインの所有の確認，証明書発行までは全てコマンドラインで完結させることができる．そしてこのフローはLet&amp;rsquo;s Encrypt以外のCAでも利用できるように標準化が進められている．これはAutomated Certificate Management Environment（ACME）プロトコルと呼ばれる（ちなみにLet&amp;rsquo;s encryptの証明証の有効期限は90日である．これはセキュリティ強化の面もあるが自動化の促進という面もある（cf. Why ninety-day lifetimes for certificates?））．
Let&amp;rsquo;s Encryptは専用のACMEクライアントを提供している（letsencrypt）．基本はこれを使えば証明書の発行や，Apacheやnginxの設定ファイルの書き換え(!)などができる（やりすぎ感が気にくわないと感じるひとが多いようでsimple alternativeがいくつか登場している&amp;hellip;）．
それだけではなくACMEベースのCA（つまりLet&amp;rsquo;s encrypt）はBoulderとう名前でOSSベースで開発されている（Go言語で実装されている）．つまりBoulderを使えば誰でもACMEをサポートしたCAになることができる．
本記事ではおそらく将来的には意識しないでよくなる（であろう）ACMEプロトコルがどのようなものかを理解する．boulderをローカルで動かし（Dockerfileが提供されている），非公式であるがGo言語のACMEクライアントericchiang/letsencryptを使ってACMEを喋ってみる．
なおACMEはまだ仕様策定中なので以下の説明は変更される可能性がある．
boulderを動かす まず準備としてboulderを動かす．今回は例としてexample.orgの証明証を発行する．ローカルでこれを実行するためには以下の準備が必要になる．
 cmd/policy-loader/base-rules.jsonのブラックリストからexample.orgを外す /etc/hostsを編集してexample.orgを127.0.0.1に向ける  完了したらboulderコンテナを起動する．
$ cd $GOPATH/src/github.com/letsencrypt/boulder/ $ ./test/run-docker.sh ACMEの概要 ACME spec draft
ACMEとは「クライアントのドメインの所有を確認して証明書を発行する」ためのプロトコルであった．これをさらに細かくブレイクダウンすると以下の操作から構成される．
 各操作を行うためのURIを知る（directory） クライアントの登録を行う（new-registration） 認証（ドメイン所有の確認）を行う（new-authorization） 証明書（Certification）を発行する（new-certificate）  ACMEはこれらのリソースを持ったRESTアプリケーションであるとみなすこともできる．各リソースはその上のリソースに依存しており，上から順番にリクエストをこなしていくことで最後の証明書の発行に到達することになる．
以下ではこれらのリソースをさらに細かく見ていく．
directory directoryは他の各種リソースのURIをクライアントに提示する．クライアントはまずここにリクエストしその後の操作でリクエストするべきendpointを知る．
ericchiang/letsencryptを使うと以下のようになる．</description>
    </item>
    
    <item>
      <title>Go言語とHTTP2</title>
      <link>https://deeeet.com/writing/2015/11/19/go-http2/</link>
      <pubDate>Thu, 19 Nov 2015 01:30:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/11/19/go-http2/</guid>
      <description>http2 in Go 1.6; dotGo 2015 - Google スライド
2015年の5月にRFCが出たばかりのHTTP2が2016年の2月にリリース予定のGo1.6で早くも利用可能になることになっている．HTTP2の勉強も兼ねてGo言語におけるHTTP2実装を追ってみる．
以下ではまず実際にHTTP2サーバを動かしChromeで接続してみる．次に現状コードがどのように管理されているかを追う．最後に実際にコードを動かしながらHTTP2の各種機能を追う．なお参照するコードはすべて以下のバージョンを利用している（まだWIPなのでコードなどは今後変わる可能性があるので注意）．
$ go version go version devel +9b299c1 darwin/amd64 HTTP2とは? HTTP/2に関してはスライドやブログ記事，Podcastなど非常に豊富な情報がインターネット上に存在する．そもそもHTTP2とは何か?なぜ必要なのか?などを理解したい場合は参考に挙げた記事などを参照するのがよい．
実際に使ってみる 最小限のコードでHTTP2サーバーを起動しChromeで接続してみる．
まず最新のGoをソースからビルドする（ビルドにはGo1.5.1を利用する）．以下では2015年11月16日時点の最新を利用した．
$ git clone --depth=1 https://go.googlesource.com/go ~/.go/latest $ export GOROOT_BOOTSTRAP=~/.go/1.5.1 $ cd ~/.go/latest/src &amp;amp;&amp;amp; ./make.bash 現時点でGoにおけるHTTP2はover TLSが前提になっている．そのためサーバー証明書と鍵が必要になる（なければ事前にopensslコマンドやcrypto/x509パッケージなどを使って自己署名証明書をつくる）．
コードは以下．
func main() { certFile, _ := filepath.Abs(&amp;#34;server.crt&amp;#34;) keyFile, _ := filepath.Abs(&amp;#34;server.key&amp;#34;) http.HandleFunc(&amp;#34;/&amp;#34;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, &amp;#34;Protocol: %s\n&amp;#34;, r.Proto) }) err := http.ListenAndServeTLS(&amp;#34;:3000&amp;#34;, certFile, keyFile, nil) if err !</description>
    </item>
    
    <item>
      <title>Go言語と暗号技術（AESからTLS）</title>
      <link>https://deeeet.com/writing/2015/11/10/go-crypto/</link>
      <pubDate>Tue, 10 Nov 2015 15:53:31 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/11/10/go-crypto/</guid>
      <description>最近マスタリングTCP/IP SSL/TLS編や暗号技術入門を読んでいた．理解を深めるためにGo言語で標準のcryptoパッケージを触り/実装を読みながら読んだ．
cryptoパッケージは他の標準パッケージと同様に素晴らしい．Go言語にはどのような暗号化手法が実装されているのか実例を含めてざっとまとめる．なお本文に書ききれなかったものを含め全ての実装例はtcnksm/go-cryptoにある．
共通鍵暗号 まずは共通鍵暗号をみる．共通鍵暗号は暗号化と復号化に同じ鍵を用いる暗号化方式である．共通鍵暗号はブロック暗号とストリーム暗号の2種類に分けることができる．ブロック暗号は特定の長さ単位で暗号化を行う方式であり，ストリーム暗号はデータの流れを順次処理していく方式である．
Go言語にはブロック暗号としてDES（Data Encryption Standard），DESを繰り返すtriple-DES，そしてAES（Advanced Encryption Standard ）が実装されている．ストリーム暗号としてはRC4が実装されている．
AESはDESに代わる新しい標準のアルゴリズムであり公募により選出された．互換性などを考慮しない限りこれを使うのが良い．実際にplainTextをAESで暗号化/復号化してみる．
plainText := []byte(&amp;#34;This is 16 bytes&amp;#34;) key := []byte(&amp;#34;passw0rdpassw0rdpassw0rdpassw0rd&amp;#34;) block, err := aes.NewCipher(key) if err != nil { fmt.Printf(&amp;#34;err: %s\n&amp;#34;, err) return } // Encrypt cipherText := make([]byte, len(plainText)) block.Encrypt(cipherText, plainText) fmt.Printf(&amp;#34;Cipher text: %x\n&amp;#34;, cipherText) // Decrypt decryptedText := make([]byte, len(cipherText)) block.Decrypt(decryptedText, cipherText) fmt.Printf(&amp;#34;Decrypted text: %s\n&amp;#34;, string(decryptedText)) AESの鍵長さは16byte，24byte，32byteのいずれかである必要がある（それぞれAES-128，AES-192，AES-256と呼ばれる）．NewCipherはcipher.Blockインタフェースを返す．このインタフェースにはEncrypt()とDecrypt()が実装されている．全てのブロック暗号にはこのインタフェースが実装されている（他の例はこちら）．
AESは16byteというブロック単位で暗号化/復号化を行うアルゴリズムである．このままでは例にあるように16byteの固定視長の平文しか暗号化を行えない．これでは使えない．
ブロック暗号のモード 任意の長さの平文を暗号化するためにはブロック暗号を繰り返し実行する必要がある．ブロック暗号にはそれを繰り返し実行するためのモードがある．
まず単純に考えると平文を分割してそれぞれにブロック暗号を適用する方法が考えられる．これはECB（Electronic CodeBook mode）モードと呼ばれる．しかし同じ平文ブロックが存在する場合は同じ暗号文ブロックが存在してしまう，かつ攻撃者が暗号文ブロックを入れ替えたら平文の順番も入れ替わってしまうというなどの問題があり実用的ではない．これらの欠点を回避するために各種モードが存在する．
Go言語では，ブロック暗号の各種モードをcipherパッケージに実装している．実装されているモードは以下，
 CBC（Cipher Block Chainning）モード - 1つ前の暗号ブロックと平文ブロックのXORをとってから暗号化を行う．1番最初の平文ブロックにはIV（Initialization Vector）とXORをとる．暗号ブロックの一部が欠損すると以後の平文全てに影響が出る．SSL/TLSに利用されている（3DES_EDE_CBC，AES_256_CBC）． CFB（Cipher FeedBack）モード - 1つ前の暗号ブロックを暗号化したもの（Key Stream）と平文ブロックのXORをとる．再生攻撃が可能． OFB（Output FeedBack）モード - 1つ前の暗号化の出力（Key Stream）を次の暗号化の入力とする．暗号化の出力（Key Stream）と平文でXORをとる（Key Streamを事前につくっておくことができる）．もし暗号結果が同じものになったらそれ以後Key Streamは全て同じ値になってしまう．暗号文を1ビット反転させると平文も1ビット反転する CTR（CounTeR）モード - 1つずつ増加していくカウンタを暗号化してKey Streamを作り出す．カウンタを暗号化してKey Streamとする．カウンタは暗号化のたびに異なる値（ノンス）をもとにしてつくる．暗号文を1ビット反転させると平文も1ビット反転する．暗号結果が同じになってもそれ以後のKey Streamが同じ値になることがない． GCM（Galois/Counter）モード - CTRが暗号文を作り出すと同時に「この暗号文は正しい暗号化によって作られたものである」とう認証子を作り出す．暗号文の偽装を見抜くことができる．TLS1.</description>
    </item>
    
    <item>
      <title>Hashicorp Ottoを読む</title>
      <link>https://deeeet.com/writing/2015/10/04/otto/</link>
      <pubDate>Sun, 04 Oct 2015 22:07:21 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/10/04/otto/</guid>
      <description>Hashicorpから2015年秋の新作が2つ登場した．
 Otto - HashiCorp Nomad - HashiCorp  Ottoがなかなか面白そうなのでコードを追いつつ，Ottoとは何か? なぜ必要になったのか? どのように動作するのか? を簡単にまとめてみる．
バージョンは 0.1.0 を対象にしている（イニシャルインプレッションである）
Ottoとは何か? 公式はVagrantの後継と表現されている．が，それはローカル開発環境の構築も担っているという意味で後継であり，自分なりの言葉で表現してみると「OttoはHashicorpの各ツールを抽象化し開発環境の構築からインフラの整備，デプロイまでを一手に担うツール」である．ちなみにOttoという名前の由来はAutomationと語感が似ているからかつ元々そういう名前のbotがいたからとのこと．
なぜOttoか? なぜVagrantでは不十分であったのか? なぜOttoが必要だったのか? 理由をまとめると以下の5つである．
 設定ファイルは似通ったものになる 設定ファイルは化石化する ローカル開発環境と同じものをデプロイしたい microservicesしたい パフォーマンスを改善したい   まず各言語/フレームワークのVagrantfileは似通ったものになる．Vagrantfileは毎回似たようなものを書く，もしくはコピペしていると思う．それならツール側が最も適したものを生成したほうがよい．Ottoは各言語のベストプラクティスな設定ファイルを持っておりそれを生成する．
そしてVagrantfileは時代とともに古くなる，つまり化石化する．秘伝のソースとして残る．Ottoは生成する設定ファイルを常に最新のものに保つ．つまり今Ottoが生成する設定ファイルは5年後に生成される設定ファイルとは異なるものになる（cf. &amp;ldquo;Otto: a modern developer&amp;rsquo;s new best friend&amp;rdquo;）
そしてローカル開発環境と同じものを本番に構築したい（Environmental parityを担保したい）．現在のVagrantでもproviderの仕組みを使えばIaaSサービスに環境を構築することはできる．が本番に適した形でそれを構築できるとは言い難い．Ottoは開発環境の構築だけではなく，デプロイ環境の構築も担う．
時代はmicroservicesである．Vagrantは単一アプリ/サービスの構築には強いが複数には弱い．Ottoは依存サービスを記述する仕組みをもつ（Appfile）．それによりmicroserviceな環境を簡単に構築することができる．
そしてパフォーマンス．最近のVagrantはどんどん遅くなっている．例えば立ち上げているVMの状態を確認するだけのstatusコマンドは2秒もかかる．Ottoはパフォーマンスの改善も目的にしている．
Ottoは何をするのか? Ottoが行うことは以下の2つに集約できる．
 Hashicorpツールの設定ファイルとスクリプトを生成する Hashicorpツールのインストール/実行をする  Ottoの各コマンドと合わせてみてみると以下のようになる．
 compile - アプリケーションのコンテキスト（e.g., 言語やフレームワーク）の判定と専用の設定ファイルであるAppfileをもとにHashicorpツールの設定ファイル（VagrantfileやTerraformの.tfファイル，Packerのマシンテンプレート.json）と各種インストールのためのシェルスクリプトを生成する dev - 開発環境を構築する．Vagrantを実行する infra - アプリをデプロイするためのインフラを整備する．例えばAWSならVPCやサブネット，ゲートウェイなどを設定する．Terraformを実行する build - アプリをデプロイ可能なイメージに固める．例えばAMIやDocker Imageなど．Packerを実行する deploy - 作成したイメージを事前に構築したインフラにデプロイする．Terraformを実行する（OttoのデプロイはImmutable Infrastructureを嗜好する）  Ottoがつくるインフラの基礎 OttoにはFoundationという概念がある（foundationという言葉は生成される設定ファイルやディレクトリ名に登場する）．これはOttoが構築するインフラの基礎，本番環境にアプリケーションをデプロイするために重要となるレイヤーを示す．このFoundationの例としては，以下のようなものが挙げられる．</description>
    </item>
    
    <item>
      <title>プレゼンするときに考えていること</title>
      <link>https://deeeet.com/writing/2015/09/25/talking/</link>
      <pubDate>Fri, 25 Sep 2015 08:48:30 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/09/25/talking/</guid>
      <description>僕はカンファレンスで喋るのが好きだ．好きだが決して得意ではない．むしろ喋るのは苦手なほうだと思う．
実際に自分でやるまではプレゼンは才能だと思っていた．大学の研究発表などで実際に自分でプレゼンをするようになり，大学の研究室で指導されまくった結果，プレゼンは技術だと認識した（もちろん才能もある）．技術であるということは学ぶことができる．それに気づいてからはたくさんプレゼンに関する本を読んだ．昔は発表前に必ず何か一冊プレゼンに関する本を読みそれを積極的に取り入れるようにした．
得意でないなりに学んで，発表を繰り返した結果なんとなく毎回考えること/意識することが固まってきた．今後のために簡単にまとめておく．
 聴衆は貴重な時間を割いて会場に来る オーガナイザーは貴重な時間を割いてカンファレンスを準備している 聴衆が誰かを妄想する 早めに準備する．早めに準備する．早めに準備する．早めに&amp;hellip; Keynoteを開く前に概要とトークの流れを書く Keynoteを先に開くと流れのない壊滅的な資料ができる 流れを書きつつここでx分/ここでy分という時間も想定する 時間超えるのはクソである むしろ早く終わった方がよい 少しでも有意義なものを受け取ってもらいたいから言いたいことは絞る 言いたいことを絞れば早く終わる 概要から始まり徐々に詳細に向かう 逆茂木型に注意する 前のスライドから次のスライドが想定できるようにする 前のスライドから次のスライドが想定できるようなきっかけを書く Itemizeは文末を揃える 例をなるべく使う 図をなるべく使う 文字を大きくする 文字の配置，大きさ，色を一貫させる 文字の配置，大きさ，色はそれだけで意味を持つ 作り終わったらちゃんと喋ってこの段階のスライドがゴミであることに気づく 喋って直す 喋って詰まるところを喋りやすいように直す どうしても詰まるなら軽くメモを書く（書き過ぎない．あくまでメモ） 会場に向かう前に一度喋っておく 直前まで微調整する プロジェクターの接続テストをする アイスブレイクなんて普通は無理．ふざけるな デモは何度も練習する どんなすごい人でもデモは失敗する 質問は最後までちゃんと聞く．わからなければ聞き直す 本当にわけわからん質問はあとで話しましょうと言う．無理しない（昔わけわからんおっさんと戦ったことがあるが無駄だった）  学術的な学会発表とは違って技術カンファレンスはとても好きだ．テーマは決まっているものの自由に話すことができる． 学会はある程度決められたフォーマットに従っていた．それは聞く側からすればわかりやすさにつながるが，喋る側からすればちょっと堅苦しかった．今は自由な感じで喋れるのを楽しんでいる．
苦手だけど喋るのはなぜか? こんなん作ったとか，こんなんわかったとかシェアしたいという思いがあるから．ブログで書くのもよいけど，プレゼンはまた違ったフォーマットで伝わり方も変わるから楽しい．あと僕は若干コミュニケーションに問題がある．懇親会などで初対面のひとに喋りに行くとかはほぼ無理だ．が，プレゼンしてると，あれを喋った僕です的な感じで喋りに行くきっかけになる．カンファレンスで喋るモチベーションはここにもある．
最後にここで書いているのは表層的な話である．本当に大切なのは内容．とにかく自分が喋る内容に対して自分が一番のプロフェッショナルになるのが大切だと思う．</description>
    </item>
    
    <item>
      <title>Google Omegaとは何か? Kubernetesとの関連は? 論文著者とのQA（翻訳）</title>
      <link>https://deeeet.com/writing/2015/09/17/qa-omega/</link>
      <pubDate>Thu, 17 Sep 2015 18:47:11 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/09/17/qa-omega/</guid>
      <description>エンタープライズ向けのKubernetesサポートを行っているkismatic Inc.による&amp;ldquo;Omega, and what it means for Kubernetes: a Q&amp;amp;A about cluster scheduling&amp;rdquo;が非常に良いインタビュー記事だった．Google Omegaとは何か? 今までのスケジューリングと何が違うのか? 何を解決しようとしているのか? 今後クラスタのスケジューリングにはどうなっていくのか? をとてもクリアに理解することができた．
自分にとってスケジューリングは今後大事になる分野であるし，勉強していきたい分野であるのでKismaticの@asynchio氏と論文の共著者であるMalte Schwarzkopf氏に許可をもらい翻訳させてもらった．
TL;DR 2013年に発表されたOmega論文の共著者であるMalte SchwarzkopfがGoogle OmegaのShared-stateモデルの主な目的がScalabilityよりもむしろソフトウェア開発における柔軟性であったことを説明する．Shared-stateモデルによるスケジューリングは優先度をもったプリエンプションや競合を意識したスケジューリングを可能にする．
今までのTwo-levelモデルのスケジューラー（例えばYARNやMesos）も同様の柔軟さを提供するが，Omega論文が発表された当時は，すべての状態がクラスタから観測できない問題（information hiding）やGang Schedulingにおけるhoardingの問題に苦しんでいた．またなぜGoogleがShare-stateモデルを選択したのか，なぜGoogleは（Mesosのような）厳格な公平性をもったリソース配分を行わないのかについてもコメントする．
最後にMesosやkubernetesのようにスケジューラーのモジュール化をサポートすることでいかにOSSのクラスター管理にOmegaの利点を持ち込むことができるのかを議論する．そしてより賢いスケジューリングを実現することでユーザはGoogleのインフラと同様の効率性を獲得できること提案する．
2013年の論文の発表時と比べて何が変わったか? 何が変わらないか?
クラスタのオーケストレーションは動きの早い分野である．しかしOmegaの中心となる原則はむしろよりタイムリーでとても重要であると思う．
2013年以来の大きな変化として，とりわけKubernetesやMesosのおかげで，Omegaのようなクラスタでインフラを運用するのが一般的になってきたことが挙げられる．2011年や2012年に立ち返ってみるとOmegaのShared-stateモデルによるスケジューリングの基礎となる仮説をGoogle以外の環境で実証するのはなかなかトリッキーなことだとみなされていた．しかし今日では，既存のモジュラーなスケジューラーをハックして新しい手法を試したり，Google cluster traceの公開されたTrace結果を使うことでより簡単にそれができる．
さらにOmegaで挑んだ，いかにクラスタ内で異なるタイプのタスクを効率良くスケジューリングするのか，いかに異なるチームがクラスタの全ての状態にアクセスし彼ら自身のスケジューラーを実装するのか，といった問題は業界で広く認識されてきた．コンテナにより多くの異なるタイプのアプリケーションを共有クラスタ内にデプロイすることが可能になり，開発者たちは全てのタスクを同じようにスケジューリングすることはできない/するべきではないことを認識し始めた．そしてオーケストレーション（例えばZookeeperやetcd）は共有された分散状態を管理する問題であるであるとみなされるようになった．
Omegaのコアにある考え方，つまりジョブのスケジューリングのモデリングとShared-stateモデルに基づくより概括的なクラスタのオペレーションは，まだ適切であると信じている．実際Kubernetesの中心にある考え方，ユーザが望むべき状態を指定しKubernetesがクラスタをそのゴールの状態に移行させること，はまさにOmegaにおいてスケジューラーが次の望むべき状態にクラスタの状態の変更を提案するときに起こっていることと全く同じである．
なぜOmegaのShared-stateモデルは柔軟性があり様々な異なるタスクのリソース管理を効率的に行うことができるのか?
数年前多くの組織が運用していたインフラ環境について考えてみる．例えば1つのHadoopクラスタで複数のユーザによるMapReduceジョブが走っていた．それは非常に簡単なスケジューリングである．全てのマシンにMapReduce worker用にn個のスロットがあり，スケジューラーはmapとreduceタスクを，その全てのスロットが埋まるまでもしくは全てのタスクがなくなるまで，ワーカーに割り当てる．
しかし，このスロットという単位は非常に荒いスケジューリングの単位である．全てのMapReduceジョブが同じ量のリソースが必要であるわけではなく，全てのジョブが与えられたリソースを使い切るわけではない．実際クラスタのいくつかのサーバーではMapReduce以外のプロセスが動いている場合もある．そのためスロットという単位で静的にマシンのリソースを区切るのではなく，タスクをBin-packしてリソースを最適化させるのはより良い考え方である．現代のほとんどのスケジューラーはこれを実現している．
複数のリソース状況に基づく（複数次元の）Bin-packingは非常に難しい（NP完全問題である）．さらに異なるタイプのタスクはそれぞれ別のBin-packingの方法を好むため問題はより難しくなる．例えば，ネットワーク帯域が70%使われているマシンにMapReduceのジョブを割り当てるのは全く問題ないが，webサーバーのジョブをそのマシンに割り当てるのは好ましくない&amp;hellip;
Omegaではそれぞれのスケジューラーは全ての利用可能なリソース，すでに動いているタスク，クラスタの負荷状況を見ることができ，それに基づきスケジューリングを行うことができる（Shared-stateモデル）．言い方を変えると，それぞれのスケジューラーは好きなBin-packingアルゴリズムを使うことができ，かつ全て同様の情報を共有している．
BorgはScalabilityに制限があるのか? OmegaはBorgの置き換えなのか?
違う．論文でそれをよりクリアにできたらと思う．多くのフォローアップで「中央集権型のスケジューラーは巨大なクラスタに対してスケールできないため分散スケジューラーに移行するべきである」と述べられてきた．しかしOmegaの開発の主な目的はScalabilityではなくより柔軟なエンジニアリングにある．Omegaのゴールは，様々なチームが独立してスケジューラーを実装できることにあり，これはScalabilityよりも重要な側面だった．スケジューラーの並列化がもたらすScalabilityは付属の利点にすぎない．実際のところちゃんと開発された中央集権型のスケジューラーは巨大なクラスタとタスクを扱えるまでにスケールできる．Borg論文は実際この点について書いている．
分散スケジューラーが必須になるニッチな状況もある．例えば，既存のワーカーに対して，レイテンシにセンシティブな短いリクエストを送るジョブを高速に配置する必要があるとき．これはSparrow schedulerのターゲットであり分散デザインが適切になる．しかしタスクがレイテンシにセンシティブ，もしくはタスクを秒間に数万回も配置する必要がなければ，中央集権型のスケジューラーであっても1万台のマシンを超えても問題ない．
Omega論文内で指摘しているMesosのTwo-levelモデルのスケジューリングの欠点は何か?
まずOmega論文におけるMesosの説明は2012年当時のMesosに基づいている．それから数年が経っており，論文でのいくつかの指摘はすでに取組まれており，同じように語ることはできない．
オファーベースのモデル，もしくは別のスケジューラーに対してクラスタ状態のサブセットのみを公開するモデルには大きく2つの欠点がある．これは例えばYARNのようなリクエストベースのデザインにも同様のことが言える．
まず1つ目はスケジューラーが割り当てらてた/提供されたリソースのみを見ることができることに関連する．Mesosのオファーシステムにおいて，リソースマネージャーはアプケーションスケジューラーに対して「これだけのリソースがあるよ．どれが使いたい?」と尋ね，アプリケーションスケジューラーはその中から選択を行う．しかしそのときアプリケーションスケジューラーはに別の関連する情報を知ることができない．例えば，自分には提供されなかったリソースは誰が使っているのか，より好ましいリソースがあるのか（そのために提供されたリソースを拒否してより良いリソースを待ったほうがよいのか）という情報を知ることができない．これがinformation hidingの問題である．information hidingによって問題になる他の例には優先度をもったプリエンプションがある．もし優先度の高いタスクが優先度の低いタスクを追い出すことができる必要があるとき，優先度の低いタスクが配置されるどの場所もまた効率的なリソースのオファーがある，がスケジューラーはそれをみることができない．
もちろんこれは解くことができる問題である．例えばMesosは優先度の低いタスクに利用されているリソースを優先度の高いタスクに提供することができる．もしくはスケジューラーのフィルターでプリエンプション可能なリソースを指定することもできる．しかしこれはリソースマネージャーのAPIとロジックが複雑になる．
2つ目はhoardingの問題．これは特にGang Shedulingによりスケジューリングされたジョブに影響を与える．このようなジョブは他のジョブが起動する前にリクエストした全てのリソースを獲得しておかなければならない．例えばこのようなジョブにはMPIがあるが，他にもstatefulなストリーム処理は起動するためにパイプライン全体が確保されている必要がある．MesosのようなTwo-levelモデルではこれらのジョブには問題が生じる．アプリケーションスケジューラーは要求したリソースが全て揃うまで待つ（揃わないかもしれない）こともできるし，十分なリソースを蓄積するため少量のオファーを順番に受け入れることもできる．もし後者なら，しばらくの間他のジョブ（例えば優先度の低いMapReduceのジョブなど）に効率良く利用できる可能性があるのにも関わらず，十分な量のリクエストが受け入れられるまでリソースは使われることなく蓄積（hoarding）される．
最近MesosphereでMesosの開発をしているBen Hindmanとこの問題ついて話したが，彼らはこれらを解決する並列のリソースオファー/予約が可能になるようにコアのオファーモデルを変更する計画があると話していた．例えば，Mesosは複数のスケジューラーに対して同じリソースをOptimisticに提供し，消失したスケジューラーのオファーを「無効にする」ことが可能になる．これはOmegaと同じ競合の解決が必要になる．その時点で2つのモデルは同じところに到達する．もしMesosがクラスタの全てのリソースをスケジューラーに提供するならそのスケジューラーはOmegaと同じ視点をもつことになる（詳しくは&amp;ldquo;proposal: Mesos is isomorphic to Omega if makes offers for everything available&amp;rdquo;）．しかしまだ実装は初期段階にある．
MesosのTwo-levelモデルのスケジューリングはGoogleには適していないのか? クラスタの全てのリソースを全てのスケジューラーに共有するのはなぜ良いのか?</description>
    </item>
    
    <item>
      <title>Apache Kafkaに入門した</title>
      <link>https://deeeet.com/writing/2015/09/01/apache-kafka/</link>
      <pubDate>Tue, 01 Sep 2015 18:13:38 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/09/01/apache-kafka/</guid>
      <description>Apache kafka
最近仕事でApache Kafkaの導入を進めている．Kafkaとは何か? どこで使われているのか? どのような理由で作られたのか? どのように動作するのか（特にメッセージの読み出しについて）? を簡単にまとめておく（メッセージングはまだまだ勉強中なのでおかしなところがあればツッコミをいただければ幸いです）．
バージョンは 0.8.2 を対象に書いている．
Apache Kafkaとは? 2011年にLinkedInから公開されたオープンソースの分散メッセージングシステムである．Kafkaはウェブサービスなどから発せられる大容量のデータ（e.g., ログやイベント）を高スループット/低レイテンシに収集/配信することを目的に開発されている．公式のトップページに掲載されているセールスポイントは以下の4つ．
 Fast とにかく大量のメッセージを扱うことができる Scalable Kafkaはシングルクラスタで大規模なメッセージを扱うことができダウンタイムなしでElasticかつ透過的にスケールすることができる Durable メッセージはディスクにファイルとして保存され，かつクラスタ内でレプリカが作成されるためデータの損失を防げる（パフォーマンスに影響なくTBのメッセージを扱うことができる） Distributed by Design クラスタは耐障害性のある設計になっている  どこで使われているのか? Use Casesをあげると，メッセージキューやウェブサイトのアクティビティのトラッキング（LinkedInのもともとのUse Case），メトリクスやログの収集，StormやSamzaを使ったストリーム処理などがあげられる．
利用している企業は例えばTwitterやNetflix，Square，Spotify，Uberなどがある（cf. Powered By）．
Kafkaの初期衝動 Kafkaのデザインを理解するにはLinkedInでなぜKafkaが必要になったのかを理解するのが早い．それについては2012年のIEEEの論文&amp;ldquo;Building LinkedIn&amp;rsquo;s Real-time Activity Data Pipeline&amp;rdquo;を読むのが良い．簡単にまとめると以下のようになる．
LinkedInでは大きく2つのデータを扱っている．1つはウェブサイトから集められる大量のユーザのアクティビティデータ．これらをHadoop（バッチ処理）を通して機械学習しレコメンド/ニュースフィードなどサービスの改善に用いている．それだけではなくこれらのデータはサービスの監視（セキュリティなど）にも用いている．2つ目はシステムのログ．これらをリアルタイムで処理してサービスのモニタリングを行っている．これらは近年のウェブサービスではよく見かける風景．
問題はそれぞれのデータの流れが1本道になっていたこと．アクティビティデータはバッチ処理に特化していたためリアルタイム処理ができない，つまりサービス監視には遅れが生じていた．同様にシステムのログは，リアルタイム処理のみに特化していたため長期間にわたるキャパシティプランニングやシステムのデバッグには使えなかった．サービスを改善するにはそれぞれタイプの異なるデータフィードを最小コストで統合できるようにする必要があった．またLinkedInのようにデータがビジネスのコアになる企業ではそのデータを様々なチームが簡単に利用できる必要があった．
これら問題を解決するために大ボリュームのあらゆるデータを収集し様々なタイプのシステム（バッチ/リアルタイム）からそれを読めるようにする統一的ななメッセージプラットフォームの構築が始まった．
最初は既存のメッセージシステム（論文にはActiveMQを試したとある）の上に構築しようとした．しかしプロダクションレベルのデータを流すと以下のような問題が生じた．
 並列でキューのメッセージを読むにはメッセージごとに誰に読まれたかを記録する必要がある（mutex）．そのため大量のデータを扱うとメモリが足りなくなった．メモリが足りなくなると大量のRamdom IOが発生しパフォーマンスに深刻な影響がでた バッチ処理/リアルタイム処理の両方でキューを読むには少なくとも2つのデータのコピーが必要になり非効率になった  このような問題から新しいメッセージシステム，Kafkaの開発が必要になった．Kafkaが目指したのは以下．
 あらゆる種類のデータ/大容量のデータを統一的に扱う 様々なタイプのシステム（バッチ/リアルタイム）が同じデータを読める 高スループットでデータを処理する（並列でデータを読める）  どのように動作するのか?（概要） KafkaはBroker（クラスタ）とProducer，Consumerという3つのコンポーネントで構成される．Producerはメッセージの配信を行いConsumerはメッセージの購読を行う．そしてKafkaのコアであるBrokerはクラスタを構成しProducerとConsumerの間でメッセージの受け渡しを行うキューとして動作する．
http://kafka.apache.org/images/producer_consumer.png
メッセージのやりとり
KafkaはTopicを介してメッセージのやりとりを行う．Topicとはメッセージのフィードのようなものである．例えば，検索に関わるデータを&amp;quot;Search&amp;quot;というTopic名でBrokerに配信しておき，検索に関わるデータが欲しいConsumerは&amp;quot;Search&amp;quot;というTopic名を使ってそれをBrokerから購読する．
Pull vs Push
BrokerがConsumerにデータをPushするのか（fluentd，logstash，flume），もしくはConsumerがBrokerからデータをPullするのかはメッセージシステムのデザインに大きな影響を与える．もちろんそれぞれにPros/Consはある．KafkaはPull型のConsumerを採用している．それは以下の理由による．
 Pushだと様々なConsumerを扱うのが難しく，Brokerがデータの転送量などを意識しないといけない．Kafkaの目標は最大限のスピードでデータを消費することだが，（予期せぬアクセスなどで）転送量を見誤るとConsumerを圧倒してまう．PullだとConsumerが消費量を自らが管理できる． Pullだとバッチ処理にも対応できる．Pushだと自らそれを溜め込んだ上でConsumerがそれを扱えるか否かに関わらずそれを送らないといけない (PullでしんどいのはBrokerにデータがまだ届いてない場合のコストだがlong pollingなどでそれに対応している)  メッセージのライフサイクル</description>
    </item>
    
    <item>
      <title>Go言語のDependency/Vendoringの問題と今後．gbあるいはGo1.5</title>
      <link>https://deeeet.com/writing/2015/06/26/golang-dependency-vendoring/</link>
      <pubDate>Fri, 26 Jun 2015 12:15:03 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/06/26/golang-dependency-vendoring/</guid>
      <description>Go言語のDependency/Vendoringは長く批判の的になってきた（cf. &amp;ldquo;0x74696d | go get considered harmful&amp;rdquo;, HN）．Go1.5からは実験的にVendoringの機能が入り，サードパーティからはDave Chaney氏を中心としてgbというプロジェクベースのビルドツールが登場している．なぜこれらのリリースやツールが登場したのか?それらはどのように問題を解決しようとしているのか?をつらつらと書いてみる．
Dependencyの問題 最初にGo言語におけるDependecy（依存解決）の問題についてまとめる．Go言語のDependencyで問題なのはビルドの再現性が保証できないこと．この原因はimport文にある．
Go言語で外部パッケージを利用したいときはimport文を使ってソースコード内にそれを記述する．このimport文は2通りの解釈のされ方をする．go getはリモートレポジトリのfetch URLとして解釈し，コンパイラはローカルディスク上のソースのPathとして解釈する．例えばコマンドラインツールを作るときに外部パッケージとしてmitchellh/cliを使いたい場合は以下のように記述する．
import &amp;#34;github.com/mitchellh/cli&amp;#34; これが書かれたコードをgo getすると，ローカルディスクにmitchellh/cliがなければ$GOPATH/src以下にそれがfetchされる．ビルド時はそのPathに存在するコードが利用される．
importで問題になるのは，そこにバージョン（もしくはタグ，Revision）を指定できないこと．そのため独立した2つのgo getが異なるコードをfetchしてしまう可能性がある．そのコードが互換をぶっ壊していたらビルドは失敗するかもしれない．つまり現状何もしないとビルドの再現性は保証できない．
では以下のようにタグやバージョンを書けるようにすれば?となる．が，これは言語の互換を壊すことになる．
import &amp;#34;github.com/pkg/term&amp;#34; &amp;#34;{hash,tag,version}&amp;#34; 以下のようにディレクトリ名にバージョン番号を埋め込むという方法もよく見る．が，これも結局異なるRevisionのコードをFetchしてまうことに変わりはなくビルドに再現性があるとは言えない．
import &amp;#34;github.com/project/v7/library&amp;#34; Vendoring 再現性の問題を解決する方法として，依存するレポジトリを自分のレポジトリにそのまま含めてしまう（vendoringと呼ばれる）方法がある．こうしておくと依存レポジトリのupstreamの変更に影響を受けず，いつでもどのマシンでもビルドを再現できる．
しかし何もしないとコンパイラがそのレポジトリのPathを探せなくなりビルドができなくなる．ビルドするには以下のどちらかを行う必要がある．
 $GOPATHの書き換え importの書き換え  $GOPATHの書き換え まずは$GOPATHを書き換える方法．この場合はそもそもコードをvendoringするときに$GOPATH/src/github.com...と同じディレクトリ構成を作らなければならない．その上でそのディレクトリを$GOPATHに追加してビルドを実行する．
例えば外部パッケージmitchellh/cliをレポジトリ内のextディレクトリにvendoringしたい場合は，まず以下のようなディレクトリ構成でそれをvendoringをする．
$ tree ext ext └── src └── github.com └── mitchellh └── cli そしてビルド時は以下のように$GOPATHにextディレクトリを含めるようにする．
$ GOPATH=$(pwd)/ext:$GOPATH go build このやり方が微妙なのは毎回自分で$GOPATHの変更を意識しないといけないこと（Fork先でも意識してもらわないといけない）．
importの書き換え 次にimportを書き換える方法．レポジトリ内のvendoringしたディレクトリへと書き換えてしまう．例えばgithub.com/tcnksm/rというレポジトリのextディレクトリに外部パッケージmitchellh/cliをvendoringしたとする．この場合は以下のようにimportを書き換える．
import &amp;#34;github.com/mitchellh/cli&amp;#34; // Before import &amp;#34;github.com/tcnksm/r/ext/cli&amp;#34; // After これはあまり見ない．そもそもソースを書き換えるのが好まれないし，upstreamを見失うかもしれない．また多くの場合import文が異常に長く複雑になる．
Godep $GOPATHの書き換えやそれに合ったディレクトリの作成，importの書き換えを自分で管理するのは煩雑なのでこれらを簡単にするツールは多く登場している．その中で多く使われているのがgodepというツール．
godepは使い始めるのも簡単で，例えば現在のレポジトリの依存をすべてvendoringするには以下を実行するだけで良い．
$ godep save godepはレポジトリ内にGodep/_workspaceを作成しその中に$GOPATHの流儀に従い依存をvendoringする．そして同時にGodep.</description>
    </item>
    
    <item>
      <title>Docker社を訪問した</title>
      <link>https://deeeet.com/writing/2015/06/03/visit-docker-inc/</link>
      <pubDate>Wed, 03 Jun 2015 16:56:34 -0700</pubDate>
      
      <guid>https://deeeet.com/writing/2015/06/03/visit-docker-inc/</guid>
      <description>Google I/O 2015のためにサンフランシスコを訪れたついでにDocker社に遊びに行った．Docker社はサンフランシスコのダウンタウンを南に下った475 Brannan St.にある（ちなみに275にはGitHub社がある）．
迎えてくれたのはNathan．Nathanとは昨年東京で開催されたCommunities meetup Chef, Docker, Openstack, Puppetで出会った．その後もtwitterで絡んでおり今回訪問させてもらうことになった．
まず，近くにカフェ（Blue Bottleで焙煎された豆を使っていた）がありコーヒーを片手に近況などをゆっくり話した．NathanはDocker Machineをメインに担当していて，最近追加した機能や今後の予定などについて語ってくれた．今はv0.3.0に向けてRCを出して絶賛テスト中とのこと．Docker Machineは他社のサービスに依存するのでテストはなかなか大変らしい．
Docker Machineは今後Dockerデーモンの煩雑な設定を楽にする方向に向かうとのこと（詳しくはどこまで言っていいのかわからないので書きません）．Docker Machineでデーモン層をDocker Composeでコンテナ層をと担当を分けるてDockerを使った開発・運用を楽にしていく．Dockerを使った開発環境の構築を楽にしたいという思いがめちゃ伝わって良かった．
もともとWebエンジニアでPHPとかを書いてたけどGo言語に興味もって書いててGo言語ならDockerっしょとなりDockerに就職できたという夢のある話も聞いた．Go言語書くぞ！
その後はオフィスの様子を見せてもらった．
オフィスの様子 建物の入り口（他にもいくつかの企業が入っていた）
受付
オフィス
Nathanの（自作）スタンディングデスク
キッチン
非公式キャラクター（クジラは飼えない）
任天堂との繋がり
コンテナ
まとめ Nathanは今年のYAPC::2015で発表する予定になっているので絶対聞きに行きましょう！（&amp;ldquo;Docker For Polyglots : Where We&amp;rsquo;ve Come From, and Where We Can Go&amp;rdquo;）．日本でDockerのエンジニアの話を聞けるのはめちゃめちゃ貴重だと思います．
Thanks, Nathan :)</description>
    </item>
    
    <item>
      <title>Go言語でプラグイン機構をつくる</title>
      <link>https://deeeet.com/writing/2015/04/28/pingo/</link>
      <pubDate>Tue, 28 Apr 2015 13:18:07 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/04/28/pingo/</guid>
      <description>dullgiulio/pingo
Go言語でのプラグイン機構の提供方法は実装者の好みによると思う（cf. fluentd の go 実装におけるプラグイン構想）．Go言語はクロスコンパイルも含めビルドは楽なのでプラグインを含めて再ビルドでも良いと思う．が，使う人がみなGo言語の環境を準備しているとも限らないし，使い始めてもらう障壁はなるべく下げたい．プラグインのバイナリだけを持ってこればすぐに使えるという機構は魅力的だと思う．
Go言語によるプラグイン機構はHashicorpの一連のプロダクトやCloudFoundryのCLIなどが既に提供していてかっこいい．net/rpcを使っているのは見ていてこれを自分で1から実装するのは面倒だなと思っていた．
dullgiulio/pingoを使うと実装の面倒な部分を受け持ってくれて気軽にプラグイン機構を作れる．
使い方 サンプルに従ってプラグインを呼び出す本体とプラグインを実装してみる．
まず，プラグイン側の実装（plugins/hello-world/main.go）は以下．
package main import ( &amp;#34;github.com/dullgiulio/pingo&amp;#34; ) type HelloPlugin struct{} func (p *HelloPlugin) Say(name string, msg *string) error { *msg = &amp;#34;Hello, &amp;#34; + name return nil } func main() { plugin := &amp;amp;HelloPlugin{} pingo.Register(plugin) pingo.Run() } structとしてプラグインを定義し，メソッドを定義する．メイン関数はそれを登録（Register）して起動（Run）するだけ．
プラグインはあらかじめビルドしておく．
$ cd plugins/hello-world $ go build 次にプラグインを呼び出す本体の実装は以下．上のプラグインで実装したSay()を呼び出す．
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;github.com/dullgiulio/pingo&amp;#34; ) func main() { p := pingo.NewPlugin(&amp;#34;tcp&amp;#34;, &amp;#34;plugins/hello-world/hello-world&amp;#34;) p.</description>
    </item>
    
    <item>
      <title>Go言語のCLIツールのpanicをラップしてクラッシュレポートをつくる</title>
      <link>https://deeeet.com/writing/2015/04/17/panicwrap/</link>
      <pubDate>Fri, 17 Apr 2015 00:18:38 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/04/17/panicwrap/</guid>
      <description>mitchellh/panicwrap
Go言語でpanicが発生したらどうしようもない．普通はちゃんとテストをしてそもそもpanicが発生しないようにする（もしくはトップレベルでrecoverする）．しかし，クロスコンパイルして様々な環境に配布することを，もしくはユーザが作者が思ってもいない使いかたをすることを考慮すると，すべてを作者の想像力の範疇のテストでカバーし，panicをゼロにできるとは限らない．
panicが発生した場合，ユーザからすると何が起こったか分からない（Go言語を使ったことがあるユーザなら「あの表示」を見て，panicが起こったことがわかるかもしれない）．適切なエラーメッセージを表示できると良い．開発者からすると，そのpanicの詳しい発生状況を基に修正を行い，新たなテストケースを追加して二度とそのバグが発生しないようにしておきたい．
mitchellh/panicwrapを使うと，panicが発生したときにそれ（バイナリ）を再び実行し，設定したhandlerを実行することで，その標準出力/エラー出力を取得することができる．このパッケージを使えばpanicが起こったときに詳細なクラッシュレポートを作成し，ユーザにそれを報告してもらうことができる．
使い方 使い方は簡単でトップレベルにhandlerを登録するだけ．まず簡単に動作例を説明する．以下の例はpanicが発生したときにそのpanicの出力をcrash.logに書き込む例．
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;os&amp;#34; &amp;#34;github.com/mitchellh/panicwrap&amp;#34; ) func main() { // (1)  exitStatus, _ := panicwrap.BasicWrap(handler) // (2)  if exitStatus &amp;gt;= 0 { os.Exit(exitStatus) } // (3)  panic(&amp;#34;Panic happend here...&amp;#34;) } func handler(output string) { f, _ := os.Create(&amp;#34;crash.log&amp;#34;) fmt.Fprintf(f, &amp;#34;The child panicked!\n\n%s&amp;#34;,output) os.Exit(1) } 以下のように動作する．
 (1)ではhandlerをBasicWrapに登録する (2)はpanicが発生した場合．この場合exitStatusは0以上の値になる (3)は通常の実行．ここでpanicを発生させている．  動作としては(2)でpanicが発生し(0)で登録したhandler（ここではcrash.logへの書き込み）を実行し，(1)で終了する．
内部の仕組み panicwrapが何をしているかを簡単に見てみる．BasicWrapからWrapが呼ばれ中で自分自身（バイナリ）を実行している．
exePath, err := osext.Executable() .... cmd := exec.</description>
    </item>
    
    <item>
      <title>デプロイ自動化とServerspec</title>
      <link>https://deeeet.com/writing/2015/03/17/serverspec-for-automation/</link>
      <pubDate>Tue, 17 Mar 2015 16:16:17 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/03/17/serverspec-for-automation/</guid>
      <description>Serverspec本の献本ありがとうございました．とても面白かったです．詳しい書評はすでに素晴らしい記事がいくつかあるので，僕は現チームでどのようにServerspecを導入したか，どのように使っているかについて書きたいと思います．
Serverspec導入の背景 今のチームではサーバーのセッアップおよびデプロイにChefを使っている．本にも書かれているようにこのような構成管理ツールを使っている場合はそのツールを信頼するべきであり，Serverspecのようなテストツールは必要ない．僕らのチームもそのような理由でServerspecの導入には至っていなかった．
しかしアプリケーションが複雑になりChefのレシピも混沌とするようになるとそれは成立しなくなる．見通しの悪いレシピはChefへの信頼度を落とす．信頼度の低下はデプロイ不信に繋がり人手（筋肉）によるテストが始まる．
サーバーの数がそこまで多くなければなんとか運用できるかもしれない．しかしサーバーの数が膨大になるとデプロイ担当者が登場し，人手によるテストに時間がかかり，本来やるべき仕事が失われる．
このような状況を解決するためにServerspecを導入した．具体的には以下を行うことを目的とした．
 Chefレシピのリファクタリング デプロイ時の人手による確認作業の自動化  1つ目に関しては現在インフラCI環境の構築中でまだ成果は出ていない（し，他に良い資料がたくさんある）．2つ目のデプロイ自動化への組み込みに関しては成果が出ているのでそちらについて工夫したことなどを簡単に書いておく．
デプロイの3ステップ デプロイは以下の3つのステップに分割できる．
 サービスアウト セットアップ サービスイン  まず，サービスアウトではサーバーをロードバランサから切り離してユーザ影響をなくし，デーモン化したジョブを停止してセットアップの準備を行う．次に，セットアップではChefなどを用いてミドルウェアのインストールやアプリケーションのアップデート，設定値の更新などを行う．最後に，サービスインでは遮断したアクセスの復帰，デーモンジョブの再開を行いサーバーを本来のあるべき状態に戻す．
うまく自動化できてない場合，ステップごとに人手のテストを行うことになる．これをなくすために各ステップで専用のServerspecを組み込むようにした．例えばサービスアウトをテストする場合は，以下のようにする．
$ rake spec:service_out:all テストが失敗した場合は，そのステップでデプロイを停止するようにした．これによりステップが確実に成功した上で次のステップ進んでいることを保証できるし，失敗した場合は問題を切り分けられるようになった．
ディレクトリの構造は以下のようになる．
├── Rakefile └── spec ├── role1 ├── role2 ├── role3 ├── service_in │ ├── role1 │ ├── role2 │ └── role3 └── service_out ├── role1 ├── role2 └── role3 サービスが大きいとサービスインのやり方でさえも異なることがあるので，以下で説明するロールごとにディレクトリを分けるという戦略をサービスイン，サービスアウトでも使えるようなディレクトリ構造を採用した．
失敗を書き出す サーバーの数が多いとホストごとにディレクトリを準備するServerspecデフォルトのやり方では限界がある．そういう場合は，ロール毎，モジュール毎ににspecをまとめ，ホストとそのロール情報を別ファイル（JSON形式など）で準備し，それを読み込みRakeタスクを定義するのが良い．
今のチームではそもそもホストとそのロールのリストを準備しそれをもとにChefを実行するという運用があったので，そのリストをそのまま利用することにした．
さらにどのホストでSpecが失敗したかを知るのも大切である．これは以下のように新しくタスククラスを作って失敗したホスト情報をファイルに書き出すようにした．
class ServerspecTask &amp;lt; RSpec::Core::RakeTask attr_accessor :target attr_accessor :failed_list def run_task(verbose) success = system(&amp;#34;#{spec_command}&amp;#34;) save_failed_vm if not success end def save_failed_vm file = File.</description>
    </item>
    
    <item>
      <title>AppcとCoreOS/Rocket</title>
      <link>https://deeeet.com/writing/2015/03/12/rocket/</link>
      <pubDate>Thu, 12 Mar 2015 00:58:54 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/03/12/rocket/</guid>
      <description>Dockerの諸問題とRocket登場の経緯
Rocketはリリースした直後にちょっと触ってそのまま放置していた．App containerの一連のツールとRocketが現状どんな感じかをざっと触ってみる．まだまだ全然使えると思えないが今後差分だけ追えるようにしておく．
なお，今回試した一連のツールをすぐに試せるVagrantfileをつくったので触ってみたいひとはどうぞ．
https://github.com/tcnksm/vagrant-appc
概要 App Container SpecやRocketが登場の経緯は前回書いたのでここでは省略し，これらは一体何なのかを簡単に書いておく．
まず，App Container（appc）Specはコンテナで動くアプリケーションの&amp;quot;仕様&amp;quot;である．なぜ仕様が必要かというと，コンテナという概念は今まで存在したが曖昧なものだったため．namespaceやcgroupを使った..という何となくのものはあったが，統一的なものは存在しなかったため．appc specはOpenかつSecure，Composable，Simpleであることを理念に掲げて作成されている．
appcには仕様だけではなくいくつかのツールも提供されている．例えば，appcの元になるApp Container Image (ACI)の構築と検証を行うactoolや，DockerイメージからACIをつくるdocker2aci，Go言語のバイナリからACIをつくるgoaciなどがある．
では，Rocketは何かというと，そのappcを動かすruntimeの実装の1つである．つまりappcとRocketは別のものであり実装は他にも存在する．例えば，現時点ではFreeBSDのJail/ZFSとGo言語で実装されたJetpackや，C++のライブラリとしてlibappcとそれを使ったruntimeであるNose Coneなどがある．
今回はこれらのappc関連ツールとRocketを実際に触ってみる．
Appc tools まず，https://github.com/appcにあるAppcの一連のツールを触ってみる．
actoolによるイメージのbuild actoolはRootファイルシステムとjsonで既述されるmanifestファイルを基にACIをビルドするツール．ビルドだけではなく，manifestやACIが仕様通りであるかの検証を行うこともできる．
例として，以下のGo言語で書かれてサンプルWebアプリケーションを動かすためのACIを作成する．
package main import ( &amp;#34;log&amp;#34; &amp;#34;net/http&amp;#34; ) func main() http.HandleFunc(&amp;#34;/&amp;#34;, func(w http.ResponseWriter, r *http.Request) { log.Printf(&amp;#34;Request from %vn&amp;#34;, r.RemoteAddr) w.Write([]byte(&amp;#34;Hello from App Container&amp;#34;)) }) log.Fatal(http.ListenAndServe(&amp;#34;:5000&amp;#34;, nil)) } ルートファイルシステムを準備する．
$ mkdir hello $ mkdir hello/rootfs $ mkdir hello/rootfs サンプルアプリケーションを静的リンクでビルドする（go1.4の場合は-installsuffixが必要）．
$ CGO_ENABLED=0 GOOS=linux go build -a -tags netgo -ldflags &amp;#39;-w&amp;#39; -o hello-web $ file hello-web hello-web: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, not stripped バイナリをRootFS内に配置する．</description>
    </item>
    
    <item>
      <title>Dockerの諸問題とRocket登場の経緯</title>
      <link>https://deeeet.com/writing/2015/02/17/docker-bad-points/</link>
      <pubDate>Tue, 17 Feb 2015 23:32:16 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/02/17/docker-bad-points/</guid>
      <description>2014年の後半あたりからDocker，Docker Inc.への批判を多く見かけるようになった（もちろんもともと懸念や嫌悪を表明するひとはいた）．それを象徴する出来事としてCoreOSチームによる新しいコンテナのRuntimeであるRocketのリリースと，オープンなアプリケーションコンテナの仕様の策定を目指したApp Containerプロジェクトの開始があった．
 CoreOS is building a container runtime, Rocket  批判は，セキュリティであったり，ドキュメントされていない謎の仕様やバグだったり，コミュニティの運営だったり，と多方面にわたる．これらは具体的にどういうことなのか？なぜRocketが必要なのか？は具体的に整理されていないと思う．これらは，今後コンテナ技術を使っていく上で，オーケストレーションとかと同じくらい重要な部分だと思うので，ここで一度まとめておきたい．
なお僕自身は，コンテナ技術に初めて触れたのがDockerであり，かつ長い間Dockerに触れているので，Docker派的な思考が強いと思う．またセキュリティに関しても専門ではない．なので，以下の記事はなるべく引用を多くすることを意識した．また，あくまで僕の観測範囲であり，深追いしていないところもある，気になるひとは自分で掘ってみて欲しい．
セキュリティ問題 Dockerを使ったことがあるひとならわかると思うがDockerを使うにはルート権限が必須である．デーモンが常に動いており，それにクライアントがコマンドを発行するアーキテクチャになっているので，Dockerコンテナが動いているホストでは常にルートのプロセスが動き続けることになる．クライアントとデーモンはHTTPでやりとりするため，外部ホストからコマンドを叩くこともできてしまう．
これは怖い．コンテナはカーネルを共有しているので，もし特権昇格の脆弱性であるコンテナがハイジャックされたら，他の全てのコンテナとホストも攻撃されることになる（Container Security: Isolation Heaven or Dependency Hell | Red Hat Security）．
実際Docker 1.3.1以前のバージョンでは脆弱性も見つかっている．
 CVE-2014-6407 CVE-2014-6408  docker pullは安全なの？ 上記の脆弱性では悪意のあるイメージによる攻撃が指摘されており，攻撃を受けやすいのはdocker pullで外部からイメージを取得/展開するところである．ではここはちゃんと安全になっているのか？答えは「No」で，あまりよろしくないモデルになっており，よく批判されるところでもある．
 Docker Image Insecurity · Jonathan Rudenberg  これはFlynnの開発者が現在のdocker pullの危険性を指摘したブログ記事．要約すると，Dockerは署名されたManifestなるもので公式のDockerイメージの信頼性を確認していると主張しているがそれが全く動作していない，モデルとして危ないということを言っている．具体的には，
 イメージの検証は，[decompress] -&amp;gt; [tarsum] -&amp;gt; [unpack]処理の後に実行されるが，そもそもここに脆弱性が入り込む余地がある キーはDockerのコードには存在しておらず，イメージをダウンロードする前にCDNからHTTPSで取得するようになっており，これは&amp;hellip;  この問題を回避する方法が以下で紹介されている．
 Before you initiate a &amp;ldquo;docker pull&amp;rdquo; | Red Hat Security  ここで紹介されているのはdocker pullを使わない方法．具体的には，信頼できるサイトから，イメージの.tarファイルをダウンロードして，Checksumがあればそれをチエックしたうえで，docker loadでそれを読み込むという方法．</description>
    </item>
    
    <item>
      <title>OctopressからHugoへ移行した</title>
      <link>https://deeeet.com/writing/2014/12/25/hugo/</link>
      <pubDate>Thu, 25 Dec 2014 01:34:57 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/25/hugo/</guid>
      <description>このブログは2年ほどOctopressを使って生成してきたが，不満が限界に達したので，Go言語で作られたHugoに移行した．
Octopressへの不満は，とにかく生成が遅いこと．100記事を超えた辺から耐えられない遅さになり，最終的には約150記事の生成に40秒もかかっていた．ブログは頻繁に書くのでかなりストレスになっていた．
Hugoのうりは生成速度．試しに使ったところ，明らかに速く，すぐに移行を決めた．最終的な生成時間は以下．爆速．
$ time hugo hugo 0.30s user 0.06s system 296% cpu 0.121 total 他に良いところを挙げると，まずとてもシンプル．Octopressと比べても圧倒的に必要なファイルは少ない．また，後発だけあって嬉しい機能もいくつかある．例えば，draftタグを記事のヘッダに書いておけば，ローカルでは生成されても，本番用の生成からは外されるなどなど．
インストール Go言語で書かれているのでgo getして，デザインテーマをCloneするだけで動かせる．バイナリも配布されてるので，Go言語の環境がなくても使える（この楽さもRuby製のOctopressと比べて良い）．
$ go get -v github.com/spf13/hugo $ git clone --recursive https://github.com/spf13/hugoThemes themes 使いかたは公式に十分なドキュメントがある．
移行方法 Octopressからの移行はとても簡単だった．source/_posts内の記事を移せばとりあえず動く．以下ではこれ以外の移行作業を簡単にまとめておく．
まず，設定ファイルは，yamlもしくは，toml，json形式で書く．ブログの移行でめんどくさいのはURLの維持だが，Octopressと同様にpermalinkを設定できる．例えば，tomlを使う場合は以下のように書く．
[permalinks] post = &amp;quot;/:year/:month/:day/:filename/&amp;quot; 次に記事のヘッダ．OctopressとHugoでは日付フォーマットが若干異なる．Octopressの場合は，2014-12-25 01:34でHugoの場合は2014-12-25T01:34:57となる．これでも動くが，うまくパースされない．とりあえず，時刻を消せばちゃんとパースされるので，以下のようなワンライナーを書く．
$ find . -type f -exec sed -i &amp;#34;&amp;#34; -e &amp;#39;s/date: \([0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\).*$/date: \1/g&amp;#39; {} \; 最後にOctopressのタグ（e.g., {% img ...%}）．OctopressのタグはHugoでは使えない．これもワンライナーを使ってHTMLタグに変換する．例えば，イメージタグを変換は以下のようにした．
$ find . -type f -exec sed -i &amp;#34;&amp;#34; -e &amp;#39;s/{%.*img.*\/images\/post\/\(.*\) .</description>
    </item>
    
    <item>
      <title>Go言語でテストしやすいコマンドラインツールをつくる</title>
      <link>https://deeeet.com/writing/2014/12/18/golang-cli-test/</link>
      <pubDate>Thu, 18 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/18/golang-cli-test/</guid>
      <description>本記事はGo Advent Calendar 2014の18日目の記事です．
Go言語は，クロスコンパイルや配布のしやすさからコマンドラインツールの作成に採用されることが多い．自分もGo言語でいくつかのコマンドラインツールを作成してきた．例えば，GitHub Releaseへのツールのアップロードを簡単に行うghrというコマンドラインツールを開発をしている．
コマンドラインツールをつくるときもテストは重要である．Go言語では標準テストパッケージだけで十分なテストを書くことができる．しかし，コマンドラインツールは標準出力や標準入力といったI/O処理が多く発生する．そのテスト，例えばある引数を受けたらこの出力を返し，この終了ステータスで終了するといったテストは，ちゃんとした手法が確立されているわけではなく，迷うことが多い（少なくとも自分は結構悩んだ）．
本記事では，いくつかのOSSツール（得にhashicorp/atlas-upload-cli）を参考に，Go言語によるコマンドラインツールおいてI/O処理に関するテストを書きやすくし，すっきりとしたコードを既述する方法について解説する．
なお，特別なパッケージは使用せず，標準パッケージのみを利用する．
TL;DR io.Writerを入力とするメソッドをつくり，そこに実処理を書く．main関数やテストからはio.Writerを書き換えて，それを呼び出すようにする（文脈によりioの向き先を変える）．
実例 ここでは，簡単な例としてawesome-cliというコマンドラインツールを作成し，その出力結果と終了コードのテストを書く．
awesome-cliは-versionオプションを与えると，以下のような出力と，終了コードが得られるとする．
$ awesome-cli -version awesome-cli version v0.1.0 $ echo $? 0 以下では，この挙動のテストをどのように書くかを，awesome-cliのコードそのものと共に解説する．
コード awesome-cliは以下の2つのソースで構成する．
 cli.go - オプション引数処理を含めた具体的な処理 main.go - main関数  そしてcli_test.goにI/Oに関わるテスト，ここでは-versionオプション引数を与えたときの出力とその終了コードのテスト，を既述する．以下ではこれらを具体的に説明する．
cli.go まず，引数処理を含めた具体的な処理を行うcli.goは以下のように既述する．引数処理には標準のflagパッケージを利用する．
package main import ( &amp;#34;flag&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io&amp;#34; ) // 終了コード const ( ExitCodeOK = 0 ExitCodeParseFlagError = 1 ) type CLI struct outStream, errStream io.Writer } // 引数処理を含めた具体的な処理 func (c *CLI) Run(args []string) int { // オプション引数のパース  var version bool flags := flag.</description>
    </item>
    
    <item>
      <title>Dockerコンテナ接続パターン (2014年冬)</title>
      <link>https://deeeet.com/writing/2014/12/01/docker-link-pattern/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/01/docker-link-pattern/</guid>
      <description>本記事はDocker Advent Calendar 2014の1日目の記事です．
Dockerによるコンテナ化はリソース隔離として素晴らしい技術である．しかし，通常は1つのコンテナに全ての機能を詰め込むようなことはしない．マイクロサービス的にコンテナごとに役割を分け，それらを接続し，協調させ，全体として1つのサービスを作り上げるのが通常の使い方になっている．
コンテナ同士の接続と言っても，シングルホスト内ではどうするのか，マルチホストになったときにどうするのかなど様々なパターンが考えられる．Dockerが注目された2014年だけでも，とても多くの手法や考え方が登場している．
僕の観測範囲で全てを追いきれているかは分からないが，現状見られるDockerコンテナの接続パターンを実例と共にまとめておく．
なお今回利用するコードは全て以下のレポジトリをcloneして自分で試せるようになっている．
 tcnksm/docker-link-pattern  概要 本記事では以下について説明する．
 link機能（シングルホスト） fig（シングルホスト） Ambassadorパターン（マルチホスト） 動的Ambassadorパターン（マルチホスト） weaveによる独自ネットワークの構築（マルチホスト） Kubernetes（マルチホスト）  事前知識 事前知識として，Dockerがそのネットワークをどのように制御しているかを知っていると良い．それに関しては以下で書いた．
 Dockerのネットワークの基礎 | SOTA  利用する状況 以下ではすべてのパターンを，同じ状況で説明する．redisコンテナ（crosbymichael/redis）を立て．それにresdis-cliコンテナ（relateiq/redis-cli）で接続するという状況を考える．
link機能（シングルホスト） まず，基礎．DockerはLinksというコンテナ同士の連携を簡単に行う仕組みを標準でもっている．これは，--link &amp;lt;連携したいコンテナ名&amp;gt;:&amp;lt;エイリアス名&amp;gt;オプションで新しいコンテナを起動すると，そのコンテナ内で連携したいコンテナのポート番号やIPを環境変数として利用できるという機能である．
今回の例でいうと，まず，redisという名前でredisコンテナを立てておく．
$ docker run -d --name redis crosbymichael/redis これに接続するには，以下のようにする．
$ docker run -it --rm --link redis:redis relateiq/redis-cli redis 172.17.0.42:6379&amp;gt; ping PONG relateiq/redis-cliコンテナの起動スクリプトは以下のようになっている．
# !/bin/bash if [ $# -eq 0 ]; then /redis/src/redis-cli -h $REDIS_PORT_6379_TCP_ADDR -p $REDIS_PORT_6379_TCP_PORT else /redis/src/redis-cli &amp;#34;$@&amp;#34; fi 引数なしで起動すると，relateiq/redis-cliは環境変数， $REDIS_PORT_6379_TCP_ADDRに接続しようとする．--link redis:redisでこれを起動することで，この環境変数が設定され，接続できる．</description>
    </item>
    
    <item>
      <title>Go Conference 2014 Autumnの手伝いをした</title>
      <link>https://deeeet.com/writing/2014/12/01/go-conference-2014-autumn/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/01/go-conference-2014-autumn/</guid>
      <description>Go Conference 2014 autumn - connpass
自分の所属チームをはじめ弊社でもGolangの導入は始まっているため，合コンの会場の提供及び，運営の手伝いをした．会議の内容については他に良いまとめ記事があるので，そちらに任せ，普段あまり語られない会場について軽く書いておく．
今回やってみてこういう大規模なカンファレンスを余裕でやる設備あるなと思った．運営とかの&amp;quot;複雑さを隠蔽して&amp;quot;良いところを挙げると，
 500人以上は余裕で入れるキャパシティがある プロジェクターが全面に配置されている（どの席からでもスライドちゃんと見える） Wifiがめちゃしっかりしてる（普段から何千人が普通に使えてる） 音響もめちゃしっかりしてる  しかも設備は，タッチパネルで余裕の操作ができる．毎週全世界の支社を含めた，全社員が参加する会をやってるくらいなので，それに耐えうる設備がある．
逆にしんどい部分を挙げると，
 セキュリティが厳重（当たり前だけど柔軟さとのトレードオフ） パイプ椅子なのでケツが死ぬ 電源不足（でもこれは僕の怠惰による準備不足） 会場の自販機がEdyしか使えない  今回他の会場を探す機会があったが，費用を考えた場合に，300人以上の会場はなかなかない．今後，大規模なカンファレンスをやる機会があれば，少し考慮に入れてもらっても良いかもしれない．僕はしばらくやりたくないけど，社員に知り合いがいればなんとかなるかもしれません．
謝辞 @hyoshiokさんの協力がなければ，何もできませんでした．ありがとうございました．あと会場の関係上，当日は社員スタッフに手伝っていただきました．本当にありがとうございました！
主催の@tenntennさん，@jxck_さん，@ymotongpooさん，めちゃおもろいカンファレンスを開いて頂いてありがとうございました！またスタッフの方々もお疲れ様でした！
あと運営の手伝いをして，普段自分が気軽に参加している勉強会やカンファレンスのありがたさを実感した．
次回は発表枠で参加したい．
参考  Go Conference 2014 autumn を終えて #gocon 私のGopherコレクション2014 #golang  </description>
    </item>
    
    <item>
      <title>CoreOSに入門した</title>
      <link>https://deeeet.com/writing/2014/11/17/coreos/</link>
      <pubDate>Mon, 17 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/11/17/coreos/</guid>
      <description>CoreOS is Linux for Massive Server Deployments · CoreOS
CoreOS + Docker Meetup Tokyo #1に参加してCoreOSにめっちゃ感動したので，CoreOSに入門していろいろ触ってみた．
まず，CoreOSの概要とそれを支える技術について説明する．次に実際にDigitalOcenan上にVagrantを使って実際にCoreOSクラスタを立てて，CoreOSで遊ぶ方法について書く．
CoreOSとは何か CoreOSは，GoogleやFacebook，Twitterといった企業が実現している柔軟かつスケーラブル，耐障害性の高いインフラの構築を目的としたLinuxディストリビューションである．軽量かつ使い捨てを前提にしており，クラウドなアーキテクチャのベストプラクティスを取り入れている．CoreOSの特徴は大きく4つ挙げられる．
 ミニマルなデザイン 容易かつ安全なOSアップデート Dockerコンテナによるアプリケーションの起動 クラスタリング  CoreOSはとてもミニマルである．従来のLinuxディストリビューションが機能を追加することでその価値を高めていったのに対して，CoreOSは必要最低限まで機能を削ぎ落としていることに価値がある（&amp;ldquo;CoreOS の調査：足し算から引き算へと，Linux ディストリビューションを再編する&amp;rdquo;）．
CoreOSは安全かつ容易なOSアップデート機構を持っている．これにはOmahaというChromeOSやChromeの更新に利用されているUpdate Engineを使っており，RootFSを丸ごと入れ替えることでアップデートを行う．これによりShellShockのような脆弱性が発見されても，いちいちパッチを当てるといったことやらずに済む．
CoreOSは専用のパッケージマネージャーをもたない．またRubyやPythonといった言語のRuntimeも持たない．全てのアプリケーションをDockerコンテナとして動作させる．これによりプロセスの隔離と，安全なマシンリソースの共有，アプリケーションのポータビリティという恩恵を受けることができる．
CoreOSはクラスタリングの機構を標準で持っている．クラスタリングについては，先週来日していたCoreOSのKelsey氏は&amp;quot;Datacenter as a Computer&amp;quot;という言葉を使っていた．データセンターの大量のサーバー群からクラスタを構築してまるでそれが1つのコンピュータとして扱えるようにすることをゴールとしているといった説明をしていた．
CoreOSはクラウドネイティブなOSである．Amazon EC2，DigitalOcean，Rackspace，OpenStack，QEMU/KVMといったあらゆるプラットフォームが対応を始めている．1つのクラスタを異なる2つのクラウドサーバにまたがって構築することもできるし，クラウドと自社のベアメタルサーバーを使って構築することもできる．
CoreOSの特徴については，@mopemopeさんの &amp;ldquo;CoreOS入門 - Qiita&amp;rdquo;や，@yungsangさんの&amp;ldquo;CoreOS とその関連技術に関するここ半年間の私の活動まとめ&amp;rdquo;が詳しい．
CoreOSを支える技術 CoreOSを支える技術キーワードを挙げるとすれば以下の3つになる．
 Docker etcd fleet  これらについてざっと説明する．
Docker CoreOSは専用のパッケージマネージャーをもたない．またRubyやPythonといった言語のRuntimeも持たない．全てのアプリケーションをDockerコンテナとして動作させる．
https://coreos.com/assets/images/media/Host-Diagram.png
Dockerを使うことで上図のようにコンテナによるプロセスの隔離と，安全なマシンリソースの共有，アプリケーションのポータビリティという恩恵を受けることができる．
etcd CoreOSは複数のマシンからクラスタを形成する．クラスタを形成するために，CoreOSはetcdという分散Key-Valuesストアを使い，各種設定をノード間で共有する（etcdってのは&amp;quot;/etc distributed&amp;quot;という意味）．
https://coreos.com/assets/images/media/Three-Tier-Webapp.png
etcdはクラスタのサービスディスカバリーとしても利用される．クラスタのメンバーの状態などを共有し，共有情報に基づき動的にアプリケーションの設定を行う．これらを行うetcdのコアはRaftのコンセンサスアルゴリズムである．Raftについては，&amp;ldquo;Raft - The Secret Lives of Data&amp;rdquo;を見るとビジュアルにその動作を見ることができる．
etcdはlocksmithというクラスタの再起動時のリブートマネジャーにも使われている．
fleet コンテナによるサービスをクラスタ内のどのマシンで起動するかをいちいち人手で決めるわけにはいけない．クラスタ内のリソースの状態や動いているサービスに基づき，適切なマシンでコンテナを動かすスケジューリングの仕組みが必要になる．
このスケジューリングとコンテナの管理にCoreOSはfleetを用いる．fleetはクラスタ全体のinit systemとして，クラスタのプロセス管理を行う．fleetはこれを各マシンのsystemdを束ねることでこれを実現している．fleetで管理するサービスはsystemdのUnitファイルを改良したものを用いる．
https://coreos.com/assets/images/media/Fleet-Scheduling.png</description>
    </item>
    
    <item>
      <title>PaaSエンジニアになった</title>
      <link>https://deeeet.com/writing/2014/11/14/work-as-paas-engineer/</link>
      <pubDate>Fri, 14 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/11/14/work-as-paas-engineer/</guid>
      <description>今まではモバイルアプリ向けのAPIの開発に携わりつつ，CIやデプロイ自動化といったDeveloper Productivity的なことをメインとしていたが，PaaSチームにジョインした．後に自分がどういうことを考えて舵を取ったかを見返すために簡単に今思っていることを綴ってみる．
PaaSという選択 &amp;ldquo;実践Heroku入門&amp;rdquo;の&amp;rsquo;はじめに&amp;rsquo;にしつこく書かれているようにPaaSの大きな目標は「アプリケーション開発者の効率を最大化」することにある．もともとDeveloper Productivity的なことが好きでいろいろやってきたが，その究極的な形がPaaSではないかと思う．
PaaSといってもプライベートPaaSだが，素晴らしいアイディアがあり，それを簡単にリリースでき，かつスケールもできる，そういうプラットフォームを社内にもっているのは大きな強みになると思う．どうすれば開発者にとって使いやすいプラットフォームになるのか，それがいかにビジネスとしてうまくスケールできるのか，といったことをどんどん突き詰めていきたい．
PaaSへの興味は，FlynnやDeisのコミュニティやその成長を見てきたことも大きい．この辺の技術の動きは本当に面白い．
技術的な興味 自分が関わり始めたPaaSは結構なサービスを動かしつつも，パフォーマンスや運用上の問題を抱えている．それらを解決するために，Dockerや各種DevOpsツール，そして言語としてGolangに舵をとろうとしてる．
昨年あたりから個人的に興味をもってDocker等のツールやGolangを触ってきたが，個人レベルと実際の運用ではかなりの乖離がある．気に入った技術をガチな環境で試していける機会はなかなかない．実運用でこそ，大変さがわかるし，本当の良さがわかると思う．
PaaSというと今はポリグロット（他言語）対応が当たり前だが，それを運用するにはその対応しているプログラミング言語に精通する必要がある．自分のメイン言語以外も，ちゃんと運用できるくらいの知識はもっていたいので，その辺も魅力に感じている．
また，自分は開発の知識はあっても運用の経験は全くない．現在のチームは開発だけでなく，数千台規模のサーバの運用も自分たちでこなしているため，しっかりした運用の知識をつけることもできる．言葉としてはなく実感として．
まだチームに入って2週間程度だが，毎日知らないことが入ってくるのでとても新鮮！
環境 少数精鋭かつ，フラット，スクラム体制などが良い点として挙げられるが，一番気に入っているのが，OSSにどんどんコミットしていこうという姿勢．良いツールができれば公開するし，使っているツールに問題があればどんどんプルリクエストを送る．仕事としてそれが奨励されている．
また，日本人がほとんどいない．完全に英語．日本という住みやすい場所で英語使って働けるのはとても良いと思っている（いろいろ意見はあると思うけど）．
まとめ こういう機会が得られたのもブログ書いたり，外で発表してた結果だと思う．巡り巡って現在のチームのひとに知られることになった．こういうことを良くいろんな人が言ってて本当かよ！と思ってたけど実際本当だった．これからもどんどん続けていきたい．</description>
    </item>
    
    <item>
      <title>&#34;Microservices&#34;を読んだ</title>
      <link>https://deeeet.com/writing/2014/09/10/microservices/</link>
      <pubDate>Wed, 10 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/09/10/microservices/</guid>
      <description>James Lewis氏とMartin Fowler氏による&amp;ldquo;Microservices&amp;rdquo;を読んだ．以前ざっと目を通したが，最近よく耳にするようになったのでちゃんと読んだ．以下はそのメモ．
概要  &amp;ldquo;Microservices&amp;rdquo; とはソフトウェアシステムの開発スタイルである  近年このスタイルでの開発を見てきて良い結果が出ている 初出は2012年の3月の&amp;ldquo;Micro services - Java, the Unix Way&amp;rdquo;   Microserviceは一連の小さなサービスで1つのアプリケーションを開発する手法  それぞれのサービスは自身のプロセスで動いており，軽量な機構（e.g., HTTP API）を通じて情報をやりとりする これらのサービスは独立して自動デプロイされる   一枚岩として構築されるMonolithicスタイルのアプリケーションと比較すると分かりやすい  一般的なエンタープライズのアプリケーションは，クライアントサイドのユーザインターフェース，データベース，サーバーサイドのアプリケーションの3つで構成される サーバーサイドのアプリケーションは，HTTPリクエストを受け，データベースとやりとりし，クライアントにHTMLを返す  このようなサーバーサイドアプリケーションはMonolithicであり，システムへの変更は新しいバージョンのアプリケーションのビルドとデプロイを要する     Monolithicシステムの構築は一般的には成功したスタイルである  リクエストを処理するロジックは単一のプロセスで動く ロードバランサを配置しスケールアウトさせることもできる   クラウドに多くのアプリケーションがデプロイされ始めるとMonolithicアプリケーションはフラストレーションになってきた  システムの変更サイクルは，全て結びついている モジュール構造の維持や影響範囲の限定が困難になる アプリケーションの一部だけスケールが必要なのに全体をスケールしなければならない   これらのフラストレーションがMicroservicesアーキテクチャーを導きだした，Monolithicなアプリケーションと比較してMicroservicesは:  独立してデプロイできる 独立してスケールできる しっかりしたモジュールの境界をもつ（影響範囲の限定） 様々なプログラミング言語を利用できる 異なるチームで運用できる   Microservicesは新しい考え方ではない  少なくともその根源はUNIXのデザイン哲学に立ち戻っている    Microserviceの特徴  正式な定義はないが，共通の特徴を述べる  すべてのMicroservicesが全ての特徴を満たすわけではない    サービスによるコンポーネント化  コンポーネントを組み合わせてシステムを作りたい コンポーネントを入れ替え可能/アップグレード可能な独立したソフトウェアと定義する Microservicesはライブラリを使うが，主要なコンポーネント化はサービスへ分割することで行う  ライブラリを1つのプログラム内で連結し，インメモリーで関数呼び出しを行うコンポーネントと定義する サービスを別プロセス動作し，HTTPリクエストやRPCなどで連携するコンポーネントと定義する   サービスをコンポーネントとして扱う主要な理由の1つは独立してデプロイできること  良いMicroservicesアーキテクチャーはサービス間をなるべく粗結合にして，変更時のデプロイを少なくする   サービスをコンポーネントとして扱うとインターフェースがより明確になる プロセス内のコールと比べてリモートのコールはコストが高いのでAPIはなるべく粗くある必要がある  ビジネス能力に基づく組織化  巨大なアプリケーションを分割するとき普通は技術レイヤーでそれを区切る（図）  e.</description>
    </item>
    
    <item>
      <title>コマンドラインツールを作るときに参考にしている資料</title>
      <link>https://deeeet.com/writing/2014/08/27/cli-reference/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/27/cli-reference/</guid>
      <description>コマンドラインツールについて語るときに僕の語ること - YAPC::Asia Tokyo 2014
コマンドラインツールが好きで昔からつくってきた． 今年のYAPCで，そのコマンドラインツールをつくるときにどういうことを意識して作っているのか？どのような流れで開発しているのか？といったことを語る機会をもらえた． 具体的な内容については，是非トークを聴きに来てもらうとして， スライドをつくるにあったって過去に読んだ資料や，よく参考にしている記事を集め直したので，その一部を参考資料としてまとめておく．
UNIXという考え方 UNIXという考え方
Mike GancarzによるUNIXの思想や哲学をまとめた本．古いが全然色あせてない． コマンドラインツールの作り方を書いた本ではないが，これらの思想の上で動くツールはこの思想に準拠して作られるべきだと思う．何度も読んで考え方を染み付かせた．
 小さいものは美しい 一つのプログラムには一つのことをうまくやらせる できるだけ早く試作する 効率より移植性を優先する データをフラットなテキストデータとして保存する ソフトウェアを梃子（てこ）として使う シェルスクリプトによって梃子の効果と移植性を高める 過度の対話インターフェースを避ける 全てのプログラムをフィルタとして設計する  小定理  好みに応じて自分で環境を調整できるようにする オペレーティングシステムのカーネルを小さく軽くする 小文字を使い，短く 木を守る（ドキュメント） 沈黙は金（エラーメッセージの出力について) 同時に考える（並列処理） 部分の総和は全体よりも大きい（小さな部品を集めて大きなアプリケーションを作る） 90パーセントの解を目指す 劣る方が優れている 階層的に考える  GNU標準インターフェース Standards for Command Line Interfaces
コマンドラインツールには長い歴史がある．つまり慣習がある．慣習を外れない簡単な方法は，標準に従うこと． 普段からコマンドラインツールは使っているので，インターフェースはわかりきっていると思うかも知れないが，いざ自分がつくるとなると見落としていることは多い．
また，オプションは短オプション（e.g., -f）と長オプション（e.g., --force）の両方を準備するべきだが，長オプションの名前に迷うときがある．そういうときのために，GNUでよく使われている長オプションが以下にまとめられている．
Table of Long Options
Build Awesome Command-line tool Build Awesome Command-Line Applications in Ruby 2: Control Your Computer, Simplify Your Life</description>
    </item>
    
    <item>
      <title>好きなPodcast</title>
      <link>https://deeeet.com/writing/2014/08/06/podcast-2014/</link>
      <pubDate>Wed, 06 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/06/podcast-2014/</guid>
      <description>twitterでちょっとつぶやいてたけど，最近自分がよく聴いてるPodcastをまとめてみる．Tech系以外もすこし混じってる．他にオススメあれば教えてください．
日本語  Rebuild - Podcast by Tatsuhiko Miyagawa - Podcastを聴くという習慣はここから始まった．大学院生のころからずっと聴いてる．Liveもできる限り聴いてる．大ファン．取り上げる技術もすごい尖っていて面白い．全エピソード好きだけど，敢えてあげるなら，&amp;ldquo;3: MessagePack&amp;rdquo;，&amp;ldquo;14: DevOps with Docker, chef and serverspec&amp;rdquo;，&amp;ldquo;27: Dragon Quest, Docker and AngularJS&amp;rdquo;，&amp;ldquo;35: You Don&amp;rsquo;t Need API Version 2&amp;rdquo;, &amp;ldquo;37: N Factor Auth&amp;rdquo;，&amp;ldquo;42: When in Golang, Do as the Gophers Do&amp;rdquo;，&amp;ldquo;45: Remembering WSDL&amp;rdquo;&amp;hellip; mozaic.fm - #1から全て聴いてる．ある特定の技術テーマについてものすごい深い，仕様策定レベルの話が聴ける．自分が全く知らない世界で，こんな話が聴けるのかーって毎回思ってる．#6 WebRTC，#4 Security (protocol)，#2 HTTP2が面白かった． backspace.fm - 最近聴きはじめた．その週のガジェット系のニュースが聴ける．とにかく話がうまくて面白い．編集のテンポもとても良くて，さらっと聴ける．自分だとこんな製品出たんだーで終わるけど，それに対する深い洞察/実際に使った意見が聴けるのが素晴らしい． だんごゆっけの平和な話 - たまに聴く．日曜日とかに買い物行きながら聴く．タイトル通りにyusukebeさんとkamadangoさん等のすごい平和な話が聴ける．境界線の哲学大好き． ライムスター宇多丸のウィークエンド・シャッフル - Tech系ではない．映画が大好きなので，週間映画批評を毎週欠かさず聴いている．毎週ガチャで当たった映画を観に行ってそれを批評するというコーナー．宇多丸さんがすごいのは，つまらない映画をつまらないで終わらせないこと，いかにつまらないかを面白く語るところ．映画の観方はこれで学んだ． たまむすび - これも映画関係．町山さんの回を聴く．全部聴いてるわけではなくて，見た映画で検索して聴いてる．  英語 おそらく自分より，&amp;ldquo;にわか Podcast ファン - steps to phantasien&amp;rdquo;が参考になると思う．めっちゃ聴いてる訳ではなくてつまみ食いが多い．</description>
    </item>
    
    <item>
      <title>わかりやすいREADME.mdを書く</title>
      <link>https://deeeet.com/writing/2014/07/31/readme/</link>
      <pubDate>Thu, 31 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/31/readme/</guid>
      <description>GitHubなどに自分のツールやライブラリを公開するとき，README.mdは重要な役割を担っている．レポジトリを訪れたユーザが自分のツールを使ってくれるか否かの第一歩はREADME.mdにかかっている，と言っても過言ではない．実際自分が使う側になったときも，まずREADME.mdを読んで判断していると思う．
成功しているプロジェクトを参考にしつつ，自分が実践していることをまとめておく．ここに書いていることはあくまで（自分の中で）最低限的なものである．プロジェクトが成長していくにつれてREADMEはあるべき姿に成長していくべきだと思う．
READMEの役割 README.mdには大きく2つの役割がある．
 プロジェクト，ツールの使い方，インストール方法 プロジェクト，ツールの宣伝  元々READMEは前者の役割しかなかったが，GitHubの仕組み上，後者の役割も徐々に重要になっている．
さらに自分の場合は，README.mdを簡単な設計書としても使う．新しくツールやライブラリを書き始めるときは，まずREADME.mdを書く．Usageを書くことでツールの簡単なインターフェース，オプションを定義する．Installを書くことで配布の仕方を定義する．これにより作りたいツールのゴールが明確になる．
以下で詳しく書くが，自分は社内プロジェクトでもREADMEを準備する，準備するようにチームに呼びかけている．その場合は，基本的な使い方に加えて，プロジェクトに新たに参加したメンバーに対してその道しるべになるようにREADMEを使ってる．
テンプレート 自分は以下のテンプレートを使ってる．
Name ==== Overview ## Description ## Demo ## VS.  ## Requirement ## Usage ## Install ## Contribution ## Licence [MIT](https://github.com/tcnksm/tool/blob/master/LICENCE) ## Author [tcnksm](https://github.com/tcnksm) 何ができるのか？ Name まず，一番上には名前を書く．かっこいい名前を考える．
Overview 名前のすぐ下にこのツールの概要を一言で書く．レポジトリを訪れたユーザがまず最初に目にし，このツールは何ができるのかを判断する．例えば，
 kennethreitz/requests - Requests is an Apache2 Licensed HTTP library, written in Python, for human beings. progrium/dokku - Docker powered mini-Heroku. The smallest PaaS implementation you&amp;rsquo;ve ever seen. mitchellh/gox - Gox is a simple, no-frills tool for Go cross compilation that behaves a lot like standard go build.</description>
    </item>
    
    <item>
      <title>Go言語でCPU数に応じて並列処理数を制限する</title>
      <link>https://deeeet.com/writing/2014/07/30/golang-parallel-by-cpu/</link>
      <pubDate>Wed, 30 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/30/golang-parallel-by-cpu/</guid>
      <description>負荷のかかる処理を制限なしに並列化しても意味ない．処理の並列数を予測可能な場合は，当たりをつけて最適化するのもよいが，不明確な場合は，CPU数による制限が単純な1つの解になる．
TL;DR CPU数に応じたバッファ長のChannelを使ってセマフォを実装する．
実例  mitchellh/gox  goxはGo言語製のツールを並列コンパイルするツール．コンパイルの処理は重いため，デフォルトで並列処理数をCPU数で制限している．
簡単な例 例えば，以下のような単純な並列処理を考える．heavy()（重い処理）を並列で実行する．
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) func heavy(i int) { fmt.Println(i) time.Sleep(5 * time.Second) } func main() { var wg sync.WaitGroup for i := 0; i &amp;lt;= 100; i++ { wg.Add(1) go func(i int) { defer wg.Done() heavy(i) }(i) } wg.Wait() } この並列処理の同時実行数をCPU数で制限する．
まず，利用可能なCPUのコア数は，runtimeパッケージのNumCPU()で取得できる．
func NumCPU() int 次に，CPU数をバッファ長としたChannelを作成する．
cpus := runtime.NumCPU() semaphore := make(chan int, cpus) 後は，heavy()をChannelへの送受信で囲む．これで，CPU数だけバッファが溜まると，Channelへの送信がブロックされ，新しい並列処理の開始もブロックされる．
最終的な実装は以下のようになる．</description>
    </item>
    
    <item>
      <title>GithubのGo言語プロジェクトにPull Requestを送るときのimport問題</title>
      <link>https://deeeet.com/writing/2014/07/23/golang-pull-request/</link>
      <pubDate>Wed, 23 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/23/golang-pull-request/</guid>
      <description>TL;DR fork元（オリジナル）をgo getしてその中で作業，forkした自分のレポジトリにpushしてPull Requestを送る．
問題 Github上のGo言語のプロジェクトにコミットするとき，cloneの仕方で若干ハマることがある．普通のOSSプロジェクトの場合は，forkしてそれをcloneしてpush，Pull Requestとすればよい．Go言語のプロジェクトでは，同じレポジトリの中でパッケージを分け，それをimportして使ってるものがある．そういう場合にforkしたものをそのままcloneすると，importの参照先がfork元の名前になりハマる．
例えば，github.com/someone/toolがあるとする．このレポジトリはgithub.com/someone/tool/utilsという別パッケージを持っており，mainがそれを使っているとする．つまり以下のようになっているとする．
package main import ( &amp;#34;github.com/someone/tool/utils&amp;#34; ) ... この場合に，通常のやりかたでforkしてソースを取得する．
$ go get -d github.com/you/tool/... するとソースは，$GOPATH/src/github.com/youに，importしてるutilsパッケージは$GOPATH/src/github.com/someone/tool/utilsにあるといったことがおこる．で，$GOPATH/src/github.com/you/utils直しても反映されない，import書き換えないと！とかなる．
良さげなやりかた @mopemopeさんが言及していたり，&amp;ldquo;GitHub and Go: forking, pull requests, and go-getting&amp;rdquo;に書かれているやり方が今のところ良さそう．
まず，fork元（オリジナル）のソースを取得する．
$ go get -d github.com/someone/tool/... 作業は，$GOPATH/src/github.com/someone/tool内でブランチを切って行う．
pushはforkした自分のレポジトリにする．
$ git remote add fork https://github.com/you/tool.git $ git push fork あとは，そこからPull Requestを送る．
他のやりかた forkして以下のようにcloneするというやり方も見かけた．
$ git clone https://github.com/you/tool.git $GOPATH/src/github.com/someone/tool 他にベストなやり方があれば教えてほしい．</description>
    </item>
    
    <item>
      <title>DockerによるマルチホストのPaaS flynnの概要とそのアーキテクチャー</title>
      <link>https://deeeet.com/writing/2014/07/07/flynn/</link>
      <pubDate>Mon, 07 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/07/flynn/</guid>
      <description>&amp;ldquo;flynnの時代&amp;rdquo;
&amp;ldquo;Docker meetup tokyo #3&amp;rdquo;で発表してきた．内容は，Dockerの応用の１つであるOSSでPaaSをつくるflynnというプロジェクトの概要とそのアーキテクチャーの紹介．このflynnというプロジェクトの中には，Dockerの面白い使い方がたくさん詰まってるため，今後Dockerを使う人が，その応用の際の参考になればという思いで紹介させてもらった．
今回の発表のために資料を集めまくり，理解できない部分は出来る限りコードも読んだ．発表スライドの補完にもなると思うので，そのメモ書き（一応体裁は整えた）を公開しておく．
デモ 以下は，簡単なデモ．
やっていることは以下．
 nodeのアプリケーションをデプロイ ルーティングの追加 スケール  コマンドを含めた詳しい解説は以下で解説する．
前提知識 (Herokuの動作) まず，前提知識としてPaaS (ここではHeroku) がどのように動作しているのかをそのワークフローとともにまとめておく．
$ heroku create  Stackと呼ばれるベースとなるOSを準備する  e.g., Cedar stack    $ git push heroku master  アプリケーションがデプロイされる slug compilerでアプリケーションをビルドしてslugを作成する  slug compiler  各言語のBuildpackの集合 依存関係のインストール  e.g., RubyならGemfileをもとにrubygemsをインストール     slug  ソースと依存ライブラリ，言語のランタイムを含んだ圧縮されたファイルシステム(SquashFS)     アプリケーションの実行環境（Dyno）を準備する  Dyno  LXCをベースにしたContainer環境     Dynoにslugをロードする Procfileをもとにアプリケーションを起動する  Procfile  プロセスの起動コマンドを記述  e.</description>
    </item>
    
    <item>
      <title>HerokuのAPIデザイン</title>
      <link>https://deeeet.com/writing/2014/06/02/heroku-api-design/</link>
      <pubDate>Mon, 02 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/06/02/heroku-api-design/</guid>
      <description>Herokuが自ら実践しているAPIデザインガイドをGithubに公開した．
&amp;ldquo;HTTP API Design Guide&amp;rdquo;
このガイドは些細なデザイン上の議論を避けて，ビジネスロジックに集中すること目的としている．Heroku特有なものではなく，一般にも十分適用できる知見となっている．
最近は，モバイル向けにAPIをつくることも多いため，勉強もかねて抄訳した．なお内容は，HTTP+JSONのAPIについて基本的な知識があることが前提となっている．
適切なステータスコードを返す それぞれのレスポンスは適切なHTTPステータスコード返すこと．例えば，&amp;ldquo;成功&amp;quot;を示すステータスコードは以下に従う．
 200: GETやDELETE，PATCHリクエストが成功し，同時に処理が完了した場合 201: POSTリクエストが成功し，同時に処理が完了した場合 202: POSTやDELETE，PATCHリクエストが成功し，非同期で処理が完了する場合 206: GETのリクエストは成功したが，レスポンスがリソースに対して部分的である場合  その他のクライアントエラーやサーバエラーに関しては，RFC 2616を参照（日本語だと，このサイトや&amp;ldquo;Webを支える技術&amp;rdquo;が詳しい）．
可能な全てのリソースを提供する そのレスポンスで可能な全てのリソース表現（つまり，全ての要素とそのオブジェクト）を提供すること．ステータスコードが200もしくは201のときは常に全てのリソースを提供する．これはPUTやPATCH，DELETEリクエストでも同様．例えば，
$ curl -X DELETE \  https://service.com/apps/1f9b/domains/0fd4 HTTP/1.1 200 OK Content-Type: application/json;charset=utf-8 ... { &amp;#34;created_at&amp;#34;: &amp;#34;2012-01-01T12:00:00Z&amp;#34;, &amp;#34;hostname&amp;#34;: &amp;#34;subdomain.example.com&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;01234567-89ab-cdef-0123-456789abcdef&amp;#34;, &amp;#34;updated_at&amp;#34;: &amp;#34;2012-01-01T12:00:00Z&amp;#34; } ステータスコードが202の場合は，完全なリソース表現は含めない．例えば，
$ curl -X DELETE \  https://service.com/apps/1f9b/dynos/05bd HTTP/1.1 202 Accepted Content-Type: application/json;charset=utf-8 ... {} リクエストボディ中のシリアル化されたJSONを受け入れる フォームデータに加えて，もしくは代わりに，PUTやPATCH，POSTのリクエストボディ中のシリアル化されたJSONを受け入れること．これにより，リクエストとレスポンスが対称になる．例えば，
$ curl -X POST https://service.com/apps \  -H &amp;#34;Content-Type: application/json&amp;#34; \  -d &amp;#39;{&amp;#34;name&amp;#34;: &amp;#34;demoapp&amp;#34;}&amp;#39; { &amp;#34;id&amp;#34;: &amp;#34;01234567-89ab-cdef-0123-456789abcdef&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;demoapp&amp;#34;, &amp;#34;owner&amp;#34;: { &amp;#34;email&amp;#34;: &amp;#34;username@example.</description>
    </item>
    
    <item>
      <title>Go言語のコードレビュー</title>
      <link>https://deeeet.com/writing/2014/05/26/go-code-review/</link>
      <pubDate>Mon, 26 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/26/go-code-review/</guid>
      <description>SoundCloudが2年半ほどGo言語を利用したプロダクトを本番で運用した知見をGopherConで発表していた（&amp;ldquo;Go: Best Practices for Production Environments&amp;rdquo;）．その中で&amp;ldquo;CodeReviewCommentsというGoogleでのGo言語のコードレビューにおいてよくあるコメントをまとめたサイトが紹介されていた．
最近Go言語を書くようになり，使えそうなのでざっと抄訳してみた．&amp;ldquo;リーダブルコード&amp;rdquo;的な視点も含まれており，Go以外の言語でも使えそう．
 gofmtでコードの整形をすること コメントは文章で書くこと．godocがいい感じに抜き出してくれる．対象となる関数（変数）名で初めて，ピリオドで終わること  // A Request represents a request to run a command. type Request struct { ... // Encode writes the JSON encoding of req to w. func Encode(w io.Writer, req *Request) { ...   外から参照されるトップレベルの識別子にはコメントを書くべき
  通常のエラー処理にpanicを使わないこと．errorと複数の戻り値を使うこと
  エラー文字列は他の出力で利用されることが多いので，（固有名詞や頭字語でない限り）大文字で始めたり，句読点で終わったりしないこと
 例えば，fmt.Errorf(&amp;quot;Something bad&amp;quot;)のように大文字で始めるのではなく，fmt.Errorf(&amp;quot;something bad&amp;quot;)のようにしておくことで，log.Print(&amp;quot;Reading %s: %v&amp;quot;, filename, err)としても，文の途中に大文字が入るようなことがなくなる    エラーの戻り値を_で破棄しないこと．関数がエラーを返すなら，関数が成功したかをチェックすること．エラーハンドリングをして，どうしようもないときにpanicとする
  パッケージのインポートは空行を入れることでグループとしてまとめるとよい
  import ( &amp;#34;fmt&amp;#34; &amp;#34;hash/adler32&amp;#34; &amp;#34;os&amp;#34; &amp;#34;appengine/user&amp;#34; &amp;#34;appengine/foo&amp;#34; &amp;#34;code.</description>
    </item>
    
    <item>
      <title>使いやすいシェルスクリプトを書く</title>
      <link>https://deeeet.com/writing/2014/05/18/shell-template/</link>
      <pubDate>Sun, 18 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/18/shell-template/</guid>
      <description>できればシェルスクリプトなんて書きたくないんだけど，まだまだ書く機会は多い．シェル芸やワンライナーのような凝ったことではなく，他のひとが使いやすいシェルスクリプトを書くために自分が実践していることをまとめておく．
ヘルプメッセージ 書いてるシェルスクリプトが使い捨てではなく何度も使うものである場合は，本体を書き始める前に，そのスクリプトの使い方を表示するusage関数を書いてしまう．
これを書いておくと，後々チームへ共有がしやすくなる．とりあえずusage見てくださいと言える．また，あらかじめ書くことで，単なるシェルスクリプトであっても自分の中で動作を整理してから書き始めることができる．関数として書くのは，usageを表示してあげるとよい場面がいくつかあり，使い回すことができるため．
以下のように書く．
function usage { cat &amp;lt;&amp;lt;EOF $(basename ${0}) is a tool for ... Usage: $(basename ${0}) [command] [&amp;lt;options&amp;gt;] Options: --version, -v print $(basename ${0}) version --help, -h print this EOF } バージョンを書いたりもする．
function version { echo &amp;#34;$(basename ${0})version 0.0.1 &amp;#34; } 出力に色をつける ErrorやWarningによって出力の色を変えて出力を目立たせられると良い．コンソールの出力への色づけはエスケープシーケンスを利用する．基本の構文は以下．
\033[{属性値}m{文字列}\033[m 属性値を変更するだけで，文字色や背景色，文字種を変更することができる．自分は以下のような関数を準備して使う．
red=31 green=32 yellow=33 blue=34 function cecho { color=$1 shift echo -e &amp;#34;\033[${color}m$@\033[m&amp;#34; } 以下のように使う．
cecho $red &amp;#34;hello&amp;#34; 対話処理　 例えば，以下のようにユーザ名やパスワードを対話的に入力させることはよくある．
printf &amp;#34;ID: &amp;#34; read ID stty -echo printf &amp;#34;PASSWORD: &amp;#34; read PASSWORD stty echo これが何度も実行するスクリプトだったりすると，毎回入力させるのは鬱陶しい．環境変数で事前に設定できるようにしてあげると親切．</description>
    </item>
    
    <item>
      <title>DockerのHost networking機能</title>
      <link>https://deeeet.com/writing/2014/05/11/docker-host-networking/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/11/docker-host-networking/</guid>
      <description>DOCKER 0.11 IS THE RELEASE CANDIDATE FOR 1.0
1.0のRCである0.11はいくつかの新機能が追加された．例えば，SELinuxのサポートや，Host networking機能，Link機能でのホスト名，Docker deamonへのpingなど．
この中でもHost networking機能がなかなか面白いので，実際に手を動かして検証してみた．事前知識として&amp;ldquo;Dockerのネットワークの基礎&amp;rdquo;も書きました．ネットワークに関して不安があるひとが先にみると，Host Networing機能の利点／欠点もわかりやすいと思います．
TL;DR Host networking機能を使うと，異なるホスト間のコンテナの連携がちょっぴりやりやすくなる．SerfやConsulのようなサービスディスカバリーツールとの相性も良さそう．
まだ出たばかりの機能で実際に使ってるひとがいないので，あくまで個人の実感．HNのコメントで同様の発言は見かけた．
ネットワークモード コンテナを起動するとき，--netオプションで4つのネットワークモードを選択することができる．
 --net=bridge：仮想ブリッジdocker0に対して新しくネットワークスタックを作成する（default） --net=container:&amp;lt;コンテナ名|コンテナID&amp;gt;：他のコンテナのネットワークスタックを再利用する --net=host：ホストのネットワークスタックをコンテナ内で利用する --net=none：ネットワークスタックを作成しない  bridge ブリッジモードはデフォルトの挙動で，ループバックのloと仮想インターフェースのeth1がつくられる．eth1はホストのveth（Virtual Ethernet）とパイプされる．このモードは外部のネットワークとは隔離される．
$ docker run --net=bridge ubuntu ifconfig eth0 Link encap:Ethernet HWaddr 96:e7:26:24:69:55 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 container コンテナモードでは既に起動しているコンテナのネットワークスタックが再利用される．以下の場合だと，あらかじめ起動したhelloコンテナで作成したネットワークスタックがそのまま利用される．つまり，helloコンテナを起動したときにホスト側でつくられたvethと仮想インターフェースeth0のパイプがそのまま利用される．
$ docker run -d --name hello ubuntu /bin/sh -c &amp;#34;while true; do echo hello world; sleep 1; done&amp;#34; $ docker run --net=container:hello ubuntu ifconfig eth0 Link encap:Ethernet HWaddr b2:f4:26:c4:17:16 inet addr:172.</description>
    </item>
    
    <item>
      <title>Dockerのネットワークの基礎</title>
      <link>https://deeeet.com/writing/2014/05/11/docker-network/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/11/docker-network/</guid>
      <description>今までいろいろ触ってきて，Dockerネットワーク周りに関しては何となくは理解していたが，人に説明できるほど理解してなかったのでまとめておく．基本は，Advanced networking - Docker Documentationがベースになっている．
仮想ブリッジの仕組み Dockerのネットワークは，仮想ブリッジdocker0を通じて管理され，他のネットワークとは隔離された環境で動作する．
Dockerデーモンを起動すると，
 仮想ブリッジdocker0の作成 ホストの既存ルートからの空きのIPアドレス空間を検索 空きから特定の範囲のIPアドレス空間を取得 取得したIPアドレス空間をdocker0に割り当て  が行われる．
コンテナを起動すると，コンテナには以下が割り当てられる．
 docker0に紐づいたveth（Virtual Ethernet）インターフェース docker0に割り当てられたIPアドレス空間から専用のIPアドレス  そしてdocker0はコンテナのデフォルトのgatewayとして利用されるようになる．コンテナに付与されるvethは仮想NICで，コンテナ側からはeth0として見える．2つはチューブのように接続され，あらゆるやりとりはここを経由して行われるようになる．
実際にコンテナを起動して確認する．まず，インターフェースから．
$ brctl show bridge name bridge id STP enabled interfaces docker0 8000.000000000000 no $ docker run -d ubuntu /bin/sh -c &amp;#34;while true; do echo hello world; sleep 1; done&amp;#34; b9ffb0800ca5 $ docker run -d ubuntu /bin/sh -c &amp;#34;while true; do echo hello world; sleep 1; done&amp;#34; 4c0d9b786e8f $ brctl show bridge name bridge id STP enabled interfaces docker0 8000.</description>
    </item>
    
    <item>
      <title>Dockerとは何か？どこで使うべきか？</title>
      <link>https://deeeet.com/writing/2014/05/01/what-is-docker/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/01/what-is-docker/</guid>
      <description>この記事はDockerに関する実験的な記事や，Buildpackを使ってHeroku AppをDocker Containerとして使えるようにする&amp;ldquo;building&amp;rdquo;の開発などで知られるCenturyLink Labsの &amp;ldquo;What is Docker and When To Use It&amp;rdquo;の翻訳です． Dockerとは何か？Dockerをどこで使うべきか？についてよく見かける記事とは違った視点から説明されています． 翻訳は許可をとった上で行っています．
Dockerとは何でないか Dockerとは何かを説明する前に，Dockerは何でないかについて述べる．Dockerの否定形は何か？Dockerの制限は何か？Dockerが得意でないことは何か？
 DockerはLXCのようなLinux Containerではない DockerはLXCだけのラッパーではない（理論的には仮想マシンも管理できる） DockerはChefやPuppet，SaltStackのようなConfiguration toolの代替ではない DockerはPaaSではない Dockerは異なるホスト間での連携が得意ではない DockerはLXC同士を隔離するのが得意ではない  Dockerとは何か では，Dockerは何ができるのか？メリットはなにか？
 Dockerはインフラを管理することができる Dockerはイメージのビルドや，Docker Indexを通じたイメージの共有ができる DockerはChefやPuppetといったConfiguration toolによりビルドされたサーバのテンプレートにとって，イメージ配布の良いモデルである DockerはCopy-on-wirteのファイルシステムであるbtrfsを使っており，Gitのようにファイルシステムの差分を管理することができる Dockerはイメージのリモートレポジトリをもっているため，簡単にそれらを様々なOS上で動かすことができる  Dockerの代替は何か AmazonのAWS MarketplaceはDocker Indexに近い．ただし，AMIはAWS上でしか動かすことができないのに対して，Dockerイメージは，Dockerが動いているLinuxサーバであればどこでも動かすことができる．
Cloud FoundryのWardenはLXCの管理ツールであり，Dockerに近い．ただし，Docker Indexのような他人とイメージを共有する仕組みを持っていない．
Dockerをいつ使うべきか DockerはGitやJavaのような基本的な開発ツールになりうるものであり，日々の開発やオペレーションに導入し始めるべきである．
例えば，
 インフラのバージョン管理システムとして使う チームにアプケーション用のインフラを配布したいときに使う 稼働中のサーバーと同様の環境をラップトップ上に再現して，コードを実行したいときに使う（例えばbuildingを使う） 複数の開発フェーズ（dev，stg，prod，QA）が必要なときに使う ChefのCookbookやPuppetのManifestと使う  DockerとJavaはどこが似ているのか Javaには&amp;quot;Write Once. Run Anywhere（一度書けばどこでも実行できる）&amp;ldquo;という文言がある．
Dockerはそれに似ている．Dockerは，一度イメージをビルドすると，Dockerが動いているLinuxサーバであれば全く同じようにそれを動かすことができる（&amp;ldquo;Build Once．Run Anywhere&amp;rdquo;）．
Javaの場合，例えば以下のようなJavaコードがあるとする．
// HelloWorld.java class HelloWorldApp { public static void main(String[] args) { System.</description>
    </item>
    
  </channel>
</rss>
