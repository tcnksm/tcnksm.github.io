<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Writings on SOTA</title>
    <link>https://deeeet.com/writing/</link>
    <description>Recent content in Writings on SOTA</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <copyright>Copyright (C) 2013-2019 Taichi Nakashima All Right Reserved.</copyright>
    <lastBuildDate>Wed, 07 Oct 2020 08:43:26 +0900</lastBuildDate>
    
	<atom:link href="https://deeeet.com/writing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>社内PlatformチームのProduct Management</title>
      <link>https://deeeet.com/writing/2020/10/07/internal-platform-product-management/</link>
      <pubDate>Wed, 07 Oct 2020 08:43:26 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/10/07/internal-platform-product-management/</guid>
      <description>現職においてPlatform チーム（社内基盤チーム）として働き始めて2年近くがたった．このチームにおいて自分はTech Leadをメインに努めてきたが，同時にPlatformの「どのような機能を」「どのような優先度で」作るか? を決めるProduct Manager的な役割も果たしてきた（ちなみにTech Leadに関してはメルカリのテックリードが学んだ、HowよりWhyを重視することが大切なわけ で少し話した）．これは何度も失敗しながら悪戦苦闘しつつやってきたが自分たちなりのフレームワークをつくり実際に回すことができている．
未だに試行錯誤しているのでここで書いていることが正解だとは思っていないが，今後同じようにPlatformチーム的なことを始めるひとに向けて現状自分たちがどのようにやっているのかについて簡単にまとめておく（他の会社がどのようにやってるのかも聞きたいのでもし同じようなことをやってるひとがいたら会話しましょう!）．
本来なら会社のTech Blogとかに書くべき内容だが，技術的なことであれば自信を持って書けてもProduct Managementに関しては正直自信を持って書くことができない．上手くいったこともあるけどそうじゃないことのほうが多いと思う．動き始めていたプロジェクトを止めて大きく方向転換をすることになりチームに迷惑をかけてしまったこともある．今でもこれで良いのか?を試行錯誤している．そういうこともあり個人ブログで書くことにした．
以下ではまず大まかなアイディアについて簡単に紹介しその後「何を」「どのような優先度で」を具体的にどのように決めているのか?についてまとめる．
Platformをプロダクトとして考える これはすでに各所で言われていることだがInternal Platformをやるにあたって一番大切なマインドセットはPaltformを単なる「ツール」としてではなくて「プロダクト」として考えることだと思う．つまりPlatformを利用する社内の開発者をCustomerとしてみて，Customerに対してPlatformという「プロダクト」を提供していると考える．そしてそのCustomerのためにより良い機能の追加や改善を行う．例えば以下の記事が参考になると思う．
 Applying product management to internal platforms Product for Internal Platforms Product management in infrastructure engineering. Code less, engineer more - Increment: Teams  もちろんInternal PlatformはCloud Providerのように会社として外部に提供しているプロダクトではない．会社としてはメインのビジネスがある．その場合にPlatform Teamはどのような立ち位置になるかというと，直接的に会社のCustomerに価値を提供するのではなく，自分たちにとってのCustomerである開発者に価値を提供することで，開発者がよりよいサービスを開発できるようになり，間接的に会社のCustomerに価値を提供すると考える（もちろんインフラのReliabilityといった直接的な価値もあるが）．
こう考えると一般的なProduct managementとやるべきことの差は大きくはない．社内が対象であることによるコミュニケーションの違いだったり，対象がPlatformという技術に特化した領域であるくらいの違いしかない．サービスを開発するにあたって開発者がどのような課題を抱えているのかを見つけそれらに優先度をつけて順番に実行していくというのが大まかな流れになる．
フレームワーク Product managementの1つのゴールは具体的なRelease Sprintとして具体的にタスクにまで落とし込むことだと思う．ここではその大まかな流れについて紹介する．以下はそれを簡単に図にしたものである．
まずチームとしてのMissionを持つ．次にMissionをもとに長期的なRoadmapを作る．そしてRoadmapをもとにRelease Sprintのタスクに落とし込む．以下ではこれらを具体的に紹介する．
Vision &amp;amp; Mission まず最初はPlatformチームとしてのVisionとMissionを決める．What is company vision? A picture of a better place の定義に従うと，Visionとはどこへ向かうのか（Where）を示し，Missionはそこへ向かうために何をするか（What）を示す．会社の中のチームの場合はもちろん会社のVisionにアラインしつつそこに向かうためにチームとして何ができるのかを考える．特にMissionの定義は責任の範囲を明確にしたり，無限にある問題空間を限定することにもつながる．
VisionとMissionを決めるにあたっては以下が参考になった．
 re:Work - Guide: Set and communicate a team vision Writing strategies and visions.</description>
    </item>
    
    <item>
      <title>Infrastructure as Dataとは何か</title>
      <link>https://deeeet.com/writing/2020/05/11/infrastructure-as-data/</link>
      <pubDate>Mon, 11 May 2020 06:18:53 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/05/11/infrastructure-as-data/</guid>
      <description>最近GCPから登場したKubernetes YAMLのPackage managerであるKptは「Infrastructure as Data（Configuration as Data）」という考えかたを基礎としてそれを推し進めようとしている．それ以外にもKubernetesのEcosystemには（明示はされていなくても）この考え方が中心にある．Infrastructure as Codeとは何が違うのかなど歴史を振り返りつつまとめてみる．
（指針はBorg, Omega, and Kubernetesという論文にあるが「Infrastrcuture as Data（Configuration as Data）」という言葉を明確に定義した文章はない．この記事はReferencesに挙げるいくつかのPodcastにおける@kelseyhightowerの発言や，それに反応する@bgrant0607のTweetなどを中心に自分なりに考えをまとめているだけで，概念は変わらずとも名前などは今後変わるかもしれない）
Infrastructure as Code以前 Infrastructure as Codeという言葉が登場する前はインフラのセットアップは手順書に基づくManual operation行われていた．今でこそCloudが中心となり数百のMachineを扱うのは当たり前になってきたが，オンプレ時代では今ほど大規模でもなく1つのPhysical machineを使い続けることも多く（長く使うほどコストパフォーマンスが高い）インフラはStaticなものだった．そのためManual operationでも十分といえば十分だったことも多かったと思う．もちろん専用のSoftwareの登場を待つことなくShell scriptなどで自動化を早くから進めてInfrastructure as a scriptはやられていたと思う．
Infrastructure as Code Hardware virtualizationの登場により（Virtual）Machineを立てたり消したりすることが容易になり，またそれをサービスとして提供するAWSやGCPといったCloud providerが使った分だけ課金するPay-as-you-goモデルを採用したことで（Auto-scalingなどにより）必要に応じて必要なMachineをProvisioningするようになり，インフラはよりDynamicなものになった．DynamicになったことによりインフラのセットアップやアプリケーションのDeployのReproducibility（再現性）も求められるようになった．また扱うトラフィックも大規模になりそれをさばくためのインフラも大規模になった．
これらの背景から登場したのがPuppetやChef，AnsibleといったSoftwareを中心としたInfrastructure as Code（IaC）である．IaCはコード（e.g., DSL）によってインフラのセットアップを自動化する方法である．IaCにより，Error-proneなManual operationはなくなり，インフラのReproducibilityも高くなった．これだけではなくIaCはソフトウェア開発の方法論をインフラ管理に持ち込んだ．つまり，GitによりVersion管理をし，Github上でPull Requestによる変更を行い，CI/CDを可能にした．今でこそ当たり前だがこれらを可能にしたことは大きい．
TerraformもIaCに属するがTerrafromはVMやManaged DBのセットアップといったCloud providerの提供するResourceの管理に特化しており上に挙げたSoftwareとはフォーカスしてるエリアが異なる（Chefとかでもできるとは思うが&amp;hellip;）．ChefやPuppetは立ち上げられたVMをProvisioningするが，Terraformはその前段であるMachineの立ち上げることにフォーカスしてるといった感じ．この後のコンテナ時代により自分はChefやAnsibleは使わなくなり（Packerとかでコンテナでも利用できないかとか考えてた時期もあったがw），自分の中ではIaC = Terraformになっている．
また従来のShell scriptによる自動化との大きな違いはDeclarativeであることだろう．ChefやTerraformのDSLでの記述としては「こうあるべき」というDesiredな状態を書くようになり，一連の動作をにImperativeに記述するShell scriptとは大きく異なる．ここで登場したDeclarative configurationは今日も使われておりInfrastructure as Dataにも繋がる．
Immutable Infrastructure VMの立ち上げが容易になったとは言え，当時はセットアップが完了しアプリケーションが動いているMachineに新たにChefのRecipeを流し込んでSoftwareの更新を行うことは普通だった．そのためChef・Ansible時代によく言われたのは「Idemponence（冪等性）を満たせ」だった．つまりChefのRecipeを書く場合は何度も実行しても結果が同じようになるように書けという意味だ．巨大なChefのCookbookを運用した人はわかると思うが正直それは難しかった．またChefやAnsibleは1つMachineのセットアップには強いが複数のMachineのセットアップには弱く，継続的な実行が行われなとConfiguration Driftが避けられないという問題があった（Continuous Deliveryをしてないと流し漏れは発生する…自動化してても人がManualで何かを変えてしまうことはある）．
アプリケーションのDeployに関しても動いているMachineに直接変更を流し込むのは普通であり，そこにConfiguration Driftが加わることで当時のDeployはDeterministic（決定論的）ではなかった．つまり同じで同じコードをデプロイしてるのにMachineによって結果が異なることがあった．デプロイが失敗したときに確実にそれをRollbackできる保証もなかった．
この問題を解決するために出てきたのがImmutable Infrastructureである．Immutable infrastructureとは動いてるMachineには変更を加えず設定を変更したり，新しいバージョンのアプリケーションをDeployするときはVMごと作り直すという考え方である．これによりDeployはDeterministicになった．Deterministicになることで，Rollbackが容易になる = 失敗してもすぐに戻せる = 失敗を恐れずリリースを速く行うことができる，ということが可能になった．今ではImmutableになってないと怖くてデプロイはできない．
ここで登場してきたのがDockerを中心としたContainer技術である．もちろんPackerなどを使うことでアプリケーションコードごとEC2やGCEのVMイメージをつくりそれをDeployすることでImmutable infrastructureを実現することは可能である．しかし，VMと比較して起動の早さ，Registryによる配布の容易さ，そして何よりもUtilizationの高さという利点によって，Immutable DeployとしてContainerが利用されることが多くなってきた（その後のServerlessまで考えるとインフラの進化はUtilizationの改善と紐付けて考えられる）．</description>
    </item>
    
    <item>
      <title>Competing With Unicornsを読んだ</title>
      <link>https://deeeet.com/writing/2020/03/05/competing-with-unicorns/</link>
      <pubDate>Thu, 05 Mar 2020 10:53:25 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/03/05/competing-with-unicorns/</guid>
      <description>Competing with Unicorns: How the World’s Best Companies Ship Software and Work Differently
The Agile Samuraiの作者でありSpotifyにおいてAgile CoachとEngineerを努めたJonathan Rasmussonによる本．本書はUnicornもしくはTech companyがどのようにチームをつくり，組織をスケールさせ，文化を作っているのかについて書いている．タイトルにUnicornとあり複数の企業を扱ってるように見えるが，基本的には作者のSpotifyにおける体験が基になっておりSpotifyの話が中心になっている．
なぜMicroservicesか?ではMicroservicesの最終ゴールは組織にあると書いた．これは共通の見解（のはず）である一方で，Microservicesにおいてどのような組織構造・チーム構成を作っていくのが良いのかについて具体的な例を基に書かれたものはあまり見たことがない．自分は組織作りにまで関われているわけではないし，専門でもないが，これまでいくかの記事，発表を見てきた中でもSpotifyはこれを非常にうまくやっているように感じていた．
Spotifyがどのようなチームや組織を作っているかについてはScaling Agile @ Spotifyという2012年に公開されたブログが一番有名であると思う．そこではメインのコンセプトであるSquadやTribe，Guildという概念が紹介されている．また2019年にはその組織やエンジニアリング文化について紹介する20分の動画も公開されており（Spotify Engineering Culture part1・part2）2012年からのアップデートがわかる．もう一つ自分が感銘を受けたのがBreaking Hierarchy - How Spotify Enables Engineer Decision MakingというQCon New York 2019の発表で，そこではいかに組織のヒエラルキーをぶっ壊してエンジニアやチームに意思決定を促しているかについて紹介されている．
Competing with Unicornはこれらをより詳細にまとめた本になる．SquadやTribeによる組織構造や，各SquadのAutonomous（自律性）を保ちつつCompany betsによるAlignmentの方法，Productivityへの投資やこのような組織におけるLeadershipのあり方などについて詳しく解説されている．
Squad and Tribe SpotifyのようなUnicornではいかに組織を拡大していくかが大きな課題になる．スケールしつつもStartupのような俊敏さを失わないようにするのはとても難しい．Spotifyはこの課題を解決するためにSquad，Tribe，Chapter，Guildという組織構造を取り入れている．
まず一番基礎となる単位がSquadである．Squadは8人以下のメンバーで構成され，Mission vs. Projectで詳しく紹介するようにそれぞれにMissionが与えられMini-startupのように動けるように設計されている．Squadは自己組織化されており，何をどのようにつくるかという意思決定や，開発からリリース，運用までなるべく自分たちで完結できるようになっている．Squadが最も重要な単位でありその他のTribeやChapterはこれを補助するためだけに存在してる（この辺はTeam topologiesのTeam first thinkingと同じ）．SquadはCVとかにも書かれているし結構一般的に使われているっぽい．
同様のMissionをもったSquadをまとめたのがTribeである．例えば，App Integration Squad（Facebookなどのアプリケーションとの連携を行なう）やHome Consumer Electronics Squad（家電との連携を行なう）などをまとめてPartner and Platform Eeperience Tribeを構成する．TribeはDunbar’s Numberを基に40人から150人で構成され，Squadと同様にMissionを持つ．Tribeの利点は同様の課題を持ったSquadをまとめることでアイディアやコードなどを共有しやすくなることにある．自律性を保つためにSquadは協力はし合うが依存は少なくしている，Tribe間は更に依存はなくしている．
Tribe内部において特定の技術領域などでまとまったのがChapterである．例えばQA ChapterやWeb Engineer Chapterなどがある．それぞれのChapterにはChapter leadがおり採用から給与の決定，キャリアの開発などを担う．Chapterの利点はコミュニケーションを形成して最新の技術やよりよいプラクティスなどをやり取りできるようになる部分である．
Chapterを複数Tribe間にまで拡大したのがGuildである．例えばiOS Guildなどがある．ChapterとGuildの違いはGuildが基本的には任意のコミュニティであることである．iOS Guildに入るのにiOS開発者である必要はないしGuildの集まりに毎回必ず出席する必要もない．</description>
    </item>
    
    <item>
      <title>Team Topologiesを読んだ</title>
      <link>https://deeeet.com/writing/2020/02/06/team-topologies/</link>
      <pubDate>Thu, 06 Feb 2020 10:05:05 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2020/02/06/team-topologies/</guid>
      <description>https://teamtopologies.com/
DevOps consultantとして技術と組織の両面からDevOpsの支援を行なってるMatthew SkeltonとManuel Paisによる本．Consultant本は大体中身が薄く感じることが多くなり手に取ることは少なくなってきたが，各所で見かけたり，2人によるDevOpsにおけるチームのあり方のパターンをまとめたWhat Team Structure is Right for DevOps to Flourish?が良かったので読んでみた．
本書はDevOpsの視点から高速なDeliveryを実現するためにどのようなチームや組織を作るべきかについてまとめている．個人ではなくチームをDeliveryの最も重要な単位と考え（Team first-thinking），チームが最大限にパフォーマンスを発揮するために，チームの人数やその責任の範囲のデザインの仕方（Team API）から，基本的なチームタイプ（Fundamental team topology）やそのチーム間のコミュニケーションパターン（Team interaction mode）とそれをどのようの変化させていくか（Organizational sensing・Topology evolution）が紹介されている．また理論だけではなくてCase studyとして各社の事例も各章で紹介されている．
本書は大きく3つのPartからなる．Part1ではConwayの法則を再考しつつ現実の組織がいかにアーキテクチャやコミュニケーションパターンが考慮されていないか?という本書で解こうとしている問題がまとめられている．以降のPartはその解法としてPart2は基本的なチームタイプについてPart3はそのチームのコミュニケーションのパターンとそれをいかに進化させていくかについて紹介される．以下ではこれらを簡単にまとめておく．
Team-first Thinking 本書に限らず多くのところで述べられているように「小さく長期的に安定した」なチームを作ることは非常に大切である（例えば How Twilio scaled its engineering structureやHow to build a startup engineering teamなど）
「小さな」は本書では具体的には5-9人である．この人数の根拠としてDunbar&amp;rsquo;s Numberを使っている．Dunbar&amp;rsquo;s NumberはDunbarが提唱した人間が安定的な社会関係を維持できるとされる人数の認知的な上限である．簡単に言うと何人までは互いに信頼でき，何人までは覚えていられるか？という人数のラインを示している．これをMicroservicesの組織の形態でよく使われてるSpotifyのチームの形（Scaling Agile @ Spotify）に落とし込むとSquadが8-10人，Tribeが50-150人，Divisionが150-人となりチームの粒度やそのグルーピングの限度の指標に利用できる．またコミュニケーションのリンクの数からも「小さな」が良いことは理解できる（以下はチームの人数とそこでのコミュニケーションのパスの数 ．12人でも既に66もある．Two-Pizza Teams: The Science Behind Jeff Bezos&amp;rsquo; Rule）
「長期的に安定」するべきはのは，チームとは互いに信頼し合い働き方や考え方を一致させて初めて高いパフォーマンスを出せる（Tuckman&amp;rsquo;s stages of group development）ものであり，それには時間がかかるためである．プロジェクトの度にチームがコロコロ変わっていてはチームとしてのゲル化は進まずパフォーマンスも上がらない．逆に高いパフォーマンスのチームを分けたりするとProductivityは一気に落ちる．
「長期で安定した」チームを持つことで初めてOwnershipについて考えることができるようになる．Ownershipとは「Continuity of care」を持つことであり，チームはそれによって段階的に長期にものを考えることができるようになる．目の前の問題を解決する段階から，数ヶ月先のこと考えて実験を行う段階へ向かうことができるようになる．NetflixがどのようにOwnershipを育てているかについて語っているMistakes and Discoveries While Cultivating OwnershipではOwnershipのLevelを定義しており，目指すべきところ（かつLeadershipがメンバーに持たせるのは）は「Vision」であるとしている．「Vision」はまさに長期で目指すべきところを考えることに他ならない（これをさらに組織全体でTribeレベルでもできてるのがSpotify Breaking Hierarchy - How Spotify Enables Engineer Decision Making ）．長期でVisionを持つことは直前の問題解決の意思決定にも直結する（例えば3ヶ月後にxをする予定だから今はyに時間をかけずに単純なzという手法を使おうと思うことができる）．</description>
    </item>
    
    <item>
      <title>2019年振り返り</title>
      <link>https://deeeet.com/writing/2019/12/31/2019/</link>
      <pubDate>Tue, 31 Dec 2019 23:22:13 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2019/12/31/2019/</guid>
      <description>2019年のアウトプットとインプットを簡単に振り返っておく．
Working 業務でのチームとしてのアウトプットはMercari Microservices Platformの進捗（2019年）にまとめた．前年に引き続きPlatformの開発と運用を続けている．
昨年はAPI gatewayの開発など自分で手を動かすことが多かったが，今年は自分が具体的なプロジェクトを持ち自ら手を動かすことは意識的に少なくし，Tech leadとしてチームのアウトプットをどのように最大化にするか?ということを常に考えていた．技術的な視点や意思決定も時間的に影響範囲的により広く見るように意識し始めた（インプットも組織やチームに関連するものが多くなった）．見えやすいアウトプットは少ないが，プロジェクトを進めつつ，これまで曖昧だったPlatformのMissionは何かを明確に定義し，チームが拡大しても皆が同じ方向を向けるようにPlatformとしてどうなるべきか?というVisionを定義するなどした（まだうまく書けてないがチームのPrincipleとPracticeも書き始めている）．
社内基盤であってもそれをProductとして見てそれをいかに成長させるかを考えるProduct manager的な動きもしてきた．具体的にはPlatformとしてどのような機能を提供するべきかを調査し（社内向けのDeveloper surveyを行ったり，SREやSecurity，Architectチームからの意見を聞き入れたり），それらの意見と自分らがやりたいことから優先度を決め，次の半年何ができているべきか? 抽象度を上げて1-3年後にはどうあるべきか?を考えてロードマップをつくり，さらにそれをSprintベースのEpicに落とし込みそれを実行する，といったことをやってきた．うまく回りつつあるがまだ課題もあるので引き続き改善していきたい．
PlatformのMissionは定義したがチームのResponsibilityはより大きくなり1チームとしては抱える問題が大きくなりすぎてきた．そのためチームのCognitive loadが高くなりタスクの優先度を決めるのが難しくなり開発の速度も遅くなりつつある&amp;hellip;まだベストの解はないが来年は専門性に特化した形でチームを分割する，またTech leadというRoleを育てかつ移譲していくことを来年は考えていきたいと思っている．
Output Speaking 今年は以下の対外発表をした．
 開発者向けの基盤をつくる at Hackers Champloo How We Structure Our Work At Mercari Microservices Platform Team Why Microservices? at Mercari Bold Challenge  Hackers Champlooはいつか行ってみたいと思っていたイベントなのでそこで登壇できて良かった．内容も当時自分が考えていたことを詰め込むことができて良かった．
対外発表はやりすぎると業務や私生活に影響が出る（昨年はやりすぎてしんどかった）ので今年くらいの頻度（年3回）が理想的でこれを継続していきたい．今年は会社のイベントの登壇がメインだったのでもっと社外のイベントに積極的に出ていきたい．今年は海外Conferenceでの登壇はできなかったので引き続き挑戦していく．
Writing 今年はどちらかというとインプットに集中していたこともありブログはなぜMicroservicesか?くらいしか書けなかった．来年はもう少しバランスを取れるようにしていきたい（四半期に1-2本のペースで書いていく）．
Learning Conferences 今年は以下の海外カンファレンスに行かせてもらった．
 Google Cloud Next 19 SRECon 19 Asia/Pacific  SREConは初めて行ったがとても良かった．Kubernetesといった特定の技術やクラウドによらない話が多く，組織やコミュニケーションといったテーマに結構な時間が割かれているのが良かった（結局一番大切な話でもあるので）．Conferenceの規模も500人程度でそこまで大きくなく参加者の顔が見え議論しやすいのも良かった（チームのメンバーがSRECon EUに行っていたがそちらもこれまで行ったか中でも最高だったと言っていた）．
Books 今年読んだ技術書籍は別途まとめた．その中でも良かったのはWill Larsonの「An Elegant Puzzle: Systems of Engineering Management」 ．Will LarsonはInfrastructure TeamのEMであり自分の業務とも近くチームに関することから組織の話までとても影響を受けたと思う．BasecampのProduct StrategyのRyan SingerがProduct managementに関してまとめた本であるShape Upもとても面白かった．より技術的な本だとBrendan Greggの新作BPF Performance Toolsも良かった．</description>
    </item>
    
    <item>
      <title>今年読んだ技術書籍（2019年）</title>
      <link>https://deeeet.com/writing/2019/12/05/2019-book/</link>
      <pubDate>Thu, 05 Dec 2019 07:38:22 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2019/12/05/2019-book/</guid>
      <description>今年読んだ技術書籍やレポートなどをざっくりまとめてる．Infrastructure Engineer・Platfomerとして日々の業務に直結するものから1年くらいかけてやっていきたいと思っていることなどを中心に．
Kubernetes 業務ではメインにKubernetesを使っているのでKubernetesに関わる書籍は発売されれば大体目を通すようにしている．
今年発売されたので良かったのはProgramming Kubernetes．この本はCRDやOperatorによってKubernetes nativeなアプリケーションを構築することにフォーカスしている．昨年のJapanContainerDaysでのMicroservices Platform on Kubernetes at Mercariでも話したようにKubernetesを使う大きな理由の1つはその拡張性にある．Kubebuilderなど便利なフレームワークはあれどやはり中身をちゃんと理解するのが重要なのでそれを体系的に扱ってる本は重要．
Brendan BurnsのDesigning Distributed SystemsはContainerを使った分散システムの様々なBuilding BlockやPattern（例えばSidecarパターンやAmbassadorsパターンなど）を紹介している本．Kubernetes上で作るサービスのArchitectをやっていきたい人とかは一度読む良いと思う．分散システムはデザインして構築してデバッグするのが複雑で難しいが「適切にStructuredすればよりReliableであり正しくArchitectedすればよりScalableな組織に導くことができる」というパンチラインとても好き．自分がPlatfomerとしてやりたいことを端的に表している．
先日の登壇でも話したように今自分がメインに作っている基盤にはメルカリだけではなくメルペイも動いておりセキュリティはとても重要な要素になっている．Kubernetes界隈はコンテナという新しいパラダイムのためにセキュリティの進化もめざましくて新しいツールなどを追うのはなかなか大変でHotな分野でもある．AquaのLiz RiceらによるKubernetes SecurityはKubernetesのセキュリティに特化した本（Report）．若干古くなってしまったが基礎的な部分が抑えられているのでざっと目を通すのはおすすめ．
Kubernetesの本で読むべき本を聞かれたときには自分はDeveloperならKubernetes: Up and RunningでCluster AdminならManaging Kubernetesを勧める．でそのKubernetes: Up and Runningの2nd Editionが今年出た！1st Editionからの差分はResourceの解説としてRBACやIngressが，PracticeとしてYAMLをGitでどのように管理するかといった話が，Extending KubernetesとしてCRDやAdmission webhooの紹介が追加されていてよりよい感じになっていた．
SRE 自分は今はSREではないがインフラを扱うPlatformerとしてGoogle SREの考え方やプラクティスは常に参考にしている（その辺りの話をSRE NEXTで話すので皆来てくれ!!）．
Site Reliability EngineeringとThe Site Reliability Workbookに続いてGoogleのSREとSecurity TeamからBuilding Secure Reliable Systemsという本が出た．SecurityとReliabilityは両方とても重要な要素だが同時に満たすのは難しい．それぞれに想定しなければならないリスクが異なる（自然に起こる悪意のないマシン障害 vs. 悪意のある攻撃者）．Reliabilityを優先すると脆弱になるしセキュリティを厳しくしすぎるとReliabilityに支障をきたす．じゃあそれをGoogleはどうしてるのか扱ったのが本書．まだEarly Releaseで3章までしか出てない（しかも無料で公開されてる!!）がめちゃくちゃ面白かったので全部出るのが楽しみ．
もう一つ読んだのがCase Studies in Infrastructure Change Managementという本（Report）．GFSからColossusへの移行（2年）とDisklessへの移行（6年!）というGoogles社内の大規模なインフラ移行の解説とそこから得られた学びについてそれに関わったSREのTPM（Technical Program Manager）が書いたReport．業務でも今年半年くらいかけてKuberbetes Clusterの移行という大きなインフラ移行プロジェクトをやっていたのでとてもタイムリーな内容だった．自分たちが上手くやれてるところもあればもっとうまくやれたかなーということもありこのReportに書かれてる事は規模は違えど分かるわーっての多かったし次に活かせそうな学びあって良かった．
他にSREというかインフラ関連だとCindy SridharanのDistributed Systems ObservabilityとかDaniel Stenberg（Curlの作者）によるHTTP/3の解説本であるHTTP/3 explainedとかを読んだ．
Management MicroservicesやPlatformという組織に直結することに関わっているしTech Leadとして率いているチームが大きくなってきたこともあり今年はエンジニア組織やチームに関わる本も多く読んだ．この辺は技術書とは呼べないと思うけどw
今年読んだ本で一番おもしろかったのがWill LarsonによるAn Elegant Puzzleという本．Will LarsonはDiggやUberなどを経て現StripeでInfrastructure TeamのEngineering Managerをやっているひとで，それらの企業での経験をもとにEMとしてのチームのつくりかたVPやDirectorとしての組織のつくりかたが書かれている本．1000人から6000人への組織拡大みたいなHyper scaleをやってきたのはすごい&amp;hellip; あとこの本とは別にHow to invest in technical infrastructureでは共通基盤などにいかに投資していくかといったことも書いていて最近ずっと学ばしてもらっている．</description>
    </item>
    
    <item>
      <title>なぜMicroservicesか?</title>
      <link>https://deeeet.com/writing/2019/05/20/why-microservices/</link>
      <pubDate>Mon, 20 May 2019 13:10:19 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2019/05/20/why-microservices/</guid>
      <description>現職においてMonolithアーキテクチャからMicroservicesアーキテクチャへの移行とその基盤の構築に関わって2年近くが経った．未だ道半ばであるがこれまでの経験や日々のインプットをもとにいろいろ書いておこうという気持ちになった．本記事ではそもそもMicroservicesアーキテクチャとは何かを整理し，なぜやるべきか?・なぜ避けるべきかを整理する．
Microservices? Microservicesアーキテクチャとは「Single purpose，High cohesion，そしてLoosly Couploedなサービスを組み合わせてシステムを構築する」アーキテクチャ手法である．それぞれの原則をまとめると以下のようになる．
 Single purpose: 一つのことに集中しておりそれをうまくやること Loose coupling: サービスは依存するサービスについて最小限のことを知っていること．あるサービスの変更に他のサービスの変更が必要でないこと． High cohesion: それぞれのサービスが関連する振る舞いやデータをカプセル化していること. ある機能を作るときに全ての変更が一つのサービスにまとまっていること．  Microservice Architecture at Medium
Microservicesアーキテクチャをモデル化するときはこれら3つを「全て」満たす必要がある．これによってMicroservicesアーキテクチャの利点を最大限に活かすことができる．一つでも欠けると崩壊する．
 Single purposeを満たさないとそれぞれのサービスは多くのことをやることになる．つまり複数のMonolithが存在することになる Loose couplingを満たさないと一つのサービスの変更が他のサービスに影響を与えることになる．そのため（Microservices化のメリットである）素早く安全にリリースをすることができなくなる．密結合するとデータの不整合やデータロストなどが起こる High cohesionを満たさないと分散Monolithになる．つまり一つの機能の開発のために複数のサービスを変更しないといけなくなる  コードの行数が少ないから・細かなタスクを扱うからMicroserviceではない．Microservicesアーキテクチャのゴールはできる限り多くの小さなサービスを持たないことである．また新しいテクノロジーを使っているからMicroserviceではない．Kubernetes上のコンテナとして動いているからMicroserviceではない．
Why Microservices? 「Microservicesは組織論」と言われるようにMicroservicesアーキテクチャの究極的な成果物は新たな組織図である．新たなアーキテクチャに基づく新たなチームの編成，組織の再構成を狙うのが大きな目的である（逆コンウェイの戦略，Inverse Conway Maneuverなどと呼ばれる）．
What We Got Wrong: Lessons from the Birth of Microservices
組織を再編する大きなモチベーションはサービス成長に伴う組織の拡大（エンジニアの増加）に起因することが多い．組織の拡大はそのパフォーマンスの低下を引き起こす可能性がある．Accelerateは2013年から2017年の4年間を通してスタートアップを含む2000以上の企業から「いかに組織のパフォーマンスを加速させるか」という視点で聞き取り調査を行った本である．この調査結果の一つに以下のグラフがある．
このグラフはエンジニアの数とそのパフォーマンスを組織の違いによってマッピングしたものである．グラフの縦軸はここではパフォーマンスの指標として一日あたりのエンジニアあたりのデプロイ数を示している．この調査結果から，組織を拡大しても（エンジニアが増えても）パフォーマンスは必ずしも高まるわけではなくむしろその低下をもたらすことがあることがわかる．一方で指数関数的にそのパフォーマンスが高まった組織があることもわかる．パフォーマンスに起因する要素は様々だがアーキテクチャとチーム編成が与える影響は大きい．このアーキテクチャとして近年デファクトになりつつあるのがMicroservicesアーキテクチャである．
以下はMicroservicesアーキテクチャが可能にすることを端的に表した図である．
The microservice architecture is a means to an end: enabling continuous delivery/deployment
Microservicesアーキテクチャによって可能になるのは小さく自立・独立したCross functionalなチームを各サービスに配置することである．そしてそのチームに対して適切な権限を与えて構成する「組織」とMicroservices「アーキテクチャ」が可能にするのはContinuous Deliveryという「プロセス」である．この「プロセス」が「組織としてのパフォーマンスを最大化すること」を可能にする．以下のAirbnbのKubeCon 2018 North AmericaでのKeynote Developing Kubernetes Services at Airbnb Scaleのスライドがとてもシンプルにこれを伝えていた．</description>
    </item>
    
    <item>
      <title>2018年振り返り</title>
      <link>https://deeeet.com/writing/2018/12/29/2018/</link>
      <pubDate>Sat, 29 Dec 2018 23:55:16 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/12/29/2018/</guid>
      <description>2018年のアウトプットとインプットを簡単に振り返っておく．
Work 仕事で取り組んだことは全て以下のMercari Tech Conference 2018で発表させてもらった．前年から引き続きMercariのMicroservices化に向けた基盤の構築をしている．
 MTC2018 - Microservices Platform at Mercari
大きかったのは&amp;ldquo;API GatewayによるMicroservices化&amp;rdquo;で紹介したAPI gatewayのリリースそして@terryの&amp;ldquo;Mercari API: from Monolithic to Microservices&amp;rdquo;や@morikuniさんの&amp;ldquo;Listing Service: From Monolith to Microservices&amp;rdquo;で紹介されている「出品」というMercariの中でもとても重要な機能をMicroservicesとして切り出し始めたこと．
Microservices化とその基盤の整備は来年以降もさらにコミットしていく．
Output 今年は対外発表がとても多かった．上記以外だと以下のような発表をした．
 Microservices on GKE at Mercari Continuous Delivery for Microservices with Spinnaker at Mercari Microservices Platform on Kubernetes at Mercari 今学ぶべき技術  英語で登壇はできなかったが&amp;ldquo;Interview: Taichi Nakashima from Mercari&amp;rdquo;で初めて英語でインタビューを受けたり，&amp;ldquo;Mercari with Taichi Nakashima and Tonghui (Terry) Li&amp;rdquo;でGCP Podcastに日本企業として初めて出演するという経験もした（写真）．
ブログは6記事書いた．特に読まれたのは&amp;ldquo;Service meshとは何か&amp;rdquo;だった．来年は月1記事くらいは書きたい．
またAPI Gatewayを開発するなかで書いた小さなGo PackageをOSSにすることもした（mercari/go-dnscache）．長期的にはAPI GatewayそのものをOSSにしていきたい．</description>
    </item>
    
    <item>
      <title>Kubernetesがいかに自動化の考え方を変えたか?</title>
      <link>https://deeeet.com/writing/2018/12/13/how-kubernetes-change-our-way-of-automation/</link>
      <pubDate>Thu, 13 Dec 2018 00:43:34 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/12/13/how-kubernetes-change-our-way-of-automation/</guid>
      <description>先日Japan Container Days v18.12の基調講演で話をさせていただく機会があった．内容としてはMercari のMicroservices Platformの基盤として「なぜ」Kubernetesを選択したか？ついて現状や今後の展望を踏まえて紹介をした．
 Microservices Platform on Kubernetes at Mercari
「なぜ」の回答としては，CRDやAdmission webhookといった拡張機構を使うことで今後起こりうる様々なWorkloadに特化したPaaSや抽象化レイヤーを書いていけるExtensibilityの高さとそのBuilding BlockとしてのEcosystemの強さを挙げた．
このトークのExtensibilityの文脈で話したくて時間がなかったのが「Kubernetesがいかに我々の自動化に対する考え方を変えたか？」だ．本記事ではその話せなかった部分をは吐き出しておく．
Preface 「Custom Controller書くぞ！」はMercari のMicroservices Platform チーム内で自動化について議論していると必ず出てくる発言だ．
Kubernetes以前の自動化ではコマンドラインツールを書くバッチスクリプトを書くもしくはAnsibleのplaybookやChefのRecipeを書くといった手法が使われてきた．Kubernetesが当たり前になってからは長らくそれらをやっていないし問題の解法として頭に浮かばなくなった（むしろ避けている）．コマンドラインツールを書くのは好きだったが最近はめっきり書かなくなった．
Kubernetesを使ってるから当然だと思われるかも知れないがもっと深い部分で考え方が変わった．つまりKubernetesでなくてもこの考え方は通用する．「Custom Controller書くぞ！」という発言はKubernetesの思想や内部機構がもたらした新しい自動化の考え方の１つだ．
How Kubernetes works Kubernetesの大きな特徴のひとつは「Declarative Configuration」だ．KubernetesユーザはKubernetes APIに対してあるべきDesiredな状態を宣言（Declare）しKubernetesはその状態になるように「自律的に」動き続ける．例えばユーザが「Podを5つ動かす」という状態を宣言するとKubernetesはそれを受け「Podが5つ動いている状態」を維持するように動く．
これを実現しているのがReconciliation loopである．Kubernetesでは大量のControllerが動いておりそれぞれが独立したReconciliation loopを実行している．個々のControllerはシステムの一部の小さな機能を担っており他のシステムの状態に関しては感知しない．それぞれがそれぞれの問題のみを解決する．UNIX哲学的に作られた独立したControllerの集合こそがKubernetesである．
このReconciliation loopは以下の4つを繰り返しているだけである．
 Desired stateを知る 現在のstateをObserveする Desired stateとObserveされたstateの差を見つける Desiredな状態になるような処理を実行する  Managing Kubernetes
Controllerの根底にある重要な概念が「Level Triggering」と「Edge Triggering」である．以下の図のようにこれらはシステムがあるSignal（もしくはEvent）に対していつ反応するかに違いがある．「Edge Triggering」はSignalの変化に対して反応し，「Level Trigger」は状態を検知して反応する．
Level Triggering and Reconciliation in Kubernetes
抽象的にみれば結果は同じだがSampling Rateを考えると結果は変わる．例えば以下の図を考える．Signalの上昇を「on」降下を「off」としてシステムとしては「on」で1を足し「off」で1を引くとする．このとき「Edge Triggering」が何らかの理由で「off」のTriggerに失敗するもしくは逃すと最終的な状態は「Edge triggering」は2になり「Level Triggering」は1になる．
Level Triggering and Reconciliation in Kubernetes</description>
    </item>
    
    <item>
      <title>KustomizeでKubernetes YAMLを管理する</title>
      <link>https://deeeet.com/writing/2018/07/10/kustomize/</link>
      <pubDate>Tue, 10 Jul 2018 09:34:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/07/10/kustomize/</guid>
      <description>Kubernetes YAMLの壁で述べたようにKubernetesのYAML管理はKubernetesユーザにとって長年の課題だ．コミュニティでは様々なツールが議論されてきた．先日SIG-CLIから登場したkustomizeは将来的にkubectlに統合される前提で開発されている+他のツールと比べても非常に筋が良い（と感じている）．本記事ではkustomizeが登場した背景とKustomizeを使って何ができるのかをまとめる．
Declarativeであること Declarative ConfigurationはKubernetesの重要な機能の一つだ．KubernetesユーザはKubernetes APIに対してあるべきDesiredな状態を宣言（Declare）することでKubernetesはその状態になるように動き続ける．例えばユーザが「Podを5つ動かす」という状態を宣言するとKubernetesはそれを受け「Podが5つ動いている状態」を維持するように動く．
Declarative configurationの逆のアプローチがImperative configurationだ．ユーザは一連の動作を全て指示する．例えばPodを5つ立てたいならその状態になるために必要な動作を1つ1つ指示する．Imperative configurationは理解しやすい，「これをして，これをして&amp;hellip;」と書くだけでありDeclarativeの複雑なSyntaxを理解する必要はない．Declarative configurationが強力なのは「あるべき状態」を伝えられることだ．Kubernetesはそのあるべき状態を理解できるのでユーザのインタラクションと独立してその状態へ「自律的に」動くことができる．つまり問題や障害があっても自分でそれを直すことができる（Self-healing）．より詳しくはLevel Triggering and Reconciliation in Kubernetesを読むと良い．
GitOps Declarative Configurationの大きな利点の一つはGitでバージョン管理できるところだ．つまり変更をPull Requesでレビューし変更の履歴を残すことができる．そしてGitを「Source of truth」としてCI/CD workflowを構築することができる（もともとあったものに名前がついただけだが最近はこれをGitOpsと呼ぶ）．
kubectlの問題 既存のkubectlコマンド「のみ」ではこのGitOpsを実現するのは難しい．例えばSecretリソースをBinaryファイルから作成するには，まずBinaryファイルをBase64でエンコードしそこからSecret用のYAMLを作成する必要がある．この場合Source of truthはBinaryファイルでありYAMLファイルではないため別途スクリプトを準備して2つのファイルを関連させなければならない．これはConfigMapリソースの管理においても同様である．
もちろんkubectl createコマンドとそのオプションを使うこともできるがそれはImperativeなワークフローである．
YAML管理の問題 近年の多くのOSSツールはKubernetesにデプロイするためのYAMLファイルが一緒に提供されていることが多い．試すだけならそのまま利用すれば良いことが多いが会社などで実際に導入する場合は環境に合わせたカスタマイズが必要である．例えばCPUやメモリを使いすぎないように適切なResource Limit/Requestを設定したり内部ツールのためにLabelやAnnotationを別途付与する必要がある．
また本番環境だけではなく開発環境用のYAMLファイルも準備するのも普通であるが，多くの場合それらの設定は同じにはならない．例えばResource limitは開発環境では少なめに設定するのが普通だと思う．
既存のkubectlコマンドのみを使うのであれば愚直に共通の設定を含んだ複数のYAMLファイルを管理するしかない．共通部分の設定変更に漏れが生じることは避けられないしUpstreamのYAMLファイルの変更の追従も難しい．
Kubernetes YAMLの壁で紹介したHelmなどを使えばこの問題をある程度解決できる．しかしHelmはデファクトではないのでそもそもHelm Chartが存在していない場合は自分でそれを書かないといけない．またHelmのTemplate機構では変更したいYAMLのフィールドが変数として公開されていないといけない．そのためChartが公開されていてもForkが必要な場合がある&amp;hellip;
Kustomize これらの問題を解決するために登場したのがkustomizeである．kustomizeはSIG-CLIのサブプロジェクトであり将来的にはkubectlに統合される前提で開発されている（Goにおけるvgoのような開発スタイル）．より詳細な背景や既存の問題点を理解するにはKEPや公式ブログを読むのが良い．
KustomizeはYAMLファイルのDeclarative管理を推し進めReusabilityとCustomizabilityを高めるツールである．
Kustomizeの使い方 基本はGithubのREADMEやExampleを読むのが一番良い．
kustomization ファイルを使う まずkustomization.yamlを準備しapplicationをつくる．applicationにより複数のYAMLリソースをGroupingする．例えば以下のように書ける．以下ではDeploymentとService，ConfigMapリソースからapplicationを構成している．
これらに対してkustomize buildコマンドを実行することでkubectl apply可能な1つのYAMLファイルを生成できる．
これにより上述したSecretリソースとBinaryファイル，ConfigMapリソースと設定ファイルの紐づけ問題を解決しDeclarative管理を行えるようになる．例えばSecretリソースは以下のようにkustomization.yamlに記述できるためbuild時にファイルのDecryptコマンドの実行が行える．
secretGenerator: - name: app-tls commands: tls.crt: &amp;quot;cat secret/tls.cert&amp;quot; tls.key: &amp;quot;cat secret/tls.key&amp;quot; type: &amp;quot;kubernetes.io/tls&amp;quot;  ConfigMapリソースに関してはbuild時に設定ファイルの内容からhash値を計算しmetadataとnameのsuffixにそれを自動で付与してくれるので，設定ファイルを変更する度に名前がユニークになる．つまり設定ファイルを変えると新しくデプロイが走るようになる（+ロールバックも可能になる）．
さらにkustomization.yamlには共通のnamespaceやcommonLabelsを書けbuild時に各リソースにそれを差し込むこともできる．
Overlaysを使う さらに1つのKustomizationファイルをbaseとしてoverlayにより複数のvariantを生成することができる．つまり共通のYAMLファイルから一部の設定のみが異なるYAMLファイルを作成できる．例えばBase YAMLファイルから本番環境用のYAMLと開発環境用のYAMLを生成できる．</description>
    </item>
    
    <item>
      <title>Service meshとは何か</title>
      <link>https://deeeet.com/writing/2018/05/22/service-mesh/</link>
      <pubDate>Tue, 22 May 2018 21:50:11 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/05/22/service-mesh/</guid>
      <description>Microservicesの世界においてService meshは大きなキーワードになった．KubeCon 2017やKubeCon 2018 EUにおいても多くのセッションをService mesh（もしくはその代表格であるIstio）が占めており注目の高さも伺える．もちろんMicroservicesを進めるMercariにおいても導入を検討しており今後重要なコンポーネントの1つになると考えている．本記事ではそもそもなぜService meshという考え方が登場したのか，なぜ重要なのか? その実装としてのIstioとは何で何ができるのか? について簡単にまとめてみる．
参考文献 Service meshを一番理想的な形でサービスに使い始めその考え方を広めたのはLyftだ（と思う）．LyftはIstioのコアのコンポーネントであるEnvoyを開発しそれを用いてService meshを構築し自社のMicroservices化の課題を解決してきた．Service meshの初期衝動や真価を知るにはLyftの事例を見るのが良い．Envoyの作者であるMatt KleinによるKubeCon2017での発表&amp;ldquo;The mechanics of deploying Envoy at Lyft&amp;rdquo;や彼が寄稿しているSeeking SREの13章&amp;rdquo;The Service Mesh: Wrangler of Your Microservices?&amp;ldquo;などがとても参考になる．
またService meshを広めるきっかけとなったオープンソースのプロジェクトはIstioである．Istioはまだ登場したばかりであるが既に書籍がある．RedHatの開発者によるIntroducing Istio Service Mesh for Microservices（無料!）を読むとIstioの大まかな概要を掴めると思う．
さらに（これはまだ自分が読み途中だが）Zero Trust NetworksもService meshを知る上で重要な考え方の１つだと思う．
Microservicesの現状と課題 最初にMicroservicesの世界の現状と課題について簡単にまとめる
言語 MicroservicesおいてPolyglotは普通だ．そもそも適切なサービスで適切な技術を採用できるようにすることはMicroservicesの大きな目的の1つであり，その選択にはプログラミング言語も含まれる．特に現状では普通のAPIに関してはGoが選択されることが多いが，Machine learningのモデルのServingにはPythonをフロントエンド系にはNodeをという選択は普通に有り得る．正直そのサービスオーナーが運用や採用を含めて責任を持てるならHaskell使おうとRustを使おうと問題はない（はず）．
また言語によってはフレームワークも多彩であり，例えばPythonであればFlaskを使うこともあればDjangoを使うこともあるだろう．
Protocol Microservicesにおいてはサービス間はネットワーク越しにコミュニケーションを行う．そのためそのコミュニケーションに利用するProtocolも多彩になる．MercariのようにgRPCを共通のProtocolとして採用することもあれば，HTTP/1.1もしくはHTTP/2でRESTを使うこともある．MessagingとしてKafkaやCloud PubSubを使ったり，CacheとしてRedisやMemcacheを使ったり，DatabaseとしてMySQLやMongoを使うこともありそれぞれProtocolは異なる．
分散システム ネットワーク越しのリクエストが前提となるMicroservicesは分散システムである．Fallacies of distributed computing（分散コンピューティングの落とし穴）にあるように「ネットワークは信頼できる」と思ってはいけない．この分野では，リクエストが失敗したときにback-offつきでRetryを行うこと，Timeoutを設定すること，適切なRate-limitをつけ異常なリクエストをブロックすること，対象のサービスが何らかの障害で死んでしまってもCircuit breakingでそれを回避することなど多くのBest practiceが養われてきた．MicroservicesはこれらのPracticeを使う必要がある．
Observability Ultimately the most critical thing is observability. As I like to say: observability, observability, observability - Matt Klein 上で紹介したSeeking SREにおいてMatt Kleinが述べているようにMicroservicesでは可視化が全てだ．ログやメトリクス，分散Tracingを駆使してあるリクエストがどこで何が起こったのかを理解できるようにしなければならない．これをちゃんとやるにはログやメトリクスなどは一貫性のあるフォーマットに揃っている必要もある．</description>
    </item>
    
    <item>
      <title>Kubernetes上でgRPCサービスを動かす</title>
      <link>https://deeeet.com/writing/2018/03/30/kubernetes-grpc/</link>
      <pubDate>Fri, 30 Mar 2018 01:21:28 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/03/30/kubernetes-grpc/</guid>
      <description>Kubernetes上でgRPCサービスを動かすことが多くなってきている．が適切にロードバランスをする，リクエストを落とさずサービスをデプロイするためにいくつか注意することがあるので簡単にまとめておく．
以下の2つを意識する．
 Kubernetes ServiceはL4のLoad balancer（LB）であること gRPCはコネクションを使いまわすこと  KubernetesのPodは死んだり作られたりを繰り返す．KubernetesのPodにはそれぞれ内部IPがアサインされるが，このIPはPodが新しく作成される度に変わる．IPが変わってもPodにアクセスするためにKubernetesではServiceをつくる．ServiceはPodを抽象化しVirtual IP（VIP）を提供する．VIPを使うことでPodのIPが変わってもPodにアクセスすることができる．
VIPはNetwork interfaceに接続された実際のIPではない．VIPの目的は単純にTrafficを対象のPodにforwardすることであり実体はiptablesである（ちなみにVIPとPodsのIPのMappingはkube-proxyが担っており継続的にKubernetes API経由でServiceの変更を検知しiptablesの設定を更新する）．これよりKubernetesのSewrviceはTCP/IPベースの，つまりL4のLoad balancer（LB）であることがわかる．
Kubernetesは内部DNSも提供しServiceに対してもDNSレコードが作られる（e.g., my-svc.my-namespace.svc.cluster.local）．このDNSが返すレコードは上述のVIPでありPodのIPは返さない（理由としてはTTLを無視した古いDNSライブラリなどの影響を避けるためである）．
HTTP 1.1 / RESTの場合は都度コネクションが貼られるためこの内部DNSとServiceへのリクエストは問題なくロードバランスされる．がコネクションを使いまわすHTTP2の上にのるgRPCでは問題になる．単純にやると複数のServerがあろうと全てのリクエストは1つのServerにのみ到達する（L4のLBではgRPCは適切にロードバランスできない．詳しくはIntroduction to modern network load balancing and proxyingを読むと良い）．Clientを増やしても「運が良ければ」別のサービスにLBされるのみである．Clientの数がServerよりも少ないと幾つかのServerはidleになってしまう．
Client-side LB 現状の解法としてはHeadless Service（内部DNS経由で各PodのIPが取得できる）+ gRPC Client Side LBを使うのが良い．Goの場合はv1.6でDNS Resolverが入ったのでそれを使う（今までは自分で書かないといけなかった&amp;hellip;）．以下のようにDialする．
resolver, _ := naming.NewDNSResolverWithFreq(1 * time.Second) balancer := grpc.RoundRobin(resolver) conn, _ := grpc.DialContext(context.Background(), grpcHost, grpc.WithInsecure(), grpc.WithBalancer(balancer))  これによりgRPCはDNS経由でPodのIPを取得し全てのgRPC serverにコネクションを張りMethod call毎にロードバランスを行うようになる．また定期的にDNS resolveをしPodのIPに更新があればコネクションを張り直す．Freqはなるべく短くしておく必要がある．PodのIPは頻繁に変わるのですぐに更新される必要がある（デフォルトだと30分なので完全に死ぬ）．
MicroservicesのようにPolyglotを意識しないといけない場合はClient-side LBを言語毎に実装するのは現実的ではない（例えばPythonの場合はDNS cacheが問題になった&amp;hellip;）ためSidecarパターンを考えるのが良い．
Sidecar gRPC/HTTP2を扱える+Kubernetesで動かすProxyとしてはEnvoyがデファクトになりつつある．EnvoyはSidecarコンテナとしてClient Pod内にデプロイするようにデザインされている．ClientからServerへのリクエストをSidecar Envoy経由にすることでEnvoyが適切にgRPCリクエストのロードバランスを担ってくれるようになる．詳しくはUsing Envoy to Load Balance gRPC Trafficが詳しい．</description>
    </item>
    
    <item>
      <title>KubernetesでGPUを使う</title>
      <link>https://deeeet.com/writing/2018/01/15/kubernetes-gpu/</link>
      <pubDate>Mon, 15 Jan 2018 09:42:40 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/01/15/kubernetes-gpu/</guid>
      <description>一般的なWebアプリケーションと比較してMachine Leaning（ML）は複雑なインフラを要求する．Data processingを行う環境やModelのTraining/Validationを行う環境，実際にサービスからModelを利用するためのServingの環境といった複数の異なる環境が必要であり，WorkloadによってはCPUだけではなくGPUも必要になる．これらを効率的に扱うためのインフラを構築・運用するのは容易でなくGoogle and Uber’s Best Practices for Deep Learningにあるようにこれまで培われてきたDevOpsの知見を結集していく必要がある．
このような複雑なMLのインフラとしてContainerとKubernetesが利用されることが多くなってきている．特に複数の環境間のPortabilityやTrainingとServingでのScalability，各種ハードウェアやCloud Providerの抽象化においてその利点を多く享受できるからである．実際KubeCon2017感想: Kubernetes in 2018にも書いたようにKubeCon 2017ではML on kubernetesのセッションが多く見られたし，メルカリでもKubernetesを使ったMLモデルのServingを始めている（参考）．
ML on Kubernetesで特に気になるのはGPUをいかに使うかだろう．Kubernetesでは既にv1.6から実験的にNVIDIA GPUへのPodのスケジューリングが可能になっていた．そしてv1.8以降は新たに導入されたDevice PluginとExtended Resourcesという機構によってより容易にGPUが使えるようになっている．
ただしネット上でKubernetes GPUについて検索すると多くの試行錯誤が散らばっておりどの情報が最適な解かわからない状態になっている．本記事では現時点（2018年1月）において最適な方法を簡単に整理する．具体的にはGKEからGPUをつかう最適な方法についてまとめる．なおこの分野は今後も変化がある部分であるので定期的に更新を行う．
KubernetesでGPUを利用するのに必要なこと KubernetesでNVIDIA GPUを利用するには以下の3つが必要である．
 GPUを積んだインスタンスを準備する NVIDIA GPUのDriverを準備する NVIDIA Device Pluginを有効にする  以下ではこれらを順番に見ていく．
GPUを積んだインスタンスを準備する まずはGPUを準備しなければ何も始まらない．現時点（2018年1月）でGKEでGPUを積んだインスタンス（NodePool）を準備するにはAlpha clusterを使う必要がある．Alpha Clusterは全てのKubernetes APIと機能が有効になった実験用のClusterであり30日間しか利用できない．つまり現時点ではGKEでGPUはProduction readyでない．（追記）GKE 1.9.2-gke.1よりAlphaクラスタでなくてもGPUを積んだインスタンス（NodePool）が使えるようになった．またGPUを使えるRegionは限られておりアジアだと現時点（2018年2月）でasia-east1-a（Taiwan）のみが利用可能である（参考）．
GPUを積んだNode Poolを立てるには--acceleratorオプションを利用する．以下の例ではNvidia Tesla k80を利用する．
gcloud alpha container node-pools create “${NODE_POOL}” \ --cluster=&amp;quot;${CLUSTER}&amp;quot; \ --project=&amp;quot;${PROJECT_ID}&amp;quot; \ --zone=&amp;quot;asia-east1-a&amp;quot; \ --machine-type=n1-standard-2 \ --accelerator type=nvidia-tesla-k80,count=2  NVIDIA GPUのDriverを準備する 次に各インスタンスにNVIDIA GPUのDriverを準備する必要がある．全てのインスタンスにログインしてインストールを実行するわけにはいかないのでDaemonsetを使う．</description>
    </item>
    
    <item>
      <title>Kubernetes YAMLの壁</title>
      <link>https://deeeet.com/writing/2018/01/10/kubernetes-yaml/</link>
      <pubDate>Wed, 10 Jan 2018 11:42:29 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2018/01/10/kubernetes-yaml/</guid>
      <description>Kubernetes に入門しようする人を躊躇させる原因のひとつは間違いなくYAMLによる設定ファイルだろう．Kubernetesにアプリケーションをデプロイするとき，例えそれがシンプルなサーバーアプリケーションであっても，多くのYAMLファイルを手で記述する必要がある．初心者を慄かせるその大量のYAMLはよくwall of YAML（YAMLの壁）などと揶揄される．
初心者でなくてもKubernetesのYAMLは煩わしい．YAML自体は単なるKubernetes APIへのリクエストボディであり慣れてしまえば実はそんなに難しくない．しかし記述する内容のほとんどがBoilerplateであり何度も書いていると飽き飽きする（実際にはほとんどがコピペだが）．あるアプリケーションの開発環境と本番環境のYAMLファイルをいかに効率的に管理するかについて決定的な方法もない．
そもそもKubernetesの開発者はこのYAMLを利用者に書かせるつもりはなかったらしい（参考）．しかしKubernetesの誕生から3年が経ち未だにYAMLで設定ファイルを記述するスタイルは変わらない．今後もおそらく簡単には変わらないだろう．
ただこの状況を改善しようとするプロジェクトは多く存在する．それらのプロジェクトは煩雑なYAMLの記述を避けKubernetesへのアプリケーションのデプロイの敷居を下げることを目標としている．そして今どのプロジェクトがデファクトになっていくかが注目されている．
本記事では現時点（2018年1月）においてどのようなプロジェクトがあるのかその状況および問題を簡単にまとめる．私見や自分がどのように使っているかは書くがあくまで現状の整理を行う．
Helm https://helm.sh
まず現時点で一番有名なのはHelmだろう．Helmは Kubernetes Package Managerである．
Kubernetes Package Managerとは何か? KubernetesをKernelとみなすと複数のNodeインスタンスを束ねたClusterを１つのコンピューター，その上で動くコンテナはプロセスとみなすことができる．このような視点になるとHelmはCentOSにおけるYUM，DebianにおけるAPTと同様の役割を果たす．例えばGrafanaをインストールしたい場合は以下のようにできる．
$ helm install stable/grafana  このようにHelmはKubernetes上で動くアプリケーション（コンテナ）のパッケージング及び配布の機構を提供する．
HelmのパッケージングのフォーマットはChartと呼ばれる．Chartの中身は単純にYAMLのTemplateである（より具体的にはGo Templateである）．デプロイ時にこのTemplateの変数に対して具体的な値を渡すことでYAMLを作りそれをクラスタに実行する．例えばDockerイメージのバージョンだけを切り替えたい場合はTemplateは以下のようになる．
containers: - name: example image: gcr.io/deeeetlab/example:{{ .Values.version }} ....  HelmはWall of YAMLに対するシンプルな解法だ．Boilerplateを共通Templateとしてしまい変更が必要な部分のみを外部から与えられるようにする．Helmはコミュニティにも受け入れられており既に多くのChartが公式のレポジトリに存在する．
メルカリでもHelmは使っている．具体的にはSREが提供する共通のツールはHelmでパッケージングしている．chartmuseumを使ってインターナル向けのレポジトリも準備しているところだ．
Helmの課題 Helmが完璧かと言われるとそうでもない．特にセキュリティに関してはExploring The Security Of Helmで指摘されているように少しザルすぎるように思える．
はっきり言ってHelmでmicroservicesのパッケージングはやりたくはない．HelmのTemplateは結局YAMLでありChartを書くには結局YAMLを書く必要がある．さらにGo templateを駆使した方法はスマートには見えないところが多い．YUMやAPTでAPIアプリケーションの配布をしたいか?と言われたら答えは「No」だろう．Helmはあくまで共通系のツールの配布にしか向いていないと感じている．
共通系のツールの配布としても完璧とは言えない．公式で提供されているChartに対するカスタマイズは公開されている一部の変数のみにしか許されない．カスタマイズ性は著しく低くその作者の力量にも大きく依存する．ツールの設定ファイルなどをYAMLの中に直接書いてあったり気持ち悪い部分も多い．
Helmでインストールしたパッケージの設定ファイルをいかに宣言的に管理するか？に対する解も存在しない．APTやYUMに対してChefやAnsibleが登場したようにHelmに対してももう一段ハイレベルな管理ツールが求められるだろう．
とはいえHelmは広く使われておりデファクトになりつつある．今年3.0もリリースされる予定になっている．今後の進展は要チェックである．
ksonnet https://ksonnet.io
Helmの後発として2017年に登場したのがksonnetである．まだ登場したばかりだが既にkubeflowなどでの採用実績がある．
ksonnetはKubernetesの設定ファイルの記述にjsonnetを採用している．jsonnetはGoogleで開発されたJSONを定義するためのDSLである．変数やオブジェクトの連結などが可能なJSONのサブセットだと思えば良い．またKubernetesのAPIオブジェクトは予めksonnet-libとして共通化して提供されているためBoilerplateを避けることができる．例えばNginxコンテナのDeploymentの記述は以下のようになる．
これによりYAML問題は避けられている．
ksonnetはさらにもう一段進んだ機能を持つ．まずPrototypeというコンセプトを持つ．これはjsonnetによるTemplateである．デフォルトでいくつかの共通パターンが提供されている．例えばDeployment resourceとService resourceを使ったアプリケーションのデプロイはよく使うパターンだがこれはdeployed-serviceという名前のprototypeで提供されている．ユーザーはこのprototypeに具体的な値を与えて実際のデプロイの構成する．Helmと同様にPrototypeを自分で書き配布することもできる．
次にksonnetはEnvironmentというコンセプトを持つ．これはkubernetes本家にもHelmにもない概念だ．ksonnetはどの環境，つまりどのクラスタに，設定ファイルを適用するかを強制する．各環境ごとの設定値の変更もスマートに行える．これにより間違ったクラスタに設定ファイルを適用してしまった&amp;hellip;という問題を避けることができる．
ksonnetのコンセプトと全体像は以下の図でまとめられる．
ksonnetは流行るか コンセプト自体はとても好きだが何とも言えない．自分の中ではjsonnetがどこまで書けるようになるかが懐疑的だ．皆がこれを普通に扱えるようになるようになるのかもわからない．VS CodeのExtensionが提供されていたりするのでIDE前提にはなりそうだが&amp;hellip;
kubeflowによる採用事例はあるがまだ少ない．生態系が整っていくと未来はあるかもしれない．Helmでサポートも考えられているらしいので（参考）より広く使われる機運はありそうと感じている．
興味がある人はTutorialとTourをひと通り見てみると良い．
kubecfg https://github.com/ksonnet/kubecfg</description>
    </item>
    
    <item>
      <title>2017年振り返り</title>
      <link>https://deeeet.com/writing/2017/12/31/2017/</link>
      <pubDate>Sun, 31 Dec 2017 02:10:01 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/12/31/2017/</guid>
      <description> 今年は激動の一年だった．
1月．SREとしてMercariに転職した．試用期間の間は既存のGolangミドルウェアの改善の手伝いをしたり，Botを書いて業務の自動化をしたり（GolangでSlack Interactive Messageを使ったBotを書く），デプロイの高速化などをやっていた．
4月．試用期間が終わった直後くらいにUSへの長期出張が決まった．ちょうどMercari USが日本に先行してMicroservices化に舵を取り始めた時期でその土台作りなどを手伝った．Spinnakerを使ってCD環境を整えたり（SpinnakerによるContinuous Delivery），CI環境を整えたり，gRPCを使ったサービス自体を1から書いたり，Cloud PubSubを使ったイベント連携を仕込んだりなどやれることは全部やったと思う．あとgo-httpdocというGoパッケージを書いてOSSとして出すなどもした．
他にも出張中はCoreOS Fest 2017やGopherfest 2017，GoSF，Spinnaker 1.0 Launch Partyに参加したりもした．このようなイベントが近場で開催されていてさっと行ける，使ってるツールの作者や大規模に運用している企業のひとが普通にいて直接話を聞いたりできる，というのはとにかく良くてここで働きたいなあという気持ちが強く芽生えた．
7月．日本に帰ってきてからはUSでの知見を活かし日本でもMicorservices化の推進を始めた．SREの中に専門のチームもでき良い体制ができつつある．加えてMachine Learningのプロダクトを安全かつ高速にリリースするためのプラットフォームの構築の手伝いも始めた（具体的な成果はメルカリの今年1年間の機械学習の取り組みとこれからで紹介されている）．
8月．長期出張などもありずっと休めていなかったので長めの休みをとってノルウェーに行き友人の結婚式に参加した．海外の結婚式に出席するのは初めてでとても良い経験だった．
9月．第 1 回 Google Cloud INSIDE Games &amp;amp; AppsにてMicroservices化の取り組みについて発表した（Microservices at Mercari）．ここでの発表は反響も大きく良いフィードバックをたくさんもらえて良かった．
10月．結婚した．日本に帰ってきてからは結婚の準備と仕事との両立でとにかく大変だった&amp;hellip;（例のリストです）．
11月．Go Conference 2017 Autumnで発表した（Go+Microservices at Mercari）．Go+gRPCで書かれたMicroserviceをいかにk8s上で動かすかについての知見を紹介した．
12月．KubeCon + CloudNativeCon North America 2017に参加した．KubernetesはMercariのMicroservicesにおいて重要な基盤になっておりどのセッションも学びしかなかった（具体的な感想はKubeCon2017感想: Kubernetes in 2018に書いた）試したいことやアイディアがたくさん溜まっているので冬休み中にいろいろ検証したいと思っている．
2018年 2018年も引き続きMicroservicesとMachine Learning Opsが自分の中で大きな2軸になると思う．
今年のうちにMicroservices化をさらに加速させるための仕組みを作れたと思う．が来年多くのサービスが作られる中で解決するべき課題はまだまだたくさんあり挑戦は終わらない．
Google and Uber’s Best Practices for Deep LearningにあるようにMachine Learningの分野においてもSREやDevOpsエンジニアはますます重要な職種になっている．これまで培ってきたTDDやCI/CD，Immutable Infrastructureといったプラクティスをいかに応用してくかが大切だと思う．まだとりあえずCloudに投げれば良いってのがやりにくい分野でもあるのでやりがいがある．
転職していきなりMicroservices化という大きな仕事に抜擢してもらえてかなりやりがいのある一年だった．かなり楽しいし刺激的だけどその一方でプレッシャーも大きく感じていてそれを振り払うように働きまくってしまったように思う．来年は仕事外にもちゃんと時間を割きリズムを崩しすぎないようにしたいと思う．
References  2016年振り返り 2015年振り返り  </description>
    </item>
    
    <item>
      <title>Golangのcontext.Valueの使い方</title>
      <link>https://deeeet.com/writing/2017/02/23/go-context-value/</link>
      <pubDate>Thu, 23 Feb 2017 09:20:07 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/02/23/go-context-value/</guid>
      <description>Go1.7でcontextパッケージが標準パッケージに入りしいろいろなところで使われるようになってきた．先日リリースされたGo1.8においてもdatabase/sqlパッケージなどでcontextのサポートが入るなどますます重要なパッケージになっている．
&amp;ldquo;Go1.7のcontextパッケージ&amp;rdquo;で書いたようにcontextは「キャンセルのためのシグナルの受け渡しの標準的なインターフェース」として主に使われる．ある関数やメソッドの第1引数にcontext.Contextが渡せるようになっていればキャンセルを実行したときにその関数は適切に処理を中断しリソースを解放することを期待する．これはパッケージの作者とその利用者との間のある種の契約のようになっている（パッケージ側でgoroutine作るなというパターンもここで効いてくる）．
これだけではなくcontext.ContextインターフェースにはValueというメソッドも定義されている．これを使うと任意の値を受け渡すことができる（contextと言われるとこちらを想像する人も多い）．これは便利だが注意して使わないと崩壊するのでどう使うべきかをまとめておく（contextも分かりやすい）．
なぜ注意が必要か? context.ValueのSetとGetは以下のように定義されている．
WithValue(parent Context, key, val interface{}) Context  Value(key interface{}) interface{}  WithValueで値をセットしValueで値を取り出す．注意するべきなのは型を見ればわかるようにtype-unsafeでコンパイラでチェックができないからである．要するにmap[interface{}]interface{}である．つまり避けれるなら避けた方が良い．
例えばチームでAPIサーバーを開発していてあらゆる値が様々なHandlerで無防備にSetされたりGetされたりするようになると崩壊する．
どのようなときに使えるか? ではどのようなときにValueは有用になるか? ある特定のリクエストスコープ内で限定的な値を渡すのに便利に使える．例えば以下のようなものが考えられる．
 ユーザID 認証情報（Token） Distributed TraceのID  どのような値を渡すべきで*ない*か? あるいは適していないか．例えば，DB ClientやAPI Client，loggerなどである．これらはスコープに限定的ではないしそもそもテストがしにくくなる．これらはサーバーが依存として持つべきである．以下のようにmiddlewareで渡すかhandlerに持たせる（ジョブワーカーを書いている場合もStructを定義してそこに渡すべきである）．
func MyMiddleware(db Database, next http.Handler) http.Handler { return http.HandlerFunc(func (w http.ResponseWriter, r *http.Request) { // Use db here. next.ServeHTTP(w, r) }) }  type MyHandler struct { db Database } func (h *MyHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {...}  使い方 context.</description>
    </item>
    
    <item>
      <title>SREとしてMercariに入社した</title>
      <link>https://deeeet.com/writing/2017/02/13/mercari/</link>
      <pubDate>Mon, 13 Feb 2017 09:47:24 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/02/13/mercari/</guid>
      <description>1月16日よりMercariにてSRE/BSE（Backend System Engineer）として働いてる．
これまではとある会社で社内向けのPaaSエンジニアとして働いてきた（ref. PaaSエンジニアになった）．PaaSの目標である「アプリケーション開発者の効率を最大化」を突き詰めながら少人数のチームでいかにScalableなプラットフォームを構築するかに注力してきた．Cloud FoundryやDockerといったインフラの最前線とも言える技術やアーキテクチャに触れ，かつその中で自分の技術的な柱である自動化に取り組むことができたのは非常に刺激的で自分に大きなプラスになった．
その一方でPaaSというプラットフォームはその性質上サービスそのものからは中立的になることが避けられない（だからこそScalabilityを実現できるのだが）．よりサービスに近い部分，サービスの成長に直結する部分で開発がしたい，それを支えるインフラに関わりたいと思うようになった．
Mercariという選択は，サービスやその可能性/成長，働いているひと，使っている技術といった視点から行った．が一番の大きな理由は日本発のサービスとしてグローバルな市場を本気で獲りに行っているところだ．UberやAirbnb，Netflixを見ていても今後Webサービスはグローバルで闘えることが必須になるだろうし技術的に一番チャレンジングかつ面白いものそこにあると思う．自分が身に付けたいと思う技術もそこにある（実際早くもそれに関わる仕事ができた）．
Site Reliability Engineering (SRE)はGoogleが提唱し多くの企業で採用され始めている職種である．その初期衝動はソフトウェアエンジニアリング的にいかにインフラを設計するかにあり，Error Budgedという現実的な指標をもってInnovationの促進とReliabilityの担保というトレードオフを解決しているところが大きな特徴であると言える．これらの思想をもとに各社が独自の実装をしているのが現状だと思う．
Mercariは早くからこのSREという名前を採用している（ref. インフラチーム改め Site Reliability Engineering (SRE) チームになりました）．メンバーの各人がそれぞれの得意分野をもちAvailabilityやPerformance，Security，Deploy Automationに取り組んでる．まだチームに入って1ヶ月程度だが皆がどんどん問題を解決してるのをみて圧倒されている．自分に足りない部分は吸収しつつPaaSの世界で培ってきた考え方や技術で貢献していきたいと思う．また自分なりにSREとは何ぞやというもの考え行動していきたい．
Backend System Engineer (BSE)という職種はMercariのSREの中でもソフトウェア開発に重点を置いている職種である．開発言語にGolangを使いアプリのPush基盤やAPI gateway，Proxyサーバーといったミドルウェアを開発している．近年一番自分が使い込んできたGolangという技術を一番理想的な形で使えるのも非常に楽しみだ（とりあえずGo1.8対応はしたgaurun#57，widebullet#10）．
これからよろしくお願いします🙇
（とりあえず席は@kazeburoさんと@bokkoさんに挟まれていて緊張感がある&amp;hellip;！）</description>
    </item>
    
    <item>
      <title>Writing An Interpreter In Goを読んだ</title>
      <link>https://deeeet.com/writing/2017/01/12/go-interpreter/</link>
      <pubDate>Thu, 12 Jan 2017 17:06:29 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2017/01/12/go-interpreter/</guid>
      <description>Thorsten Ballによる&amp;ldquo;Writing An Interpreter In Go&amp;rdquo;を読んだ．
技術界隈のブログを見ているとたまにSteve Yeggeの「If you don’t know how compilers work, then you don’t know how computers work」という言葉に出会う．その度に学生のときにコンパイラの授業を受けなかったこと後悔し，社会人になって挑戦しようとして挫折したことを思い出して悲しい気持ちになる．@rui314さんのCコンパイラをスクラッチから開発してみたを読んではかっこいいなと思いつつ僕には無理だなあと心が折れていた．
どの言語を書いていてもコンパイラ（もしくはInterpreter）は切っても離せないものであり内部の動きがどうなっているかを知っておきたいという欲求はプログラマーなら誰しもあると思う（少なくとも僕にはある）．では他にも学ぶことがたくさんあるという時間制約の中でベストな学習リソースは何かと言われると自分の観測範囲ではなかなか良いものに出会うことはなかった．Dragon Bookは重すぎるしLispの処理系をx行のRubyで書いて見ました系ブログは軽すぎる．
本書&amp;ldquo;Writing An Interpreter In Go&amp;rdquo;はその1000ページのコンパイラ本と個人ブログのギャップを埋めるために書かれた本である．紙の本にした200ページ程度でさっと読める．なんちゃって言語を実装するのではなくMonkeyという本書のためにデザインされたある程度まともな言語（C言語っぽい）を実装する．スクラッチから初めてLexer，Parser， Evaluationを実装していく（HostにGolangを使うのでアセンブラなどまでは踏み込まない）．テストまでちゃんと書くとだいたい2000行程度で実装できた（時間にするとだいたい1週間程度）．
本書の利点を挙げると，
 サンプルコードがGolangで書かれている（かつ標準パッケージのみが使われている）．Golangはとにかく言語仕様のシンプルであるため本書のサンプルコードを読むのはとても簡単である．また自分の好きな言語に移植するのも容易であると思う． サンプルコードを動かしながら読み進めることができる．最新のGolangのruntimeで動かすことができるので環境を準備するのはたやすい．Lexerを書けばここまでできて，Parserを書けばここまでできて&amp;hellip;と読むことができて理解度が高い． テスト駆動で書かれている．本書に登場するコードはすべてテストもセットになっている．テストのおかげで何を期待するのかをすぐに理解することができた．またテストはGolangのベストプラクティスであるTable Driven Testsが採用されているため読みやすい（ただし途中でテストも写経するのはめんどくなった&amp;hellip;）  これらの利点以上に感動したのが本書の書き方である．そもそも作者はコンパイラを職業にしているひとではない．個人的なpassionでコンパイラについて知識を深めてきたひとである（詳しくは作者が出演したGo time #28 を参考）．そのために前提がとても優しい．僕のようなゴリゴリのCS出身ではないプログラマが疑問に思うことを一つ一つちゃんと拾ってくれる（書き方も柔らかくて「これ疑問に思ったっしょ？次にちゃんと説明するから！」的に書かれていて良い）．
表層的な部分だけでなくて内容に少し触れておくと，本書で一番面白かった・感動したのはParserの実装である．Paserは実装するのではなくyaccやbisonなどのParser generatorを使うのが一般的らしいが本書ではそれらのツールを使わない．すべて自分で1から実装する．特にExpressionをParseするためのPratt parser（JSLintで使われている）は他のParserを知らないため比較はできないがとにかくシンプルで感動した（デバッグとかしんどいしPaser generator使わなくてもシンプルにできるよという話もあるHandwritten Parsers &amp;amp; Lexers in Go．実際自分で書いたりGolangの実装を覗いたりしてみるとシンプルなものなら自分で書いても良いのでは?という気持ちにはなった）．
あとGolangを書いているひとにオススメなのはGolangのコンパイラ実装であるhttps://golang.org/pkg/go/と合わせて読むこと．名前のつけ方が似てるので比較しながらコードを追うことができとても勉強になる．
まとめ コンパイラのことがわかったのかと言われるとまだまだ自信を持ってYesとは答えられない．が全く知らないという状況は抜け出したと思う．前よりも恐れがなくなったというか身近に感じている．これを入り口にしてさらに専門的な本を読んでみようという意欲も湧いている．とにかく読んでよかった．
少しでもコンパイラに興味があり入り口が見つけられない人は是非手にとると良いと思う．オススメです！</description>
    </item>
    
    <item>
      <title>2016年振り返り</title>
      <link>https://deeeet.com/writing/2016/12/31/2016/</link>
      <pubDate>Sat, 31 Dec 2016 13:16:08 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/12/31/2016/</guid>
      <description>最初に今年やったことなどをつらつらと書いてみる．
2月．Dave Cheneyが中心となりGo1.6のRelease Partyが世界各地で開催されることになった（Go 1.6 release party）．東京でもやりたかったのでOrganizerとなりHatenaのオフィスを借りて開催した．8月のGo1.7のリリース時は特に世界規模でやる流れはなかったがフォーマットだけは借りて再びHatenaのオフィスで開催した．この時リリースの概要をまとめたスライドを作ったが@bradfitzがそれを改変して別のMeetupの発表資料に使ってくれた嬉しかった．2017年2月にリリース予定のGo1.8のリリース時は再び世界規模でやるかってのをDaveと話したのでぜひ参加してください．
5月．みんなのGo言語の執筆を主に行っていた．自分はコマンドラインツールに関する章を担当した．今まで発表やブログ記事の集大成的に書けたので良かった．（村上春樹の手法を真似て）朝5時に起きて午前中のみ/決められた分量のみに集中するという方法で書き進めたがうまくいった気がする．9月に販売したが概ね高評価でとても嬉しい（ブログなどは基本見つけたものは目を通してます！ありがとうございます）．日本語だけどDaveやGo teamの@rakyllにも手に取ってもらえた．
6月．ドイツのBambergにある支社で2週間ほど働いた．本社のメインチームと離れてリモート側になった時の問題を身にしみて感じた．Bambergはかなり田舎でみながゆっくりと生活していた．同僚とひたすらビールを飲んでいた．Berlinも休日を使って訪問したが町中からエネルギーを感じて良い街だった．いつか住んでみたい．
7月．Devenverで開催されたGopherConに参加した．有名な（プロジェクトの）開発者が普通にうろうろしている状況でとても刺激的だった．自分もLTの舞台で発表した（詳しくはGopherCon 2016でLTしたに書いた．動画もある）．次は通常セッションで参加したい．
12月．golang.tokyoでテストしやすいGoコードのデザインという発表をした．自分が今のチームでコードを書く時に/レビューをするときに意識してきたことをまとめて発表する良い機会だった．こちらも良いフィードバックをもらえたので嬉しかった．
そして12月31日をもってRakutenを退職した．新天地でも引き続きやっていきます！
書いた記事 今年はアウトプットよりもインプットを重視したため去年の半分の14本だった．年間PVは229,231 viewsだった．特に読まれたのは以下の記事．順番はPV順．
 Golangのエラー処理とpkg/errors GolangのGCを追う GolangでAPI Clientを実装する Go1.7のcontextパッケージ Golangにおけるinterfaceをつかったテスト技法  英語で書いた記事はTracing HTTP request latency in golangが多く読まれた．
来年も引き続きインプット重視の予定なので今年同様に月1本程度書けると良い．
公開したOSS OSSとしては以下を書いた．
 https://github.com/tcnksm/gotests (★254) https://github.com/tcnksm/go-input (★147) https://github.com/tcnksm/go-httpstat (★115) https://github.com/tcnksm/dutyme (★17) https://github.com/tcnksm/go-irkit (★6)  上の3つはGo Newsletterでも取り上げてもらえた．
良かった記事 良いと思った記事や発表はブクマで記録してるが今年は1000ほどあった．その中でも特に印象に残ったのは以下（Golangに関しては多かったので別途まとめた 良かったGolangの記事/発表（2016年））．
 Unikernels are unfit for production - Blog - Joyent The Netflix Tech Blog: Evolution of the Netflix Data Pipeline JavaScriptの文化とleftpadの話とpadStartについて - from scratch Your Life Is Tetris.</description>
    </item>
    
    <item>
      <title>Systems Performanceを読んだ</title>
      <link>https://deeeet.com/writing/2016/11/07/systems-performance/</link>
      <pubDate>Mon, 07 Nov 2016 09:25:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/11/07/systems-performance/</guid>
      <description>Brendan Greggによる&amp;ldquo;Systems Performance: Enterprise and the Cloud&amp;rdquo;を読んだ．
Linux（Solaris）のパフォーマンスの分野でBrendan Greggという名前を聞いたことがあるひとは多いと思う．名前を知らなくてもが書いているブログやカンファレンスでの発表資料を見かけたことはあると思う．また彼が開発したFlame Graphにお世話になってるひともいるのではないか（ref. GolangでFlame Graphを描く）．とにかくパフォーマンスに関して常に先端にいるひとである．
そんな彼がSystems（ここでいうSystemsとはCPUやメモリといったハードウェアとKernelやOSといったソフトウェアを指す）のパフォーマンスについて内部のアーキテクチャーを含め徹底的に解説したのが本書である．面白いに決まってる．
本書の根底にある考え方は前書きに書かれているknown-knowns，known-unknownsそしてunknown-unknownsという考え方である（詳しくはKnown unknownsに書いた）．パフォーマンスのチューニングやボトルネックについて考えるとき僕らは何がknownで何がunknownであるかに意識的にならないといけない．このような前提があり本書は各コンポーネントやソフトウェアを低位なレベルから詳細に紐解いていく．
詳細であるから良いというわけではない．僕はLinuxについてより深い理解を得たいと思ったときにO&amp;rsquo;ReillyのLinuxシステムプログラミングや詳解Linuxカーネルといった本に助けを求めた．これらは学びはたくさんあったが淡々と事実が並べられるだけで素直に面白いとは思えなかった（特に詳解&amp;hellip;は全てを読むことはやめてリファレンス的にしか使っていない．それゆえ辞書と呼ばれるのだろう）．
本書が面白いのは「パフォーマンス」という明確なゴールをもった上で低位な部分を解説しているところだと思う．例えばCPUについてアーキテクチャーはこうでスケジューラーはこうなっていてと説明をしながらここがパフォーマンスで問題になるといった説明をする．なぜこれが良いか？を考えた時に結局そこが日々の業務に直結するからだろうと思った．闇雲に深いレベルことを学んでも知識にはなっても業務には生かせない（もちろん大切なことだと思う）．
目次は以下のようになっている．
 Intro Methodology Operating Systems Observability Tools Applications CPUs Memory File Systems Disks Network Cloud Computing Benchmarking Case Study  本書はまずMethodology（方法論）から始まる．これが良い．パフォーマンスの何が難しいって「どこから始めるか？」である．もちろん熟練したひとたちにはそれなりのスタイルなどはあると思う．例えばNetflix（これはBrendanが書いてるが）の Linux Performance Analysis in 60,000 Millisecondsや@y_uuk1くんのLinuxサーバにログインしたらいつもやっているオペレーションがある（これは本書だとTool Methodになるかな）．これらは会社ごとに特化したものであって，よりGeneralなものは明確に言語化されてるとは言いがたいのではないかと思う．だから適当にググってヒットしたコマンドをとりあえず試してみる，といったことになる．方法論は僕のようにパフォーマンスに苦手意識を持っている人間に道筋を与えてくれるものだった．例えばUSE Methodはすぐに役に立っている．
それ以降はOSやCPU，メモリそしてネットワークに関しての詳細な説明が続く．各章の枠組みは一致していて基本的な用語の解説から始まり，コンセプト，アーキテクチャー，そしてそのコンポーネントで使える方法論（具体的なコマンドなど）が説明される．どのコンポーネントも徹底的に解説され非常に勉強になる．本書で特徴的なのはLinuxだけではなくSolarisについても言及されるところだろう．別のOSについての理解/比較は今自分が使っているOSへの深い理解にもつながる（Solarisについては流す程度で読んだけどあのTracabilityは良いなあと思ってしまったよ！）．
そしてCloud Computingの解説もある．これは近年避けられないテーマだ．OS virtualizationとHardware virtulizationそれぞれについて解説し比較が行われる．ホスト側とゲスト側の両方の視点がある．もちろんパフォーマスに関して留意するべきことも紹介される．
Benchmarkingの章は「There are lies, damn lies and then there are performance measures」という言葉から始まる．ベンチマークを行う側，そして読む側が何に気をつければ良いかについて理解できる．「ベンチマークの結果についてその結果の分析に1週間かけていなければそれはきっと間違っている」とまで言い切っている．
最後のCase Studyもとても面白かった．この章は前章で解説してきたことの集大成的な章になっている．ここではBrendannが実際に対応したパフォーマンスの問題について，彼が何を考え，どのような方法論でそれに立ち向かい，誤り，そしてそこからどのように解決に向かったが語られる．ここは結構彼の人となりが出ていて笑いながら読んだ（どんなけDtrace使いたくなってるんだとかね）．熟練のひとが何を考えているのか知れるのは方法論とは違った道筋を与えてくれる．また初心者はこの章を先に読んでも良いと勧められている．本書で解説されるものごとの意義がより伝わりやすくなるからだろう．
まとめ 紙の本で700ページ以上もあり他の本に浮気しつつ読むのに半年以上もかかってしまった．それでも上でべた褒めしたようにどのページをめくっても学びしかなかった．僕はこの本を何度も読み直すと思うし多くのひとに勧めたいと思う．少し高いがそれ以上の価値はあると思う．今後インフラ界隈（DevOpsやSRE）の必読書になっていくでしょう．オススメです．
もともとは@y_uuk1が読んでいたのを見て読み始めた．本当に良い本を紹介してくれてありがとう</description>
    </item>
    
    <item>
      <title>GolangでAPI Clientを実装する</title>
      <link>https://deeeet.com/writing/2016/11/01/go-api-client/</link>
      <pubDate>Tue, 01 Nov 2016 11:42:19 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/11/01/go-api-client/</guid>
      <description>特定のAPIを利用するコマンドラインツールやサービスを書く場合はClientパッケージ（SDKと呼ばれることも多いが本記事ではClientと呼ぶ）を使うことが多いと思う．広く使われているサービスのAPIであれば大抵はオフィシャルにClientパッケージが提供されている．例えば以下のようなものが挙げられる．
 https://github.com/aws/aws-sdk-go https://github.com/Azure/azure-sdk-for-go https://github.com/PagerDuty/go-pagerduty https://github.com/hashicorp/atlas-go  特別使いにくい場合を除けば再実装は避けオフィシャルに提供されているものを使ってしまえばよいと思う（まともなものなら互換性などをちゃんと考慮してくれるはずなので）．一方で小さなサービスや社内のサービスの場合はClientは提供されておらず自分で実装する必要がある．
自分はこれまでいくつかのAPI client パッケージを実装してきた．本記事ではその実装の自分なりの実装パターン（各人にやりかたはあると思う）といくつかのテクニックを紹介する．
Clientとは何か? API ClientとはAPIのHTTPリクエストを（言語の）メソッドの形に抽象化したものである．例えば https://api.example.com/users というエンドポイントからユーザ一覧を取得できるとする．API Clientは具体的なHTTPリクエスト（メソッドやヘッダの設定，認証など）を抽象化し ListUsers()のようなメソッドに落とし込んでその機能を提供する．
なぜ Client を書くべきか? そもそも共通化できることが多いため．それぞれのリクエストは独立していても例えばユーザ名やパスワード，Tokenなどは基本は同じものを使うし，ヘッダの設定なども共通して行える．またテストも書きやすくなる．
いつClientを書くべきか? 複数のエンドポイントに対してリクエストを投げる必要がある場合はClientを書いてしまえばいいと思う．例えば，単一のエンドポイントに決まったリクエストを投げるだけであればClientをわざわざ書く必要はない．自分の場合は3つ以上エンドポイントがあればClientをさっと書いていると思う．
基本的な実装パターン 以下では https://api.example.com （存在しない）のAPI Client パッケージを実装するとする．このAPIでは/usersというパスでユーザの作成と取得，削除が可能であるとする．また各リクエストにはBasic認証が必要であるとする．
パッケージの名前をつける https://golang.org/doc/effective_go.html#package-names
上のEffective Goにも書かれているようにパッケージ名は shortかつconciseかつevocativeのものを選択する．API Clientであればそのサービス名がそのままパッケージ名になると思う．例えば PagerDutyであれば pagerdutyがパッケージ名になる．
名前については以下でもいくつか述べる．
Client（struct）を定義する まずはClient structを実装する．Clientのフィールドにはリクエスト毎に共通に利用する値を持たせるようにする．HTTP APIの場合は例えば以下のようなものが考えられる:
 url.URL - リクエスト毎にパスは異なるがベースのドメインは基本的には共通になる．例えば今回の場合は https://api.example.com は共通である http.Client - 各HTTP リクエストにはnet/httpパッケージのClientを用いる．これは同じものを使い回す 認証情報 - 認証に利用する情報も基本的には同じになる．例えば今回の場合はBasic認証に必要なユーザ名とパスワードは共通である．他にもTokenなどが考えられる log.Logger - デバッグの出力も共通である．自分はグローバルなlogを使うよりも明示的に指定するのを好む  今回の場合は以下のように実装できる．
type Client struct { URL *url.URL HTTPClient *http.Client Username, Password string Logger *log.</description>
    </item>
    
    <item>
      <title>Golangにおけるinterfaceをつかったテスト技法</title>
      <link>https://deeeet.com/writing/2016/10/25/go-interface-testing/</link>
      <pubDate>Tue, 25 Oct 2016 09:26:51 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/10/25/go-interface-testing/</guid>
      <description>最近何度か聞かれたので自分がGolangでCLIツールやAPIサーバーを書くときに実践してるinterfaceを使ったテスト技法について簡単に書いておく．まずはinterfaceを使ったテストの基本について説明し次に自分が実践している簡単なテクニックをいくつか紹介する．
なおGolangのテストの基本については @suzuken さんによる「みんなのGo言語」 の6章が最高なので今すぐ買ってくれ！
前提 自分はテストフレームワークや外部ツールは全く使わない．標準のtestingパッケージのみを使う．https://golang.org/doc/faq#Packages_Testing にも書かれているようにテストのためのフレームワークを使うことは新たなMini language（DSL）を導入することと変わらない．最初にそれを書く人は楽になるかもしれないが新しくプロジェクトに参入してきたひとにはコストにしかならない（Golang以外も学ぶ必要がある）．例えば自分があるプロジェクトにContributeしようとして見たこともないテストフレームワークが使われているとがっくりする．
とにかくGolangだけで書くのが気持ちがいい，に尽きる．
テストとinterface テストという観点からみた場合のinterfaceの利点は何か？ interfaceを使えば「実際の実装」を気にしないで「振る舞い」を渡すことができる．つまり実装の切り替えが可能になる．interfaceを使うことでいわゆるモックが実現できる．
どこをinterfaceにするのか？ interfaceはモックポイントと思えば良い．外界とやりとりを行う境界をinterfaceにする，が基本．外界との境界とは例えばDBとやりとりを行う部分や外部APIにリクエストを投げるClientである．他にも考えられるがとりあえずここをinterfaceにする．
実例 以下では簡単なAPIサーバーを書くとしてinterfaceによるテスト手法の説明を行う．このAPサーバーはDBとしてRedisを使うとする．なおコードは全てpseudoである．
まずはDBのinterfaceを定義する．
type DB interface { Get(key string) string Set(key, value string) error }  次に実際のRedisを使った実装を書く．例えば以下のように書ける．
type Redis struct { // 接続情報など } func (r *Redis) Get(key string) string func (r *Redis) Set(key, value string) error  main関数から呼び出すときのことを考えてコンストラクタを実装すると良い（必要な接続情報などが与えられなかった時，もしくは必要な初期化処理に失敗した時にエラーを返せる）．
func NewRedis() (DB, error)  ここで重要なのは実際の実装であるRedisを返すのではなくinterfaceのDBを返すこと．サーバー側ではこのinterfaceを使う．
サーバーの実装は以下のようにする．
type Server struct { DB DB } func (s *Server) Start() error  ServerはinterfaceのDBを持ち内部の実装（例えばhandlerなど）ではこのinterfaceを利用する．</description>
    </item>
    
    <item>
      <title>PagerDutyのOn-callを一時的に自分にアサインするdutymeというツールを書いた</title>
      <link>https://deeeet.com/writing/2016/10/24/dutyme/</link>
      <pubDate>Mon, 24 Oct 2016 09:00:14 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/10/24/dutyme/</guid>
      <description>現在のチームではインシデント管理にPagerDutyを使っている．On-callはPrimaryとSecondaryの2人体勢でそれを1週間ごとにローテーションで回している．On-Callにアサインされている場合は夜中であれ日中であれPrimaryにアラートが飛ぶ（Primaryが反応できなければSecondaryにエスカレートされる）．そしてアラートを受けたら何かしらの対応を行う．
これはうまく回っているが問題もある．業務中（日中）はPrimaryやSecondaryに関係なくチームメンバーはどんどんデプロイしたりProduction環境で作業をしたりする．そしてオペレーションやデプロイ対象のコンポーネントによってはアラートが発生してしまうことがある．つまり作業者に関係なくアラートがPrimaryやSecondaryに飛んでしまう（Slackと連携しているので全員がそれをみることにはなるが）．
デプロイやオペレーションは各個人の責任でやっているのでまずは本人が対応するべきである．またPrimaryであれインシデントがない場合は自分のタスクに集中しているのでアラートが飛べばDisturbされてしまう（自分がPrimaryの場合は嫌だし自分のオペレーションで誰かをDisturbしたくもない）．そもそもスケジュールの粒度をもう少し細かく簡単に変更したい．
この問題を解決するためにdutymeというOn-callを一時的に自分にアサインするツールを書いた．以下ではこのツールの簡単な紹介を行う．なおコードは tcnksm/dutyme に公開している．
Requirement dutymeを使うにはPagerDuty API v2のTokenが必要になる．TokenはReadとWriteの権限を持っている必要がある．詳しくは&amp;ldquo;Generating an API Key – PagerDuty Support and Help&amp;rdquo;を参考．
Usage 使い方は以下．
$ dutyme start  このコマンドを実行し必要な情報（PagerDutyのEmailスケジュール）を入力するだけでアサインが変更される．一度入力した情報はファイルに保存できるので次回からは何も入力することなくアサインを変更できる．
以下は利用の様子．
How it works dutymeはスケジュールを書き換えているわけではない．PagedutyのOverrideという機能を使ってスケジュールの上書きをしている．なのでベースのスケジュールが壊れる心配はない（Overrideは消すのも簡単）．
実装はGo言語でclientには https://github.com/PagerDuty/go-pagerduty を使い（Go言語でPagerDuty関連のツールを作りたい場合はこれを使えば良さそう），tty プロンプトの制御には自分で書いた https://github.com/tcnksm/go-input を使っている．
Install インストールはgo getもしくはbrewが使える．
$ brew tap tcnksm/dutyme $ brew install dutyme  Conclusions 要望やBugなどは https://github.com/tcnksm/dutyme/issues までお願いします．現状は最低限使うための機能しかないがもう少し機能の追加はする予定．</description>
    </item>
    
    <item>
      <title>sync.ErrGroupで複数のgoroutineを制御する</title>
      <link>https://deeeet.com/writing/2016/10/12/errgroup/</link>
      <pubDate>Wed, 12 Oct 2016 09:36:20 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/10/12/errgroup/</guid>
      <description>Golangの並行処理は強力である一方で同期処理を慎重に実装する必要がある．&amp;ldquo;Go 言語における並行処理の構築部材&amp;rdquo;にまとめられているようにGolangは様々な方法でそれを実装することができる．実現したいタスクに合わせてこれらを適切に選択する必要がある．
この同期処理の機構として新たにgolang.org/x/sync/errgroupというパッケージが登場した．実際に自分のツールで使ってみて便利だったので簡単に紹介する．
使いどころ 時間のかかる1つのタスクを複数のサブタスクとして並行実行しそれらが全て終了するのを待ち合わせる処理（Latch）を書きたい場合にerrgroupは使える．その中でも「1つでもサブタスクでエラーが発生した場合に他のサブタスクを全てを終了しエラーを返したい」（複数のサブタスクが全て正常に終了して初めて1つの処理として完結する）場合が主な使いどころである．
実例 ここでは例として複数のworkerサブタスクをgoroutineで並行実行しそれらすべての終了を待ち合わせるという処理を考える．最初に今までのやりかたとしてsync.WaitGroupを使った実装を，次にerrgroupを使った実装を紹介する．
sync.WaitGroup goroutineの待機処理としてよく使われるのがsync.WaitGroupである．その名前の通り指定した数の処理（goroutine）の実行の待ち合わせに利用する．例えば以下のように書くことができる．
var wg sync.WaitGroup errCh := make(chan error, 1) for i := 0; i &amp;lt; 10; i++ { wg.Add(1) go func(i int) { defer wg.Done() worker(i) }(i) } wg.Wait()  新たなgoroutineを生成する度にAddでWaitGroupをインクリメントし処理が終了したときにDoneを呼ぶ．そして全てのworkerの処理が終了するまでWaitで処理をブロックする．これはchannelを使っても実装できるがsync.WaitGroupを使ったほうが読みやすいことも多い．
ではworkerでのエラーを処理をしたい場合にはどうするのが良いだろうか? sliceでエラーをため終了後にそれを取り出す，errorのchannelを作り外部でそれを受け取るといったパターンが考えられる．何にせよ別途自分で処理を実装する必要がある．
sync.ErrGroup errgroupパッケージを使う以下のように書くことができる．
eg := errgroup.Group{} for i := 0; i &amp;lt; 10; i++ { i := i eg.Go(func() error { return worker(i) }) } if err := eg.Wait(); err !</description>
    </item>
    
    <item>
      <title>GolangでAmazon EchoのSmart Home Skillを書く</title>
      <link>https://deeeet.com/writing/2016/08/30/alexa-irkit-ac/</link>
      <pubDate>Tue, 30 Aug 2016 09:56:39 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/08/30/alexa-irkit-ac/</guid>
      <description>Amazon Echo（以下Alexa）はAmazonが開発・販売している音声アシスタント+Bluetoothスマートスピーカーである．音楽を流す，今日の天気やニュースを聞く，Googleカレンダーの予定を聞く，TODOを追加する，家電を操作するなどなど&amp;hellip; といった多くのことを全て音声を通じて実行することができる（こちらの動画がわかりやすい）．
現時点（2016年8月）では音声認識は英語のみで対応地域もUSのみとなっている（例えば天気を聞くと地域を指定しない限りUSの天気が返ってくる）．また連携できるサービスも日本で使えるものは少ない．ただ発表当時から「これは完全に買いだ」と思っており先日GopherCon2016で渡米したときにいきおいで購入した（自分は音声アシスタントはSiriなどのスマートフォンに搭載されているものよりも据え置き型のものに未来を感じている．実は大学院では会話ロボットの研究をしていたのでこの分野には思うことはたくさんある．がそれは別途書く）．
Alexaをしばらく使った感想としてはとにかく音声認識がすごい！ Living roomに置いているがどこから話しても認識してくれる（最悪玄関から「Turn-off AC」と叫んでも認識してくれる）．音声認識の研究をしていた身からして一番驚いたのはAlexaのスピーカーから音楽を流していても認識がまともに働くこと．音楽のシグナルと言葉のシグナルを分離する的な研究はあったがここまで実用的になっているのは正直驚いた．また自分は対話システムにおけるバージイン（割り込み）を研究テーマにしていたことがあるがそちらも完璧に実用的である．Alexaの上に乗るアプリケーションにはもちろん感動するが音声対話システムとしての基礎がものすごくしっかりしていることにとても感動した．
さてAlexaは音楽を流す，天気を聞くといったBuilt-inのSkillに加えてサードパーティが提供するSkillを有効にして機能拡張することができる．そしてSkillは自分で開発することもできる．SkillはAWS LambdaのFunctionとして実装するので現状はLambdaが対応するPython，Node.jsもしくはJavaでの開発が前提となる．がGolangの場合はシングルバイナリをデプロイしてNode.jsから実行するという方法が使えるためGolangも開発の選択肢になる．
今回Golangを使い実用的なAlexa Skillを書いた．本記事ではその実装方法を簡単に紹介する．なおコードは全て https://github.com/tcnksm/alexa-irkit-ac に公開している．
デモ 以下は今回作成したAlexa Skillのデモ動画．自宅のエアコンのON/OFFを行う．ON/OFFのシグナルの送信にはIRKitを使っている．
 Turn on/off air conditioner by Amazon Echo and IRKit from deeeet on Vimeo.
実装の概要 Alexaの独自Skillの開発にはAlexa Skill Kitを用いる．Skill Kitには以下の2種類がある．
 Custom Skill Smart Home Skill  Custom SkillはよりGeneralなリクエストを受けるのに利用する．例えばWeb Seriviceに情報を問い合わせるやピザを注文するなど．若干冗長な言い回しをしないといけないが自由なワードを認識させる事ができる．Smart Home Skillは家電操作に特化したリクエストを受けるのに利用する．受け付ける言い回しは限定されているがより自然な命令ができる．今回はエアコンの操作なのでSmart Home Skillを利用した（Custom Skillを使ったNode.jsの実装はIRKitの作者の@maaashさんの&amp;ldquo;Amazon Alexaにエアコンをつけてもらう&amp;rdquo;が参考になる）．
Smart Home Skillのリクエストの流れは以下のようになる．
自分で書く必要があるのは4のLambda Functionである．Alexa Serviceからリクエストを受け家電を操作するためのAPI（この場合はIRKitのInternet HTTP API）にリクエストを投げる．
これに加えてSmart Home Skillの場合はOAuth 2.0 Authorization Frameworkを使ったAccount Linkingが必須になる．今回作成したSkillは完全に個人用途なのでAlexa Serviceからのリクエストを最小限でハンドルするシンプルなOAuthサーバーをGoで書いてIBM BluemixにPushして済ませた（実はここが一番めんどくさかった．ドキュメントが不足していたので自分でRFCを読まないといけなかった）．</description>
    </item>
    
    <item>
      <title>Go1.7のSubtestsとSub-benchmarks</title>
      <link>https://deeeet.com/writing/2016/08/02/go1_7-subtest/</link>
      <pubDate>Tue, 02 Aug 2016 09:00:00 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/08/02/go1_7-subtest/</guid>
      <description>Go1.7ではSubtestsとSub-benchmarksという機能がtestingパッケージに導入される．これを使うとテスト関数/ベンチマーク関数の中にテスト/ベンチマークを定義できるようになる．テストの場合はテストに階層を持たせることができ，ベンチマークの場合はTable Driven的にベンチマークを記述することができるようになる．さらに一連のテスト/ベンチマークに対して共通のsetupとtear-downを持たせることもできる．
テストの場合はTable Driven Testsで十分なことも多く恩恵は少ないかもしれない．それよりもベンチーマークで効果を発揮することが多い．
例えば以下のように異なる設定値を使ってFooのベンチマークをとるとする．今までであればそれぞれ設定値ごとにベンチマーク関数を準備する必要があった．
func BenchmarkFoo1(b *testing.B) { benchFoo(b, 1) } func BenchmarkFoo10(b *testing.B) { benchFoo(b, 10) } func BenchmarkFoo100(b *testing.B) { benchFoo(b, 100) } func benchFoo(b *testing.B, base int) { for i := 0; i &amp;lt; b.N; i++ { Foo(base) } }  Go1.7のSub-benchmarkを使うと以下のように書ける．
func BenchmarkFoo(b *testing.B) { cases := []struct { Base int }{ {Base: 1}, {Base: 10}, {Base: 100}, } for _, bc := range cases { b.</description>
    </item>
    
    <item>
      <title>Go1.7のcontextパッケージ</title>
      <link>https://deeeet.com/writing/2016/07/22/context/</link>
      <pubDate>Fri, 22 Jul 2016 09:12:28 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/07/22/context/</guid>
      <description>Go1.7ではgolang.org/x/net/contextがcontextパッケージとして標準パッケージに仲間入りする．そしていくつかの標準パッケージではcontextパッケージを使ったメソッド/関数も新たに登場する．contextパッケージは今後さらに重要な，Gopherは普通に扱うべき，パッケージになると考えられる．本記事ではそもそもcontextパッケージとは何か？なぜ登場したのか？なぜ重要なのか？どのように使うべきか？についてまとめる．
contextパッケージが初めて紹介されたのは2014年のThe Go Blogの記事 &amp;ldquo;Go Concurrency Patterns: Context&amp;rdquo;である．この記事ではなぜGoogleがcontextパッケージを開発したのか，どのように使うのか具体的な検索タスクを例に解説されている．まだ読んだことがない人はそちらを先に読むと良い．
contextパッケージとは何か ここでは具体的な利用例からcontextとは何かを説明する．
例えばGoの典型的な利用例であるWebアプリケーションを考える．Goのサーバにおいてリクエストはそれぞれ個別のgoroutineで処理される．そしてリクエストHandlerは新たなgoroutineを生成しバックエンドのDBや別のサーバにリクエストを投げ結果を得てユーザに対してレスポンスを返す．
このような別サーバへのリクエストのように時間のかかる処理をgoroutineで実行する場合どのようなことに注意する必要があるだろうか．まず最初に注意するべきはその処理に適切なTimeoutやDeadlineを設定して処理が停滞するのを防ぐことである．例えば別のサーバにリクエストを投げる場合にネットワークの問題でリクエストに時間がかかってしまうことは大いに考えられる．リクエストにTimeoutを設定して早めにレスポンスを返しリトライを促すべきである．
次に注意するべきは生成したgoroutineを適切にキャンセルしリソースを解放することである．例えば別のサーバにリクエストを投げる場合に適切なキャンセル処理を行わないとTimeout後もネットワークリソースが使われ続けることになる（CPUやメモリを使い続けるかもしれない）．この場合net/httpパッケージレベルでリクエストをキャンセルするべきである．
さらにそのgoroutineは別のgoroutineを呼び出しそれがまた別の&amp;hellip;と呼び出しの連鎖は深くなることが考えられる．その場合も親のTimeoutに合わせてその子は全て適切にキャンセルされリソースは解放されるべきである．．
このようにキャンセル処理は重要である．contextパッケージはこのキャンセルのためのシグナルをAPIの境界を超えて受け渡すための仕組みである．ある関数から別の関数へと，親から子へと，キャンセルを伝搬させる．
これはcontextを使わなくても実現できる．しかし標準パッケージになったことでcontextは「キャンセルのためのシグナルの受け渡しの標準的なインターフェース」として使える．この流れは別の標準パッケージに新たに追加された関数に見ることができる．
（後述するがcontextパッケージは限定されたスコープの値，例えば認証情報など，の受け渡しとしても利用できる．しかし筆者はこれは付随的な機能でありキャンセル機構としてのcontextの方が重要であると考えている）
コードで追うcontextパッケージ 言葉のみでは伝わりにくいので具体的なサンプルコードを使ってcontextパッケージの使いどころを説明する．
以下のような単純なリクエストHandlerを考える．このHandlerはユーザからのリクエストを受けバックエンドのサービスにリクエストを投げる．そして得た結果をユーザに返す（具体的なレスポンスの書き込みなどは省略している）．リクエストは別のgoroutineで投げ，エラーをchannelで受け取る．このコードを改善していく．
func handler(w http.ResponseWriter, r *http.Request) { // 新たにgoroutineを生成してバックエンドにリクエストを投げる // 結果をerror channelに入れる errCh := make(chan error, 1) go func() { errCh &amp;lt;- request() }() // error channelにリクエストの結果が返ってくるのを待つ select { case err := &amp;lt;-errCh: if err != nil { log.Println(&amp;quot;failed:&amp;quot;, err) return } } log.Println(&amp;quot;success&amp;quot;) }  まず現状のコードはネットワークの問題などでrequest()に時間がかかりユーザへのレスポンスが停止してしまう可能性がある．これを防ぐためにはTimeoutを設定するべきである．timeパッケージのtime.Afterを使うと以下のようにTimeoutを設定することができる．
func handler(w http.ResponseWriter, r *http.</description>
    </item>
    
    <item>
      <title>GopherCon 2016でLTした</title>
      <link>https://deeeet.com/writing/2016/07/12/gophercon2016-lt/</link>
      <pubDate>Tue, 12 Jul 2016 19:04:47 -0600</pubDate>
      
      <guid>https://deeeet.com/writing/2016/07/12/gophercon2016-lt/</guid>
      <description> GopherCon 2016でLTをした．@tenntennさんがやった通常トーク（50分）はなかなかハードルが高いがLTは初めの一歩として良いと思う．来年もDenverで再び開催されることがアナウンスされているので来年以降に発表するひとのためにどんな感じだったかを簡単に書いておく．
モチベーション 発表スライドを見てもらえばわかるが特に新しい話をしたわけではない．日本のミートアップなどで話したこと，ブログに書いたことを英語にしただけにすぎない（ただ実演デモをするという挑戦はした）．
「大御所たちと同じステージで喋る機会を逃すのはもったいない」（ちなみに当日のLTは僕の次がRobert Griesemer氏でその次がBrad Fitzpatrick氏だった！），「日本のGo界隈にこんなやつおるでってのを知ってもらいたい」というモチベーションで発表した．あとなんとなく自分の中でここでぶっ込まないと一生逃げると思ったのもある（通常セッションにしろやって話だが50分喋る良いネタがなかった..）．
流れ まずLTセッションの募集は会議開催の10日前ほどにアナウンスされた（&amp;ldquo;GopherCon 2016 - Lightning Talk Annoucement&amp;rdquo;）．逃さないためにはtwitterの@GopherConやGophers slackをちゃんとウォッチしておくと良い．
CFPはPaperCallで行われた．タイトルや発表内容をちゃんと書く．
結果の発表は開催前日に，発表日は通常会議の初日に，発表順は当日その場で発表された．そのため資料の準備と練習の時間はほとんどない．飛行機での移動中などに形だけ資料を完成させ会議の合間に練習するしかない．また事前にディスプレイの接続チェックなどはできないので特殊なことはしないほうがよい．
発表場所は上の写真のメインルーム．発表時間は6分で，質疑応答は次の発表者の準備が完了するまで行われた．
LTセッションは通常会議が行われた2日間両方で行われ，発表人数はそれぞれ12名だった．他の発表や自分の発表を考えると採択率は高いと感じた（直前なので申し込む人が少なかったのかもしれない）．
この流れは来年変わるかもしれないし変わらないかもしれない．もし来年移行挑戦する人がいればぜひ参考にしてください．
まとめ 偉そうに書いたが採択されてからひたすら緊張し「聴衆として普通に楽しむだけにすればよかった」と何度も思った．ただ終わってみればやってよかったという気持ちしかない（こういうリプライもらえたり，終わった後に議論できて良かった）．機会があるひとはどんどん挑戦しましょう．
ちなみに通常セッションのトークは以下が最高だったのでビデオが公開されたら全部観ましょう．
 &amp;ldquo;Understanding nil&amp;rdquo; &amp;ldquo;Navigating Unfamiliar Code with the Go Guru&amp;rdquo; &amp;ldquo;Go for Data Science&amp;rdquo; &amp;ldquo;Visualizing Concurrency in Go&amp;rdquo; &amp;ldquo;Go for Crypto Developers&amp;rdquo; &amp;ldquo;Inside the Map Implementation&amp;rdquo; &amp;ldquo;Go Without the Operating System&amp;rdquo; &amp;ldquo;The Design of the Go Assembler&amp;rdquo; &amp;ldquo;cgo: Safely Taming the Beast&amp;rdquo;  </description>
    </item>
    
    <item>
      <title>Golangの新しいGCアルゴリズム Transaction Oriented Collector（TOC）</title>
      <link>https://deeeet.com/writing/2016/06/29/toc/</link>
      <pubDate>Wed, 29 Jun 2016 10:31:00 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/06/29/toc/</guid>
      <description>http://golang.org/s/gctoc
Goの新しいGCのProposalが出た．まだProposal段階であり具体的な実装はないが簡単にどのようなものであるかをまとめておく．
GoのGCはGo1.5において単純なStop The World（STW）からConcurrent Mark &amp;amp; Sweepへと変更され大きな改善があった（詳しくは&amp;ldquo;GolangのGCを追う&amp;rdquo;に書いた）．先の記事に書いたようにGo1.5におけるGCの改善は主にレイテンシ（最大停止時間）に重きが置かれいた．数値目標として10msが掲げられGo1.6においては大きなヒープサイズ（500GB）においてそれを達成していた．
GCの評価項目はレイテンシのみではない．スループットやヒープの使用効率（断片化の対処）なども重要である．Go1.6までのGCではそれらについて大きく言及されていなかった（と思う）．例えばスループットに関してはハードウェアの進化がそれを改善するはずであるという前提が置かれていた（&amp;ldquo;Go GC: Prioritizing low latency and simplicity&amp;rdquo;）．
今回提案されたTransaction Oriented Collector（TOC）アルゴリズムはGCのスループットを改善するものである．
TOCアルゴリズムの経験則 Transaction Oriented Collector（TOC）アルゴリズムは「あるTransactionで生成されたオブジェクトはTransactionが終了すると同時にすぐ死ぬことが多い」という経験則に基づくアルゴリズムである．ここでいうTransactionとはいわゆるACIDにおける不可分な処理単位ではなく，Webサービスなどでリクエスト受けてレスポンスを返すまでの一連の処理を示す．
この仮定はGenerational GC（世代別GC）が利用している「多くのオブジェクトは生成されてすぐにゴミとなりわずかなオブジェクトだけが長く生き残る」という経験則に似ている．TOCアルゴリズムはこの経験則のGoなりの再解釈のようにも見える．
このTOCアルゴリズムの経験則はどこから来たか? Goが多くサポートしているCloudアプリケーションである．このようなアプリーションは，他のネットワークや他のGoroutineからメッセージを受け，それをUnmarshalし，それを使い計算をし，結果をMarshalし，それを他のネットワークやGoroutineに投げる．そしてそのGoroutineは死ぬか他のリクエストを受けるために停止状態になる．
リクエスト中での計算では大きなヒープからデータを読み込むことはあるかもしれないが典型的には書き込みは滅多に起きずヒープはTransaction間で一定になる．そしてGoroutine内で新たにアロケートしたオブジェクトは他のGoroutineに共有される（publish）かもしれないし共有されない（local）かもしれない．TOCアルゴリズムはこの共有されない場合の観測結果を使う，つまり「もしGoroutineがその中でアロケートしたオブジェクトを共有しない場合，そのオブジェクトはGC時に到達不可能になり関連するメモリ領域はすぐにアロケートできる」である．
TOCアルゴリズムの恩恵を受けるのはnet/http#Serverやnet/rpc#Serverを使ったアプリケーションであると想像できる．
TOCアルゴリズムの実装の提案 TOCアルゴリズムの実装はProposalのExamplesをみるとわかりやすい．
（まず前提としてGoのGCのMarkはBitmapで管理されている．BitmapはオブジェクトのヘッダにMarkbitを持たせるのではなく関連するメモリ領域をBitのテーブルとして別で集中管理する手法である．これはCopy-On-Writeとの相性が良いなどがある）．
TOCアルゴリズムでは各Goroutineは2つのPointerをもつ．1つはCurrent Sweep Pointerである．このPointerはどこまでSweepを行ったか（Allocateしたか）を示す．もう1つはInitial Sweep Pointerである．これはそのGoroutine開始時のSweep Pointerを示す．この2つのPointerの間のオブジェクトはMarkされていようがMarkされていまいが「そのGoroutineで新たにアロケートされたオブジェクト」となる．そしてMarkされていないオブジェクトは共有されていない（Publishされていない）オブジェクトであるとする．
これをどのように実現するか? ライトバリア（Write barrier）を使う．このライトバリアはそのGoroutine内で新たにアロケートされたオブジェクトがInvariantであることを保証する．つまりそのオブジェクトが他に共有されればMarkをつける．
10011110010100101010100001001011010010110100101001011101010111101 ^ &amp;lt;- before ^ after -&amp;gt; Initial Sweep Pointer Current Sweep Pointer  （Proposalの図を拝借させてもらった．1は前回のGCで到達可能であったオブジェクト，もしくはGoroutineで新たにアロケートされそしてPublishされたオブジェクトである．BeforeとInitialの間にある0はアロケートされたがPublishされていないオブジェクトである．Afterにある0はまだアロケートされていないオブジェクトである）
あとはGoroutine終了時にCurrent Sweep PointerをInitial Sweep Pointerへと戻せば良い．新たにオブジェクトが生成されていようとそれが共有されていなければMarkは立っていないので，次回のGCサイクルを待たずに次回のSweepにおいてアロケートの対象になる．
まとめ 簡単にGoの新たなGCのProposalを追ってみた．今後の実装とそれによる効果がどうなるかが楽しみである．</description>
    </item>
    
    <item>
      <title>GolangでFlame Graphを描く</title>
      <link>https://deeeet.com/writing/2016/05/29/go-flame-graph/</link>
      <pubDate>Sun, 29 May 2016 14:22:17 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/05/29/go-flame-graph/</guid>
      <description>アプリケーションのパフォーマンス問題の解決やチューニングで大切なのは問題のコアやボトルネックに最短パスで到達することである．
基本的なパフォーマンス分析の入り口はアプリケーションのスレッドがon-CPUで時間を消費しているかoff-CPUで時間を消費しているかを理解するところから始まる．on-CPUの場合はそれがuserモードかkernelモードかを特定し，さらにCPUプロファイリングによってどのcode pathがCPUを消費しているのかの分析に向かう．off-CPUの場合はI/OやLock，pagingといった問題の分析に向かう．
Flame Graphはon-CPUでのパフォーマンスの問題が発覚した時に行うCPUプロファイリングを助ける．どのcode pathがボトルネックになっているのかを1つのグラフ上で理解できる．本記事ではFlame Graphとは何か? なぜ必要なのか? を解説しGoのアプリケーションでそれを用いるために方法を解説する．
Flame Graphとは何か? Flame GraphはCPUプロファイリング結果をvisualizeしたグラフである．元Joyent，現NetflixのBrendan Gregg氏によって開発された．例えば以下はMySQLのCPUプロファイリング結果をFlame Graphで描画したものである．
CPU プロファイリング CPUプロファイリングの共通のテクニックはStack traceのサンプリングである．Stack traceというのは関数コールのリストで，code pathの先祖を追うことができる．例えば，以下はGolangのstack traceで子から親へStackと辿ることができる．
syscall.Syscall syscall.write syscall.Write os.(*File).write os.(*File).Write log.(*Logger).Output log.Printf  Flame Graphの初期衝動 CPUプロファイリングの出力は往々にしてverboseである．例えば，Brendan Gregg氏がFlame GraphをつくるきっかけとなったプロダクションのMySQLのプロファイリングの出力は500,000行近くもあったという（参考画像&amp;hellip;やばいw）．
Flame Graphはそのような膨大なCPUプロファイリングを一つのグラフ上で直感的かつ簡単に理解するために開発された．
Flame Graphの読み方 以下はFlame Graphを単純化したものである．
 Stack traceは長方形のボックスの列で表現される．1つ1つのボックスは関数（Stack frame）を示す y軸はStackの深さを示す．一番上のボックスはStack traceが収集された時にon-CPUであった関数であり，その下にあるボックスはすべて先祖になる．あるボックスの下にあるボックスはその関数の親である（高いほど悪いわけではない） x軸はその関数のSampleの割合を示す．時間ではない．それぞれの関数はアルファベット順にソートされているだけ それぞれのボックスの幅はその関数の出現頻度を示す（長いほどStack trace中に多く登場したこと意味する） 色には特に意味はない  ではこのFlame Graphからどのようなことがわかるか?
 Q. 最もon-CPUだったのはどの関数か?  A. g()（グラフの一番上を見れば良い）  Q. なぜg()はon-CPUなのか?  A. a() -&amp;gt; b() -&amp;gt; c() -&amp;gt; e() -&amp;gt; f() -&amp;gt; g()（y軸を見る）  Q.</description>
    </item>
    
    <item>
      <title>Known unknowns</title>
      <link>https://deeeet.com/writing/2016/05/24/known-unknowns/</link>
      <pubDate>Tue, 24 May 2016 08:07:56 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/05/24/known-unknowns/</guid>
      <description>&amp;ldquo;Systems Performance: Enterprise and the Cloud&amp;rdquo; をずっと読んでいる．この本はNetflixのBrendan Gregg氏がJoyent時代に書いた本である．その名の通りLinux（とSolaris）のシステムのパフォーマンスの本である（とにかく一つ一つが丁寧かつ深く解説されておりページをめくるごとに学びしかないのでパフォーマンスに関わるひとは今すぐ読むと良い）．
この本で一貫して現れてくる，通底するのが，known-knowns，known-unknownsそしてunknown-unknownsという概念である．元ネタはDonald Rumsfeld 氏の会見でのコメントだが（cf. There are known knowns），複雑なシステムのパフォーマンスの重要な原則を集約している．良い概念なので簡単に紹介する．
それぞれをパフォーマンスの観点から説明すると以下のようになる．
 known-knowns - 知っていること．そのパフォーマンスのメトリクスをチェックするべきことを知っているし，現在の値も知っている．例えば，CPUの利用率をチェックするべきことを知っているし，その平均的な値が10%であることも知っている known-unknowns - 「知らないこと」を知っていること．そのパフォーマンスのメトリクスをチェックできること，そのようなサブシステムが存在してることを知っているが，まだそれらを観測したことがない（知らない）．例えば，profilingによって何がCPUを使いまくっているのかチェックできるのを知っているけどまだそれを実施してない． unknown-unknowns - 「知らないこと」を知らないこと．例えば，デバイス割り込みがCPUを多く消費することを知らず，そのためチェックしてないかもしれない．  パフォーマンスというのは「知れば知るほど知らないことが増える」という分野である．システムについて学べば学ぶほど，unknown-unknownsに気づき，それはknown-unknownになり，次回からはそれをチェックできるようになる．
そしてこれはパフォーマンスに限った話ではない．</description>
    </item>
    
    <item>
      <title>GolangのGCを追う</title>
      <link>https://deeeet.com/writing/2016/05/08/gogc-2016/</link>
      <pubDate>Sun, 08 May 2016 23:01:06 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/05/08/gogc-2016/</guid>
      <description>Go1.5とGo1.6でGoのGCのレイテンシが大きく改善された．この変更について「ちゃんと」理解するため，アルゴリズムレベルでGoのGCについて追ってみた．
まずGoのGCの現状をパフォーマンス（レイテンシ）の観点からまとめる．次に具体的なアルゴリズムについて，そして最後に実際の現場でのチューニングはどうすれば良いのかについて解説する．
GoのGCの今 最初にGoのGCの最近の流れ（2016年5月まで）をまとめる．
Go1.4までは単純なStop The World（STW）GCが実装されていたがGo1.5からは新たなGCアルゴリズムが導入された．導入の際に設定された数値目標は大きなヒープサイズにおいてもレイテンシを10ms以下に抑えることであった．Go1.5で新たなアルゴリムが実装されGo1.6で最適化が行われた．
以下は公開されているベンチマーク．まずはGo1.5を見る．
GopherCon 2015: Rick Hudson - Go GC: Solving the Latency Problem
グラフの横軸はヒープサイズで縦軸はレイテンシである（小さいほどよい）．以前のバージョンと比較するとヒープの増加に伴ってレイテンシが3.0sを超えていたのがほぼ0sに抑えらているのがわかる．コミュニティからも以下のようなベンチマークが公開されている．
 https://twitter.com/brianhatfield/status/634166123605331968 Billions of request per day meet Go 1.5 (The new version of Go reduces our 95-percentile garbage collector from 279 milliseconds down to just 10 ms)  次に以下はGo1.6のベンチマーク．
QCon: Go GC: Prioritizing Low Latency and Simplicity
縦軸と横軸はGo1.5と同じ．まずGo1.5のグラフと比べると10倍のヒープサイズでベンチマークが行われているのがわかる．Go1.5が50GBに達する前にレイテンシが増大しているのに対してGo1.6は250GBのヒープに対しても10msのレイテンシで抑えらているのが確認できる．
Go1.7のリリースが近いが，既に今までと同じくTwitterの@brianhatfield氏がCanaryテストを行い，さらにGCのレイテンシが改善されたことが報告されている．
 Go 1.7 observed performance changes (production canary@eeca3ba)  これらのアップデートからGoにおいてGCのレイテンシは大規模プロダクション環境においても全く問題にならないレベルになっていることがわかる．つまりパフォーマンスに問題があったときに疑うべき場所としては優先度は低いと言える．</description>
    </item>
    
    <item>
      <title>Golangのエラー処理とpkg/errors</title>
      <link>https://deeeet.com/writing/2016/04/25/go-pkg-errors/</link>
      <pubDate>Mon, 25 Apr 2016 09:00:22 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2016/04/25/go-pkg-errors/</guid>
      <description>GoConでは毎回エラー処理について面白い知見が得られる．Go Conference 2014 autumn においては（実際のトークではないが）居酒屋にて@JxckさんがRob Pike氏から以下のようなテクニックを紹介してもらっていた．
 Errors are values - The Go Blog Golang Error Handling lesson by Rob Pike  これはWrite（やRead）のエラー処理が複数続く場合にerrWriter を定義して複数のエラー処理を一箇所にまとめてコードをすっきりとさせるテクニックであった．
そして今回の Go Conference 2016 spring のkeynoteにおいてもDave Cheney氏から（僕にとっては）新たなエラー処理テクニックが紹介された．
 Gocon Spring 2016  実際に使ってみて/コードを読んでみて（飲み会でもコードとともにいろいろ教えてもらった）自分の抱えている問題を解決できそうで使ってみたいと思えた．
本記事では現在のエラー処理の問題と発表で紹介されたpkg/errorsについてまとめる．なお上記のスライドにはトークノートも書かれているので具体的な内容はそちらを見るのが良い．
問題 @Jxckさんのケースは1つの関数において複数のエラーハンドリングが煩雑になる，言わば縦方向のエラー処理の問題であった．Dave氏のトークで語られているのは深さ方向のエラー処理の問題である．大きく分けて2つの問題がある．
 最終的に表示されるエラーメッセージ 特定のエラーに対する分岐処理  以下ではそれらを具体的に説明する．
エラーメッセージ まずはエラーメッセージについて．以下は基本的なGoのエラー処理である．
func Foo() error { conf, err := ReadConf() if err != nil { return err } ... return nil }  Foo()がReadConf()を呼び，ReadConf()がエラーを返せばそれをerrとして返し，そうでなければconfをつかった処理を続行し問題がなければnilを返す．</description>
    </item>
    
    <item>
      <title>2015年振り返り</title>
      <link>https://deeeet.com/writing/2015/12/31/2015/</link>
      <pubDate>Thu, 31 Dec 2015 23:19:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/31/2015/</guid>
      <description>2015年の振り返りとして自分が好きだったもの，影響を受けたものを雑多にまとめる．それに合わせて自分の活動についても振り返り，2016年の展望を書く（fogus: Send More Paramedicsの形式が良かったのでそれを真似ている）．
Blog posts read 今年読んで印象に残った，影響を受けたブログ記事．順不同．
Japanese  コードを書くことは無限の可能性を捨てて一つのやり方を選ぶということ 7年働いた時点での私の仕事の極意 志低く ソフトウェアエンジニアだけでサービス運用できる環境を作って失業した話 食べログの口コミに見る人間心理 -麻薬と性とトラウマと- 運用を楽にするためのアプリケーションコードを書くということ Webオペレーションエンジニアのアウトプットと開発力 我慢の期間 2015年Webサーバアーキテクチャ序論 A Million Hello Worlds 技術者が研究者のように論文を書くメリットはあるか OSS開発の活発さの維持と良いソフトウェア設計の間には緊張関係があるのだろうか? 持続的なプラットフォームのための難しい決断 翻訳は/誰がやっても/間違える 下から目線のコードレビュー  English  CoreOS Co-Founder Alex Polvi Talks Containers, Rocket vs. Docker, and More HTTP/2 is Done Software engineers should write After Docker: Unikernels and Immutable Infrastructure My Philosophy on Alerting Mitchell Hashimoto: Containers or No Containers? That is One Question for 2015 What makes a cluster a cluster?</description>
    </item>
    
    <item>
      <title>Go言語でファジング</title>
      <link>https://deeeet.com/writing/2015/12/21/go-fuzz/</link>
      <pubDate>Mon, 21 Dec 2015 00:25:30 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/21/go-fuzz/</guid>
      <description>この記事はGo Advent Calendar 2015の21日目の記事です．
今年もGoコミュニティーから多くのツールが登場した．その中でも異彩を放っていたのがGoogleのDynamic testing toolsチームの@dvyukov氏によるgo-fuzzである．
go-fuzzはGo関数のファジングを行うツールである．このツールはとても強力で標準パッケージで100以上，golang.org/x/パッケージで40以上，その他を含めると300以上のバグを発見するという実績を残している（cf. Trophies）．
本記事ではこのgo-fuzzの紹介を行う．
ファジングとは?  Fuzz testing - Wikipedia, the free encyclopedia ソフトウェアの脆弱性検出におけるファジングの活用  「ファジング」とはソフトウェアのテスト手法である．テスト対象となるソフトウェアにランダムなデータを大量に入力し意図しない挙動を検出する．
普通のソフトウェアは予期しないデータを受けても適切な処理，例えばエラーを返すなど，を行う．そしてそれはテストされる．しかし予期しない入力をすべてテストすることは難しい．適切に処理しているつもりであっても予期しないデータによりソフトウェアがクラッシュしてしまうことはありうる．このようなテストでファジングは光る．大量のランダムデータを入力し予期しないクラッシュを見つける．
ファジングの利点に以下が挙げられる．
 チープである バイアスがない  まずファジングは単純にランダムなデータを放り込むだけなので非常にチープな手法である．使うだけなら特別な知識は必要ない．次にランダムであるためテスターのバイアスがない．そのソフトウェアをつくっているひとほど思い込みが強くなってしまう（と思う）が，そのようなバイアスを排除することができる．
ファジングで入力となるデータは「ファズ」と呼ばれる．コマンドラインツールであれば引数や環境変数，ウェブサーバーであればHTTPリクエストである．ファジングではこのファズをいかに生成するのかが重要になる．完全にランダムにする，指定の範囲内で連続に値を変化させる，正常なデータの一部を変更させる．ある特定の制御文字列を対象にするといった手法がある．
go-fuzzとは?  dvyukov/go-fuzz GopherCon 2015: Dmitry Vyukov - Go Dynamic Tools (Slide)  Go言語の関数に対してファジングを行うために開発されたのがgo-fuzzである．go-fuzzはC/C++のafl-fuzzがベースになっている．
go-fuzzは完全にランダムなデータを入力するのではなく，正常なデータの一部を変更させランダムなデータを生成する．これにより単純にランダムな値で盲目的にテストをするのではなく，ある程度「ありそうな」データでテストを行うことができる．このためのデータセットをcorpusと呼び，go-fuzzはテストを繰り返しながらこのcorpusを成長させていく．
corpusはどのように成長するのか? go-fuzzはASTを使い対象関数のテストのカバレッジ情報を取得する．カバレッジを上げるような入力が得られればそれをcorpusに登録する．これによりテストはより網羅的になる．
corpusは事前に与えることもできる．例えば対象とする関数の入力が画像データである場合は事前に幾つかの画像データを与えることができる．もしくはユニットテストなどで既にテストしている値を使うこともできる．
入力を繰り返し意図しない挙動が得られる（例えばpanicが起こる）とgo-fuzzはそれを引き起こした入力とスタックトレースをファイルとして保存する．開発者はその結果をもとに新たにユニットテストを追加しコードを修正していく．
使いかた go-fuzzによるファジングには以下の2つが必要である
 Fuzz()関数の準備 go-fuzz-buidとgo-fuzzの実行  まずFuzz()関数は以下のような関数である．
func Fuzz(data []byte) int  dataはgo-fuzzによって与えられるランダムな値である（ほとんどはinvalidな値である）．そしてこの値をテストしたい関数に入力として与える．go-fuzzはこの入力で関数がpanicしたりクラッシュしたり，メモリを割り当てすぎてhangしないかを監視する．
Fuzz()の返り値はcorpusの作成に使われる．以下の3つの値のうちどれかを返す．
 1- 入力がふさわしいデータであると考えられる場合（例えば関数がエラーを返さずに正常に処理された場合その入力はその関数にとってふさわしい入力であると考えることができる．ここから新たなランダムな値を生成すれば新たなエラーを発見できる可能性が高い） -1 - 入力がカバレッジを上げるようなふさわしい入力であると考えられてもcorpusには追加したくない場合 0 - 上記以外の場合（例えばエラーが返った場合）  関数が書けたら以下で専用のバイナリをつくる．zip形式で出力される．</description>
    </item>
    
    <item>
      <title>自宅で美味いコーヒーを淹れる</title>
      <link>https://deeeet.com/writing/2015/12/17/coffee-2015/</link>
      <pubDate>Thu, 17 Dec 2015 00:35:03 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/17/coffee-2015/</guid>
      <description>この記事はコーヒー Advent Calendar 2015の17日目の記事です．
コーヒーを淹れること，豆を買いに行くこと，コーヒー器具を集めること，コーヒー関連の本を読むことが好きだ．コーヒーは趣味といっても過言でなない．自宅で美味しいコーヒーを淹れるために今までいろいろ試行錯誤してきたが，最近ある程度固まってきたのでその環境についてまとめてみる．
過去 最初に自分とコーヒーとの馴れ初めをつらつらと．
親がコーヒー好きなので実家では当たり前のように毎日コーヒーが淹れられていた．そのため家で自分でコーヒーを淹れて飲むのは当たり前のものとして育った．実家ではドリップマシンが使われていた．特にこだわりはなく出されるものをそのまま飲んでいたと思う．
自分でコーヒーを淹れるようになったのは大学生で一人暮らしを始めてから．最初は実家にあった使われていないドリッパー（確かHARIO）と近所のスーパーの安い豆でドリップを始めた．見よう見まねでなんとなくやっていたと思う．大学生にもなるとカフェなどでまともなコーヒーを飲むようになり，自分で淹れるコーヒーがあまり美味しくないと感じ始めた．
どうやら自分で豆を挽くと美味いということを聞きつけポーレックスのコーヒーミルを買い，近所のKALDIで豆を買い，手挽きによる豆とドリップを始めた．
こうなってくると良いドリッパーも欲しくなる．いろいろ探してChemexを買った．Chemexは未だに使っているので5年以上の付き合いになる（よくおしゃれインテリア的な感じで使ってるやついるけどああいうやつはフィルターが買えなくなって最終的に花瓶として使い始める．映画&amp;ldquo;インターステラー&amp;rdquo;では水飲み用のデキャンタとして使われていて映画の評価に響いた）．
手挽きはとにかく失敗した．特にグラインドが粗すぎて青臭くなってしまうことが多かった．また手挽きは時間がかかるため平日は厳しくて週末しかできないとう問題があった．そのため在学中は平日はお店で挽いてもらった豆でドリップし，週末に手挽きでドリップをした．ドリップは自己流でやっていて日によってばらつきがあるもののある程度まともなものが淹れられるようにはなった
現在 過去のコーヒー環境には以下の問題があった．
 手挽きをしていたため平日に自分で豆を挽くことができないこと 自己流ドリップで同じ豆でも味が固定されないこと 良質な豆を使っていないこと  まず1つ目は社会人になり財力で解決した．必要なのは良質な電動グライダー．コーヒーは一生付き合うと思いKalitaのナイスカットミルを購入した．これは最高でグラインドは綺麗に均等になるしあらゆる淹れ方（ドリップ，エスプレッソ，フレンチプレス）にあったグラインドに簡単に調整できる．何より速い．忙しい朝でも新鮮な豆を挽くことができる．
2つ目の再現性の問題．これは本を読み，またサードウェーブの流れに触れることで多くを学び解決した．これについては以下で詳しく書く．
3つ目の問題はそもそも問題と認識できていなかった．これもサードウェーブの文化に触れることで学んだ．東京にもサードウェーブの流れを汲んだカフェはたくさんある（cf. 東京サードウェーブコーヒー）．KALDIの豆は今でも買う（特にリッチブレンドが大好き）が時間があればロースターに行き新鮮な豆を買うようになった．サードウェーブの原点であるオークランドが対岸にあるサンフランシスコを訪れたときは時間があればロースターに行き，コーヒーを飲み，豆を買うなどした（cf. サンフランシスコでたくさんコーヒー飲んだ）．
良い豆? ちょっと脱線するが，良い豆って何? あるいはサードウェーブと今までの豆の違いって何? という話を．
昔ながらのコーヒーに深煎りのものが多い．それは昔は豆が熟度など不完全なものしか入ってこなくてその欠点を焙煎で消そうとしていたため．
近年はクオリティの高い豆が流通するようになった．特にサードウェーブってのは豆そのもの味を楽しもうって流れで浅煎が多い．そして素材そのものを楽しむのをよしとする．同じ農園の豆の味でも年ごとに違うし，同じ品種でもテロワール（生産地の地理/地勢/気候/土壌などの特徴）で異なってくる．今まではそれを隠そうとしたが，今はそれを楽しもうとしている．
農家に対する配慮に関してもよく語られる．これはコーヒーの美味しさは実が果実として完熟しているかどうかで決まるため．かつそれは木になっているときにしか判断できないから．良好な関係を築くことでより良い栽培/収穫方法のループを回せるようにする．スタバとかが批判されるのは同じ味を世界中で実現するために農家無視で大量生産してるから．
もちろん今でも昔ながらの深煎りも好きで全然飲むけど．
以下では最近はどのような器具を使っているのか? 再現性を高めるにはどうしているのか? などについて書く．
器具 もともとドリップ至上主義でコーヒーはドリップ，ドリップ以外は認めない，ドリップ以外は飲むに値しないという過激な立場を取ってきた．しかし豆にはその豆にあった淹れ方があることを知り，今ではこの考えを改めていろいろな抽出方法を使うようになった（メインはドリップだが）．
簡単に現在使っている器具を紹介する．
左から順番に
 Chemex - Chemexを愛して5年．これからも使い続けると思う．Chemexは専用のフィルターが必要でそれによってChemexっぽい味が出る．KALDIの豆によく使う．最近は人に淹れることもあるので6カップの購入も考えている． Donuts Dripper - Chemex以外のドリップもしようと2年ほど前に購入．しっかり濃いのに重くないスッキリした飲み心地になるようにデザインされており実際そんな感じになる．市販の安いフィルターが使えるもの良い． BIALETTI モカポット - エスプレッソマシンなど買えない．圧力はマシンに及ばないがそれに近しいものは淹れることができる．ど濃いコーヒーを飲みたい時に良い．イタリアにはどの家庭にもあるらしい． AeroPress - 素早く淹れられて洗うのも簡単．使い方次第でいろいろな淹れ方ができて面白い（大会もあるとか）．これは買ったばかりでまだうまく使えていないのでこれからもっとうまくなりたい． HARIO フレンチプレス - なかなか難しいが面白い味のコーヒーを作れる．友人からプレゼントしてもらった．  ちなみにカップはFour Barrel Coffeeのもの．ここで飲んだコーヒーが今まで飲んだなかで一番美味しかった．
再現性 過去の自分のコーヒーの淹れ方には同じ豆を使っているのに再現性がないという問題があった．それもそもはずで計測などしないで感覚でやっていたのが原因．再現性への回答は計測すること．
抽出の味のファクターは豆の量，グラインドの細かさ，抽出時間，お湯の量，お湯の温度である．HARIOのドリップスケールにははかりとタイマーが付いている．これは最高のツールで豆の量とお湯の量，抽出時間を計測しながらコーヒーを抽出することができる．これを導入したことで再現性は飛躍的に高まった．
使い方を簡単に説明する．まず以下のように豆の量を測る．
次に抽出．以下はChemexを使った例．抽出しながら，時間とお湯の量を測ることができる（温度計は別途購入した）．
豆の量や抽出時間はどこから知るのか? サードウェーブ系のロースターでは豆を買う時に聞けば大抵どこでも教えてくれる．場所によっては豆のグラインドのサンプルもくれる．簡単な説明書みたなのを準備してるところもある．新鮮な豆とそれを淹れるためのレシピを聞けば自宅でそれを再現することは容易い（しかもロースターで飲むと1杯500円くらいする．豆は100gあたり800円-1000円の価格帯になる．1杯あたりは10-15gの豆を使うので自分で淹れれば1杯100-150円程度になる．つまり自分で淹れたほうが断然良い）．</description>
    </item>
    
    <item>
      <title>Go言語でLet&#39;s EncryptのACMEを理解する</title>
      <link>https://deeeet.com/writing/2015/12/01/go-letsencrypt-acme/</link>
      <pubDate>Tue, 01 Dec 2015 23:18:42 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/12/01/go-letsencrypt-acme/</guid>
      <description>Let&amp;rsquo;s Encrypt
TL;DR Let&amp;rsquo;s EncryptのベースのプロトコルであるACMEを理解する．
まずACMEをベースとしたCAであるboulderをローカルで動かす．次にACMEのGo言語クライアントライブラリであるericchiang/letsencrypt（非公式）を使い実際にboulderと喋りながら証明書発行を行い，コードとともにACMEが具体的にどのようなものなのかを追う．
はじめに 証明書というのは面倒なもの，少なくともカジュアルなものではない，というイメージが強い．それは有料であることや自動化しにくいなどといったことに起因している（と思う）．そのようなイメージに反して近年登場する最新の技術/プロトコルはTLSを前提にしているものが少なくない（e.g., HTTP2）．
このような背景の中で登場したのがLet&amp;rsquo;s Encryptと呼ばれるCAである．Let&amp;rsquo;s Encryptは上で挙げたような問題（煩雑さ）を解決しようとしており，無料・自動・オープンを掲げている（cf. &amp;ldquo;Let&amp;rsquo;s Encrypt を支える ACME プロトコル&amp;rdquo;）．最近（2015年12月3日）Public Betaがアナウンスされすでに1日に70kの証明証が発行され始めており（cf. Let&amp;rsquo;s Encrypt Stats）大きな期待が寄せられている．特に自分は仕事で多くのドメインを扱うのでLet&amp;rsquo;s Encryptは使ってくぞ！という意識がある．
Let&amp;rsquo;s EncryptはDV証明書を発行することができるCAである．DV証明書とはドメインの所有を確認して発行されるタイプの証明書である．Let&amp;rsquo;s Encryptの大きな特徴の1つに自動化が挙げられる．申請からドメインの所有の確認，証明書発行までは全てコマンドラインで完結させることができる．そしてこのフローはLet&amp;rsquo;s Encrypt以外のCAでも利用できるように標準化が進められている．これはAutomated Certificate Management Environment（ACME）プロトコルと呼ばれる（ちなみにLet&amp;rsquo;s encryptの証明証の有効期限は90日である．これはセキュリティ強化の面もあるが自動化の促進という面もある（cf. Why ninety-day lifetimes for certificates?））．
Let&amp;rsquo;s Encryptは専用のACMEクライアントを提供している（letsencrypt）．基本はこれを使えば証明書の発行や，Apacheやnginxの設定ファイルの書き換え(!)などができる（やりすぎ感が気にくわないと感じるひとが多いようでsimple alternativeがいくつか登場している&amp;hellip;）．
それだけではなくACMEベースのCA（つまりLet&amp;rsquo;s encrypt）はBoulderとう名前でOSSベースで開発されている（Go言語で実装されている）．つまりBoulderを使えば誰でもACMEをサポートしたCAになることができる．
本記事ではおそらく将来的には意識しないでよくなる（であろう）ACMEプロトコルがどのようなものかを理解する．boulderをローカルで動かし（Dockerfileが提供されている），非公式であるがGo言語のACMEクライアントericchiang/letsencryptを使ってACMEを喋ってみる．
なおACMEはまだ仕様策定中なので以下の説明は変更される可能性がある．
boulderを動かす まず準備としてboulderを動かす．今回は例としてexample.orgの証明証を発行する．ローカルでこれを実行するためには以下の準備が必要になる．
 cmd/policy-loader/base-rules.jsonのブラックリストからexample.orgを外す /etc/hostsを編集してexample.orgを127.0.0.1に向ける  完了したらboulderコンテナを起動する．
$ cd $GOPATH/src/github.com/letsencrypt/boulder/ $ ./test/run-docker.sh  ACMEの概要 ACME spec draft
ACMEとは「クライアントのドメインの所有を確認して証明書を発行する」ためのプロトコルであった．これをさらに細かくブレイクダウンすると以下の操作から構成される．
 各操作を行うためのURIを知る（directory） クライアントの登録を行う（new-registration） 認証（ドメイン所有の確認）を行う（new-authorization） 証明書（Certification）を発行する（new-certificate）  ACMEはこれらのリソースを持ったRESTアプリケーションであるとみなすこともできる．各リソースはその上のリソースに依存しており，上から順番にリクエストをこなしていくことで最後の証明書の発行に到達することになる．
以下ではこれらのリソースをさらに細かく見ていく．
directory directoryは他の各種リソースのURIをクライアントに提示する．クライアントはまずここにリクエストしその後の操作でリクエストするべきendpointを知る．</description>
    </item>
    
    <item>
      <title>Go言語とHTTP2</title>
      <link>https://deeeet.com/writing/2015/11/19/go-http2/</link>
      <pubDate>Thu, 19 Nov 2015 01:30:18 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/11/19/go-http2/</guid>
      <description>http2 in Go 1.6; dotGo 2015 - Google スライド
2015年の5月にRFCが出たばかりのHTTP2が2016年の2月にリリース予定のGo1.6で早くも利用可能になることになっている．HTTP2の勉強も兼ねてGo言語におけるHTTP2実装を追ってみる．
以下ではまず実際にHTTP2サーバを動かしChromeで接続してみる．次に現状コードがどのように管理されているかを追う．最後に実際にコードを動かしながらHTTP2の各種機能を追う．なお参照するコードはすべて以下のバージョンを利用している（まだWIPなのでコードなどは今後変わる可能性があるので注意）．
$ go version go version devel +9b299c1 darwin/amd64  HTTP2とは? HTTP/2に関してはスライドやブログ記事，Podcastなど非常に豊富な情報がインターネット上に存在する．そもそもHTTP2とは何か?なぜ必要なのか?などを理解したい場合は参考に挙げた記事などを参照するのがよい．
実際に使ってみる 最小限のコードでHTTP2サーバーを起動しChromeで接続してみる．
まず最新のGoをソースからビルドする（ビルドにはGo1.5.1を利用する）．以下では2015年11月16日時点の最新を利用した．
$ git clone --depth=1 https://go.googlesource.com/go ~/.go/latest $ export GOROOT_BOOTSTRAP=~/.go/1.5.1 $ cd ~/.go/latest/src &amp;amp;&amp;amp; ./make.bash  現時点でGoにおけるHTTP2はover TLSが前提になっている．そのためサーバー証明書と鍵が必要になる（なければ事前にopensslコマンドやcrypto/x509パッケージなどを使って自己署名証明書をつくる）．
コードは以下．
func main() { certFile, _ := filepath.Abs(&amp;quot;server.crt&amp;quot;) keyFile, _ := filepath.Abs(&amp;quot;server.key&amp;quot;) http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, &amp;quot;Protocol: %s\n&amp;quot;, r.Proto) }) err := http.ListenAndServeTLS(&amp;quot;:3000&amp;quot;, certFile, keyFile, nil) if err !</description>
    </item>
    
    <item>
      <title>Go言語と暗号技術（AESからTLS）</title>
      <link>https://deeeet.com/writing/2015/11/10/go-crypto/</link>
      <pubDate>Tue, 10 Nov 2015 15:53:31 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/11/10/go-crypto/</guid>
      <description>最近マスタリングTCP/IP SSL/TLS編や暗号技術入門を読んでいた．理解を深めるためにGo言語で標準のcryptoパッケージを触り/実装を読みながら読んだ．
cryptoパッケージは他の標準パッケージと同様に素晴らしい．Go言語にはどのような暗号化手法が実装されているのか実例を含めてざっとまとめる．なお本文に書ききれなかったものを含め全ての実装例はtcnksm/go-cryptoにある．
共通鍵暗号 まずは共通鍵暗号をみる．共通鍵暗号は暗号化と復号化に同じ鍵を用いる暗号化方式である．共通鍵暗号はブロック暗号とストリーム暗号の2種類に分けることができる．ブロック暗号は特定の長さ単位で暗号化を行う方式であり，ストリーム暗号はデータの流れを順次処理していく方式である．
Go言語にはブロック暗号としてDES（Data Encryption Standard），DESを繰り返すtriple-DES，そしてAES（Advanced Encryption Standard ）が実装されている．ストリーム暗号としてはRC4が実装されている．
AESはDESに代わる新しい標準のアルゴリズムであり公募により選出された．互換性などを考慮しない限りこれを使うのが良い．実際にplainTextをAESで暗号化/復号化してみる．
plainText := []byte(&amp;quot;This is 16 bytes&amp;quot;) key := []byte(&amp;quot;passw0rdpassw0rdpassw0rdpassw0rd&amp;quot;) block, err := aes.NewCipher(key) if err != nil { fmt.Printf(&amp;quot;err: %s\n&amp;quot;, err) return } // Encrypt cipherText := make([]byte, len(plainText)) block.Encrypt(cipherText, plainText) fmt.Printf(&amp;quot;Cipher text: %x\n&amp;quot;, cipherText) // Decrypt decryptedText := make([]byte, len(cipherText)) block.Decrypt(decryptedText, cipherText) fmt.Printf(&amp;quot;Decrypted text: %s\n&amp;quot;, string(decryptedText))  AESの鍵長さは16byte，24byte，32byteのいずれかである必要がある（それぞれAES-128，AES-192，AES-256と呼ばれる）．NewCipherはcipher.Blockインタフェースを返す．このインタフェースにはEncrypt()とDecrypt()が実装されている．全てのブロック暗号にはこのインタフェースが実装されている（他の例はこちら）．
AESは16byteというブロック単位で暗号化/復号化を行うアルゴリズムである．このままでは例にあるように16byteの固定視長の平文しか暗号化を行えない．これでは使えない．
ブロック暗号のモード 任意の長さの平文を暗号化するためにはブロック暗号を繰り返し実行する必要がある．ブロック暗号にはそれを繰り返し実行するためのモードがある．
まず単純に考えると平文を分割してそれぞれにブロック暗号を適用する方法が考えられる．これはECB（Electronic CodeBook mode）モードと呼ばれる．しかし同じ平文ブロックが存在する場合は同じ暗号文ブロックが存在してしまう，かつ攻撃者が暗号文ブロックを入れ替えたら平文の順番も入れ替わってしまうというなどの問題があり実用的ではない．これらの欠点を回避するために各種モードが存在する．
Go言語では，ブロック暗号の各種モードをcipherパッケージに実装している．実装されているモードは以下，
 CBC（Cipher Block Chainning）モード - 1つ前の暗号ブロックと平文ブロックのXORをとってから暗号化を行う．1番最初の平文ブロックにはIV（Initialization Vector）とXORをとる．暗号ブロックの一部が欠損すると以後の平文全てに影響が出る．SSL/TLSに利用されている（3DES_EDE_CBC，AES_256_CBC）． CFB（Cipher FeedBack）モード - 1つ前の暗号ブロックを暗号化したもの（Key Stream）と平文ブロックのXORをとる．再生攻撃が可能． OFB（Output FeedBack）モード - 1つ前の暗号化の出力（Key Stream）を次の暗号化の入力とする．暗号化の出力（Key Stream）と平文でXORをとる（Key Streamを事前につくっておくことができる）．もし暗号結果が同じものになったらそれ以後Key Streamは全て同じ値になってしまう．暗号文を1ビット反転させると平文も1ビット反転する CTR（CounTeR）モード - 1つずつ増加していくカウンタを暗号化してKey Streamを作り出す．カウンタを暗号化してKey Streamとする．カウンタは暗号化のたびに異なる値（ノンス）をもとにしてつくる．暗号文を1ビット反転させると平文も1ビット反転する．暗号結果が同じになってもそれ以後のKey Streamが同じ値になることがない． GCM（Galois/Counter）モード - CTRが暗号文を作り出すと同時に「この暗号文は正しい暗号化によって作られたものである」とう認証子を作り出す．暗号文の偽装を見抜くことができる．TLS1.</description>
    </item>
    
    <item>
      <title>Hashicorp Ottoを読む</title>
      <link>https://deeeet.com/writing/2015/10/04/otto/</link>
      <pubDate>Sun, 04 Oct 2015 22:07:21 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/10/04/otto/</guid>
      <description>Hashicorpから2015年秋の新作が2つ登場した．
 Otto - HashiCorp Nomad - HashiCorp  Ottoがなかなか面白そうなのでコードを追いつつ，Ottoとは何か? なぜ必要になったのか? どのように動作するのか? を簡単にまとめてみる．
バージョンは 0.1.0 を対象にしている（イニシャルインプレッションである）
Ottoとは何か? 公式はVagrantの後継と表現されている．が，それはローカル開発環境の構築も担っているという意味で後継であり，自分なりの言葉で表現してみると「OttoはHashicorpの各ツールを抽象化し開発環境の構築からインフラの整備，デプロイまでを一手に担うツール」である．ちなみにOttoという名前の由来はAutomationと語感が似ているからかつ元々そういう名前のbotがいたからとのこと．
なぜOttoか? なぜVagrantでは不十分であったのか? なぜOttoが必要だったのか? 理由をまとめると以下の5つである．
 設定ファイルは似通ったものになる 設定ファイルは化石化する ローカル開発環境と同じものをデプロイしたい microservicesしたい パフォーマンスを改善したい   まず各言語/フレームワークのVagrantfileは似通ったものになる．Vagrantfileは毎回似たようなものを書く，もしくはコピペしていると思う．それならツール側が最も適したものを生成したほうがよい．Ottoは各言語のベストプラクティスな設定ファイルを持っておりそれを生成する．
そしてVagrantfileは時代とともに古くなる，つまり化石化する．秘伝のソースとして残る．Ottoは生成する設定ファイルを常に最新のものに保つ．つまり今Ottoが生成する設定ファイルは5年後に生成される設定ファイルとは異なるものになる（cf. &amp;ldquo;Otto: a modern developer&amp;rsquo;s new best friend&amp;rdquo;）
そしてローカル開発環境と同じものを本番に構築したい（Environmental parityを担保したい）．現在のVagrantでもproviderの仕組みを使えばIaaSサービスに環境を構築することはできる．が本番に適した形でそれを構築できるとは言い難い．Ottoは開発環境の構築だけではなく，デプロイ環境の構築も担う．
時代はmicroservicesである．Vagrantは単一アプリ/サービスの構築には強いが複数には弱い．Ottoは依存サービスを記述する仕組みをもつ（Appfile）．それによりmicroserviceな環境を簡単に構築することができる．
そしてパフォーマンス．最近のVagrantはどんどん遅くなっている．例えば立ち上げているVMの状態を確認するだけのstatusコマンドは2秒もかかる．Ottoはパフォーマンスの改善も目的にしている．
Ottoは何をするのか? Ottoが行うことは以下の2つに集約できる．
 Hashicorpツールの設定ファイルとスクリプトを生成する Hashicorpツールのインストール/実行をする  Ottoの各コマンドと合わせてみてみると以下のようになる．
 compile - アプリケーションのコンテキスト（e.g., 言語やフレームワーク）の判定と専用の設定ファイルであるAppfileをもとにHashicorpツールの設定ファイル（VagrantfileやTerraformの.tfファイル，Packerのマシンテンプレート.json）と各種インストールのためのシェルスクリプトを生成する dev - 開発環境を構築する．Vagrantを実行する infra - アプリをデプロイするためのインフラを整備する．例えばAWSならVPCやサブネット，ゲートウェイなどを設定する．Terraformを実行する build - アプリをデプロイ可能なイメージに固める．例えばAMIやDocker Imageなど．Packerを実行する deploy - 作成したイメージを事前に構築したインフラにデプロイする．Terraformを実行する（OttoのデプロイはImmutable Infrastructureを嗜好する）  Ottoがつくるインフラの基礎 OttoにはFoundationという概念がある（foundationという言葉は生成される設定ファイルやディレクトリ名に登場する）．これはOttoが構築するインフラの基礎，本番環境にアプリケーションをデプロイするために重要となるレイヤーを示す．このFoundationの例としては，以下のようなものが挙げられる．</description>
    </item>
    
    <item>
      <title>プレゼンするときに考えていること</title>
      <link>https://deeeet.com/writing/2015/09/25/talking/</link>
      <pubDate>Fri, 25 Sep 2015 08:48:30 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/09/25/talking/</guid>
      <description>僕はカンファレンスで喋るのが好きだ．好きだが決して得意ではない．むしろ喋るのは苦手なほうだと思う．
実際に自分でやるまではプレゼンは才能だと思っていた．大学の研究発表などで実際に自分でプレゼンをするようになり，大学の研究室で指導されまくった結果，プレゼンは技術だと認識した（もちろん才能もある）．技術であるということは学ぶことができる．それに気づいてからはたくさんプレゼンに関する本を読んだ．昔は発表前に必ず何か一冊プレゼンに関する本を読みそれを積極的に取り入れるようにした．
得意でないなりに学んで，発表を繰り返した結果なんとなく毎回考えること/意識することが固まってきた．今後のために簡単にまとめておく．
 聴衆は貴重な時間を割いて会場に来る オーガナイザーは貴重な時間を割いてカンファレンスを準備している 聴衆が誰かを妄想する 早めに準備する．早めに準備する．早めに準備する．早めに&amp;hellip; Keynoteを開く前に概要とトークの流れを書く Keynoteを先に開くと流れのない壊滅的な資料ができる 流れを書きつつここでx分/ここでy分という時間も想定する 時間超えるのはクソである むしろ早く終わった方がよい 少しでも有意義なものを受け取ってもらいたいから言いたいことは絞る 言いたいことを絞れば早く終わる 概要から始まり徐々に詳細に向かう 逆茂木型に注意する 前のスライドから次のスライドが想定できるようにする 前のスライドから次のスライドが想定できるようなきっかけを書く Itemizeは文末を揃える 例をなるべく使う 図をなるべく使う 文字を大きくする 文字の配置，大きさ，色を一貫させる 文字の配置，大きさ，色はそれだけで意味を持つ 作り終わったらちゃんと喋ってこの段階のスライドがゴミであることに気づく 喋って直す 喋って詰まるところを喋りやすいように直す どうしても詰まるなら軽くメモを書く（書き過ぎない．あくまでメモ） 会場に向かう前に一度喋っておく 直前まで微調整する プロジェクターの接続テストをする アイスブレイクなんて普通は無理．ふざけるな デモは何度も練習する どんなすごい人でもデモは失敗する 質問は最後までちゃんと聞く．わからなければ聞き直す 本当にわけわからん質問はあとで話しましょうと言う．無理しない（昔わけわからんおっさんと戦ったことがあるが無駄だった）  学術的な学会発表とは違って技術カンファレンスはとても好きだ．テーマは決まっているものの自由に話すことができる． 学会はある程度決められたフォーマットに従っていた．それは聞く側からすればわかりやすさにつながるが，喋る側からすればちょっと堅苦しかった．今は自由な感じで喋れるのを楽しんでいる．
苦手だけど喋るのはなぜか? こんなん作ったとか，こんなんわかったとかシェアしたいという思いがあるから．ブログで書くのもよいけど，プレゼンはまた違ったフォーマットで伝わり方も変わるから楽しい．あと僕は若干コミュニケーションに問題がある．懇親会などで初対面のひとに喋りに行くとかはほぼ無理だ．が，プレゼンしてると，あれを喋った僕です的な感じで喋りに行くきっかけになる．カンファレンスで喋るモチベーションはここにもある．
最後にここで書いているのは表層的な話である．本当に大切なのは内容．とにかく自分が喋る内容に対して自分が一番のプロフェッショナルになるのが大切だと思う．</description>
    </item>
    
    <item>
      <title>Google Omegaとは何か? Kubernetesとの関連は? 論文著者とのQA（翻訳）</title>
      <link>https://deeeet.com/writing/2015/09/17/qa-omega/</link>
      <pubDate>Thu, 17 Sep 2015 18:47:11 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/09/17/qa-omega/</guid>
      <description>エンタープライズ向けのKubernetesサポートを行っているkismatic Inc.による&amp;ldquo;Omega, and what it means for Kubernetes: a Q&amp;amp;A about cluster scheduling&amp;rdquo;が非常に良いインタビュー記事だった．Google Omegaとは何か? 今までのスケジューリングと何が違うのか? 何を解決しようとしているのか? 今後クラスタのスケジューリングにはどうなっていくのか? をとてもクリアに理解することができた．
自分にとってスケジューリングは今後大事になる分野であるし，勉強していきたい分野であるのでKismaticの@asynchio氏と論文の共著者であるMalte Schwarzkopf氏に許可をもらい翻訳させてもらった．
TL;DR 2013年に発表されたOmega論文の共著者であるMalte SchwarzkopfがGoogle OmegaのShared-stateモデルの主な目的がScalabilityよりもむしろソフトウェア開発における柔軟性であったことを説明する．Shared-stateモデルによるスケジューリングは優先度をもったプリエンプションや競合を意識したスケジューリングを可能にする．
今までのTwo-levelモデルのスケジューラー（例えばYARNやMesos）も同様の柔軟さを提供するが，Omega論文が発表された当時は，すべての状態がクラスタから観測できない問題（information hiding）やGang Schedulingにおけるhoardingの問題に苦しんでいた．またなぜGoogleがShare-stateモデルを選択したのか，なぜGoogleは（Mesosのような）厳格な公平性をもったリソース配分を行わないのかについてもコメントする．
最後にMesosやkubernetesのようにスケジューラーのモジュール化をサポートすることでいかにOSSのクラスター管理にOmegaの利点を持ち込むことができるのかを議論する．そしてより賢いスケジューリングを実現することでユーザはGoogleのインフラと同様の効率性を獲得できること提案する．
2013年の論文の発表時と比べて何が変わったか? 何が変わらないか?
クラスタのオーケストレーションは動きの早い分野である．しかしOmegaの中心となる原則はむしろよりタイムリーでとても重要であると思う．
2013年以来の大きな変化として，とりわけKubernetesやMesosのおかげで，Omegaのようなクラスタでインフラを運用するのが一般的になってきたことが挙げられる．2011年や2012年に立ち返ってみるとOmegaのShared-stateモデルによるスケジューリングの基礎となる仮説をGoogle以外の環境で実証するのはなかなかトリッキーなことだとみなされていた．しかし今日では，既存のモジュラーなスケジューラーをハックして新しい手法を試したり，Google cluster traceの公開されたTrace結果を使うことでより簡単にそれができる．
さらにOmegaで挑んだ，いかにクラスタ内で異なるタイプのタスクを効率良くスケジューリングするのか，いかに異なるチームがクラスタの全ての状態にアクセスし彼ら自身のスケジューラーを実装するのか，といった問題は業界で広く認識されてきた．コンテナにより多くの異なるタイプのアプリケーションを共有クラスタ内にデプロイすることが可能になり，開発者たちは全てのタスクを同じようにスケジューリングすることはできない/するべきではないことを認識し始めた．そしてオーケストレーション（例えばZookeeperやetcd）は共有された分散状態を管理する問題であるであるとみなされるようになった．
Omegaのコアにある考え方，つまりジョブのスケジューリングのモデリングとShared-stateモデルに基づくより概括的なクラスタのオペレーションは，まだ適切であると信じている．実際Kubernetesの中心にある考え方，ユーザが望むべき状態を指定しKubernetesがクラスタをそのゴールの状態に移行させること，はまさにOmegaにおいてスケジューラーが次の望むべき状態にクラスタの状態の変更を提案するときに起こっていることと全く同じである．
なぜOmegaのShared-stateモデルは柔軟性があり様々な異なるタスクのリソース管理を効率的に行うことができるのか?
数年前多くの組織が運用していたインフラ環境について考えてみる．例えば1つのHadoopクラスタで複数のユーザによるMapReduceジョブが走っていた．それは非常に簡単なスケジューリングである．全てのマシンにMapReduce worker用にn個のスロットがあり，スケジューラーはmapとreduceタスクを，その全てのスロットが埋まるまでもしくは全てのタスクがなくなるまで，ワーカーに割り当てる．
しかし，このスロットという単位は非常に荒いスケジューリングの単位である．全てのMapReduceジョブが同じ量のリソースが必要であるわけではなく，全てのジョブが与えられたリソースを使い切るわけではない．実際クラスタのいくつかのサーバーではMapReduce以外のプロセスが動いている場合もある．そのためスロットという単位で静的にマシンのリソースを区切るのではなく，タスクをBin-packしてリソースを最適化させるのはより良い考え方である．現代のほとんどのスケジューラーはこれを実現している．
複数のリソース状況に基づく（複数次元の）Bin-packingは非常に難しい（NP完全問題である）．さらに異なるタイプのタスクはそれぞれ別のBin-packingの方法を好むため問題はより難しくなる．例えば，ネットワーク帯域が70%使われているマシンにMapReduceのジョブを割り当てるのは全く問題ないが，webサーバーのジョブをそのマシンに割り当てるのは好ましくない&amp;hellip;
Omegaではそれぞれのスケジューラーは全ての利用可能なリソース，すでに動いているタスク，クラスタの負荷状況を見ることができ，それに基づきスケジューリングを行うことができる（Shared-stateモデル）．言い方を変えると，それぞれのスケジューラーは好きなBin-packingアルゴリズムを使うことができ，かつ全て同様の情報を共有している．
BorgはScalabilityに制限があるのか? OmegaはBorgの置き換えなのか?
違う．論文でそれをよりクリアにできたらと思う．多くのフォローアップで「中央集権型のスケジューラーは巨大なクラスタに対してスケールできないため分散スケジューラーに移行するべきである」と述べられてきた．しかしOmegaの開発の主な目的はScalabilityではなくより柔軟なエンジニアリングにある．Omegaのゴールは，様々なチームが独立してスケジューラーを実装できることにあり，これはScalabilityよりも重要な側面だった．スケジューラーの並列化がもたらすScalabilityは付属の利点にすぎない．実際のところちゃんと開発された中央集権型のスケジューラーは巨大なクラスタとタスクを扱えるまでにスケールできる．Borg論文は実際この点について書いている．
分散スケジューラーが必須になるニッチな状況もある．例えば，既存のワーカーに対して，レイテンシにセンシティブな短いリクエストを送るジョブを高速に配置する必要があるとき．これはSparrow schedulerのターゲットであり分散デザインが適切になる．しかしタスクがレイテンシにセンシティブ，もしくはタスクを秒間に数万回も配置する必要がなければ，中央集権型のスケジューラーであっても1万台のマシンを超えても問題ない．
Omega論文内で指摘しているMesosのTwo-levelモデルのスケジューリングの欠点は何か?
まずOmega論文におけるMesosの説明は2012年当時のMesosに基づいている．それから数年が経っており，論文でのいくつかの指摘はすでに取組まれており，同じように語ることはできない．
オファーベースのモデル，もしくは別のスケジューラーに対してクラスタ状態のサブセットのみを公開するモデルには大きく2つの欠点がある．これは例えばYARNのようなリクエストベースのデザインにも同様のことが言える．
まず1つ目はスケジューラーが割り当てらてた/提供されたリソースのみを見ることができることに関連する．Mesosのオファーシステムにおいて，リソースマネージャーはアプケーションスケジューラーに対して「これだけのリソースがあるよ．どれが使いたい?」と尋ね，アプリケーションスケジューラーはその中から選択を行う．しかしそのときアプリケーションスケジューラーはに別の関連する情報を知ることができない．例えば，自分には提供されなかったリソースは誰が使っているのか，より好ましいリソースがあるのか（そのために提供されたリソースを拒否してより良いリソースを待ったほうがよいのか）という情報を知ることができない．これがinformation hidingの問題である．information hidingによって問題になる他の例には優先度をもったプリエンプションがある．もし優先度の高いタスクが優先度の低いタスクを追い出すことができる必要があるとき，優先度の低いタスクが配置されるどの場所もまた効率的なリソースのオファーがある，がスケジューラーはそれをみることができない．
もちろんこれは解くことができる問題である．例えばMesosは優先度の低いタスクに利用されているリソースを優先度の高いタスクに提供することができる．もしくはスケジューラーのフィルターでプリエンプション可能なリソースを指定することもできる．しかしこれはリソースマネージャーのAPIとロジックが複雑になる．
2つ目はhoardingの問題．これは特にGang Shedulingによりスケジューリングされたジョブに影響を与える．このようなジョブは他のジョブが起動する前にリクエストした全てのリソースを獲得しておかなければならない．例えばこのようなジョブにはMPIがあるが，他にもstatefulなストリーム処理は起動するためにパイプライン全体が確保されている必要がある．MesosのようなTwo-levelモデルではこれらのジョブには問題が生じる．アプリケーションスケジューラーは要求したリソースが全て揃うまで待つ（揃わないかもしれない）こともできるし，十分なリソースを蓄積するため少量のオファーを順番に受け入れることもできる．もし後者なら，しばらくの間他のジョブ（例えば優先度の低いMapReduceのジョブなど）に効率良く利用できる可能性があるのにも関わらず，十分な量のリクエストが受け入れられるまでリソースは使われることなく蓄積（hoarding）される．
最近MesosphereでMesosの開発をしているBen Hindmanとこの問題ついて話したが，彼らはこれらを解決する並列のリソースオファー/予約が可能になるようにコアのオファーモデルを変更する計画があると話していた．例えば，Mesosは複数のスケジューラーに対して同じリソースをOptimisticに提供し，消失したスケジューラーのオファーを「無効にする」ことが可能になる．これはOmegaと同じ競合の解決が必要になる．その時点で2つのモデルは同じところに到達する．もしMesosがクラスタの全てのリソースをスケジューラーに提供するならそのスケジューラーはOmegaと同じ視点をもつことになる（詳しくは&amp;ldquo;proposal: Mesos is isomorphic to Omega if makes offers for everything available&amp;rdquo;）．しかしまだ実装は初期段階にある．</description>
    </item>
    
    <item>
      <title>Apache Kafkaに入門した</title>
      <link>https://deeeet.com/writing/2015/09/01/apache-kafka/</link>
      <pubDate>Tue, 01 Sep 2015 18:13:38 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/09/01/apache-kafka/</guid>
      <description>Apache kafka
最近仕事でApache Kafkaの導入を進めている．Kafkaとは何か? どこで使われているのか? どのような理由で作られたのか? どのように動作するのか（特にメッセージの読み出しについて）? を簡単にまとめておく（メッセージングはまだまだ勉強中なのでおかしなところがあればツッコミをいただければ幸いです）．
バージョンは 0.8.2 を対象に書いている．
Apache Kafkaとは? 2011年にLinkedInから公開されたオープンソースの分散メッセージングシステムである．Kafkaはウェブサービスなどから発せられる大容量のデータ（e.g., ログやイベント）を高スループット/低レイテンシに収集/配信することを目的に開発されている．公式のトップページに掲載されているセールスポイントは以下の4つ．
 Fast とにかく大量のメッセージを扱うことができる Scalable Kafkaはシングルクラスタで大規模なメッセージを扱うことができダウンタイムなしでElasticかつ透過的にスケールすることができる Durable メッセージはディスクにファイルとして保存され，かつクラスタ内でレプリカが作成されるためデータの損失を防げる（パフォーマンスに影響なくTBのメッセージを扱うことができる） Distributed by Design クラスタは耐障害性のある設計になっている  どこで使われているのか? Use Casesをあげると，メッセージキューやウェブサイトのアクティビティのトラッキング（LinkedInのもともとのUse Case），メトリクスやログの収集，StormやSamzaを使ったストリーム処理などがあげられる．
利用している企業は例えばTwitterやNetflix，Square，Spotify，Uberなどがある（cf. Powered By）．
Kafkaの初期衝動 Kafkaのデザインを理解するにはLinkedInでなぜKafkaが必要になったのかを理解するのが早い．それについては2012年のIEEEの論文&amp;ldquo;Building LinkedIn&amp;rsquo;s Real-time Activity Data Pipeline&amp;rdquo;を読むのが良い．簡単にまとめると以下のようになる．
LinkedInでは大きく2つのデータを扱っている．1つはウェブサイトから集められる大量のユーザのアクティビティデータ．これらをHadoop（バッチ処理）を通して機械学習しレコメンド/ニュースフィードなどサービスの改善に用いている．それだけではなくこれらのデータはサービスの監視（セキュリティなど）にも用いている．2つ目はシステムのログ．これらをリアルタイムで処理してサービスのモニタリングを行っている．これらは近年のウェブサービスではよく見かける風景．
問題はそれぞれのデータの流れが1本道になっていたこと．アクティビティデータはバッチ処理に特化していたためリアルタイム処理ができない，つまりサービス監視には遅れが生じていた．同様にシステムのログは，リアルタイム処理のみに特化していたため長期間にわたるキャパシティプランニングやシステムのデバッグには使えなかった．サービスを改善するにはそれぞれタイプの異なるデータフィードを最小コストで統合できるようにする必要があった．またLinkedInのようにデータがビジネスのコアになる企業ではそのデータを様々なチームが簡単に利用できる必要があった．
これら問題を解決するために大ボリュームのあらゆるデータを収集し様々なタイプのシステム（バッチ/リアルタイム）からそれを読めるようにする統一的ななメッセージプラットフォームの構築が始まった．
最初は既存のメッセージシステム（論文にはActiveMQを試したとある）の上に構築しようとした．しかしプロダクションレベルのデータを流すと以下のような問題が生じた．
 並列でキューのメッセージを読むにはメッセージごとに誰に読まれたかを記録する必要がある（mutex）．そのため大量のデータを扱うとメモリが足りなくなった．メモリが足りなくなると大量のRamdom IOが発生しパフォーマンスに深刻な影響がでた バッチ処理/リアルタイム処理の両方でキューを読むには少なくとも2つのデータのコピーが必要になり非効率になった  このような問題から新しいメッセージシステム，Kafkaの開発が必要になった．Kafkaが目指したのは以下．
 あらゆる種類のデータ/大容量のデータを統一的に扱う 様々なタイプのシステム（バッチ/リアルタイム）が同じデータを読める 高スループットでデータを処理する（並列でデータを読める）  どのように動作するのか?（概要） KafkaはBroker（クラスタ）とProducer，Consumerという3つのコンポーネントで構成される．Producerはメッセージの配信を行いConsumerはメッセージの購読を行う．そしてKafkaのコアであるBrokerはクラスタを構成しProducerとConsumerの間でメッセージの受け渡しを行うキューとして動作する．
http://kafka.apache.org/images/producer_consumer.png
メッセージのやりとり
KafkaはTopicを介してメッセージのやりとりを行う．Topicとはメッセージのフィードのようなものである．例えば，検索に関わるデータを&amp;rdquo;Search&amp;rdquo;というTopic名でBrokerに配信しておき，検索に関わるデータが欲しいConsumerは&amp;rdquo;Search&amp;rdquo;というTopic名を使ってそれをBrokerから購読する．
Pull vs Push
BrokerがConsumerにデータをPushするのか（fluentd，logstash，flume），もしくはConsumerがBrokerからデータをPullするのかはメッセージシステムのデザインに大きな影響を与える．もちろんそれぞれにPros/Consはある．KafkaはPull型のConsumerを採用している．それは以下の理由による．
 Pushだと様々なConsumerを扱うのが難しく，Brokerがデータの転送量などを意識しないといけない．Kafkaの目標は最大限のスピードでデータを消費することだが，（予期せぬアクセスなどで）転送量を見誤るとConsumerを圧倒してまう．PullだとConsumerが消費量を自らが管理できる． Pullだとバッチ処理にも対応できる．Pushだと自らそれを溜め込んだ上でConsumerがそれを扱えるか否かに関わらずそれを送らないといけない (PullでしんどいのはBrokerにデータがまだ届いてない場合のコストだがlong pollingなどでそれに対応している)  メッセージのライフサイクル</description>
    </item>
    
    <item>
      <title>Go1.5はクロスコンパイルがより簡単</title>
      <link>https://deeeet.com/writing/2015/07/22/go1_5-cross-compile/</link>
      <pubDate>Wed, 22 Jul 2015 09:54:57 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/07/22/go1_5-cross-compile/</guid>
      <description>Cross compilation just got a whole lot better in Go 1.5 | Dave Cheney Go 1.5: Cross compilation — Medium  Go言語の良さの一つにあらゆるOS/Archに対するクロスコンパイルがとても簡単に行えることが挙げられる．今まで（Go1.4以前）も十分に便利だったがGo 1.5ではさらに良くなる．
今までの問題を敢えて挙げるとターゲットとするプラットフォーム向けのビルドtool-chain準備する必要があるのが煩雑であった（cf. Go のクロスコンパイル環境構築 - Qiita）
$ cd $(go env GOROOT)/src $ GOOS=${TARGET_OS} GOARCH=${TARGET_ARCH} ./make.bash --no-clean  $ gox -build-toolchain  この作業は1つの環境で一度だけ行えばよいのでそれほど煩雑ではない．しかし，例えばDockerなどでクロスコンパイル環境を提供すると（e.g., tcnksm/dockerfile-gox），ビルドに時間がかかったりイメージが無駄に重くなったりという問題がおこる．初めてクロスコンパイルをしようとするひとにとってもつまづいてしまうポイントだったと思う．
Go1.5ではコンパイラがGoで書き直された（cf. Go in Go）ため，この準備作業が不要になる．goはコンパイル前に必要な標準パッケージを検出しそれらをターゲットのプラットフォーム向けにビルドしてくれる．
使ってみる Go1.5を準備する．Go1.5のビルドにはGo1.4が必要．
$ git clone https://go.googlesource.com/go $HOME/go1.5 $ cd $HOME/go1.5 &amp;amp;&amp;amp; git checkout -b 1.5 refs/tags/go1.5beta2 $ cd $HOME/go1.5/src &amp;amp;&amp;amp; GOROOT_BOOTSTRAP=$HOME/go1.</description>
    </item>
    
    <item>
      <title>Go言語のDependency/Vendoringの問題と今後．gbあるいはGo1.5</title>
      <link>https://deeeet.com/writing/2015/06/26/golang-dependency-vendoring/</link>
      <pubDate>Fri, 26 Jun 2015 12:15:03 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/06/26/golang-dependency-vendoring/</guid>
      <description>Go言語のDependency/Vendoringは長く批判の的になってきた（cf. &amp;ldquo;0x74696d | go get considered harmful&amp;rdquo;, HN）．Go1.5からは実験的にVendoringの機能が入り，サードパーティからはDave Chaney氏を中心としてgbというプロジェクベースのビルドツールが登場している．なぜこれらのリリースやツールが登場したのか?それらはどのように問題を解決しようとしているのか?をつらつらと書いてみる．
Dependencyの問題 最初にGo言語におけるDependecy（依存解決）の問題についてまとめる．Go言語のDependencyで問題なのはビルドの再現性が保証できないこと．この原因はimport文にある．
Go言語で外部パッケージを利用したいときはimport文を使ってソースコード内にそれを記述する．このimport文は2通りの解釈のされ方をする．go getはリモートレポジトリのfetch URLとして解釈し，コンパイラはローカルディスク上のソースのPathとして解釈する．例えばコマンドラインツールを作るときに外部パッケージとしてmitchellh/cliを使いたい場合は以下のように記述する．
import &amp;quot;github.com/mitchellh/cli&amp;quot;  これが書かれたコードをgo getすると，ローカルディスクにmitchellh/cliがなければ$GOPATH/src以下にそれがfetchされる．ビルド時はそのPathに存在するコードが利用される．
importで問題になるのは，そこにバージョン（もしくはタグ，Revision）を指定できないこと．そのため独立した2つのgo getが異なるコードをfetchしてしまう可能性がある．そのコードが互換をぶっ壊していたらビルドは失敗するかもしれない．つまり現状何もしないとビルドの再現性は保証できない．
では以下のようにタグやバージョンを書けるようにすれば?となる．が，これは言語の互換を壊すことになる．
import &amp;quot;github.com/pkg/term&amp;quot; &amp;quot;{hash,tag,version}&amp;quot;  以下のようにディレクトリ名にバージョン番号を埋め込むという方法もよく見る．が，これも結局異なるRevisionのコードをFetchしてまうことに変わりはなくビルドに再現性があるとは言えない．
import &amp;quot;github.com/project/v7/library&amp;quot;  Vendoring 再現性の問題を解決する方法として，依存するレポジトリを自分のレポジトリにそのまま含めてしまう（vendoringと呼ばれる）方法がある．こうしておくと依存レポジトリのupstreamの変更に影響を受けず，いつでもどのマシンでもビルドを再現できる．
しかし何もしないとコンパイラがそのレポジトリのPathを探せなくなりビルドができなくなる．ビルドするには以下のどちらかを行う必要がある．
 $GOPATHの書き換え importの書き換え  $GOPATHの書き換え まずは$GOPATHを書き換える方法．この場合はそもそもコードをvendoringするときに$GOPATH/src/github.com...と同じディレクトリ構成を作らなければならない．その上でそのディレクトリを$GOPATHに追加してビルドを実行する．
例えば外部パッケージmitchellh/cliをレポジトリ内のextディレクトリにvendoringしたい場合は，まず以下のようなディレクトリ構成でそれをvendoringをする．
$ tree ext ext └── src └── github.com └── mitchellh └── cli  そしてビルド時は以下のように$GOPATHにextディレクトリを含めるようにする．
$ GOPATH=$(pwd)/ext:$GOPATH go build  このやり方が微妙なのは毎回自分で$GOPATHの変更を意識しないといけないこと（Fork先でも意識してもらわないといけない）．
importの書き換え 次にimportを書き換える方法．レポジトリ内のvendoringしたディレクトリへと書き換えてしまう．例えばgithub.com/tcnksm/rというレポジトリのextディレクトリに外部パッケージmitchellh/cliをvendoringしたとする．この場合は以下のようにimportを書き換える．
import &amp;quot;github.com/mitchellh/cli&amp;quot; // Before  import &amp;quot;github.com/tcnksm/r/ext/cli&amp;quot; // After  これはあまり見ない．そもそもソースを書き換えるのが好まれないし，upstreamを見失うかもしれない．また多くの場合import文が異常に長く複雑になる．</description>
    </item>
    
    <item>
      <title>サンフランシスコでたくさんコーヒー飲んだ</title>
      <link>https://deeeet.com/writing/2015/06/07/sf-coffee/</link>
      <pubDate>Sun, 07 Jun 2015 23:03:03 -0700</pubDate>
      
      <guid>https://deeeet.com/writing/2015/06/07/sf-coffee/</guid>
      <description>Google I/O 2015のためにサンフランシスコを訪れた．サンフランシスコといえば対岸のオークランドにサードウェーブの代表格となったBlue Bottle Coffeeの焙煎所があり，サードウェーブコーヒーの流れを受けた様々な有名ロースターが存在している．コーヒー狂としてはロースター巡りをせずにはいられず滞在中は時間があればロースターに行っていた．以下は行ったところまとめ．
Four Barrel Coffee．Valencia St.にある．Drip Coffee（Pour-over）のために別途スタンドが準備されていて店員が豆の説明をしながら丁寧にドリップをしてくれる．COLOMBIAのla CABANA農場のものを飲んだがこれが圧倒的に美味かった（今回一番美味しかったと思う）．衝撃を受けた．ここは&amp;ldquo;Four Barrel, San Francisco Coffee Shop, Bans Instagram Photos, &amp;lsquo;Hipster Topics&amp;rsquo;&amp;rdquo;が有名で，厳しい感じなのかなと思ったが，今はこの張り紙は見当たらなかった．店内はエイジングされた木材と鉄のインテリアを基調とした男臭い感じでかっこ良かった．
Ritual Coffee．これもValencia St.にある（Mission Dolores Parkの近く）．赤色のロゴマークが特徴．ここでは&amp;ldquo;Sweet Tooth Espresso&amp;rdquo;を飲んだ．Sweet toothという豆は農園に対して金銭的/知的投資をし長年かけて丁寧に作られたことで有名．これも衝撃的だった．最初に少し強目の酸味を感じるが後味はすっきりしていて体験したことのない味だった（玄人向けらしい&amp;hellip;）．店内も白を基調としていても良い雰囲気だった（Flickrの社員とかが昔ミーティングをここでやっていたとか）
Sightglass Coffee．SOMA地区にある（夕方になるとちょっと怖い）．SquareのJack Dorseyが投資していることで有名．ここは2回訪れてそれぞれDripとAffogato（Espresso+Ice cream）を飲んだ．Affogatoは初めて飲んだがSingle Origin豆の独特の酸味と苦味+アイスの甘さ（アイスは試食して好きなの選べる）を合わせて不思議な味がした．Sightglassは2階まで席があり，自然光を取り込んだ店内はいい感じに明るくて過ごしやすい．PCを開いて作業をしているひとや最近作ったアプリについてアツく議論している人などがいて良い雰囲気だった．ここで書くGo言語は最高だった．通いたい．
Beacon Coffee &amp;amp; Pantry．Fisherman&amp;rsquo;s wharfの帰りに偶然発見した．Washington Square付近にある．Sightglassの豆を使っている．丁寧にDripをしてくれてとても美味しかった．自分も家で使っている青芳製作所のヤカンを使ってドリップをしていた（他の店でも日本製のドリッパーなどをたくさん見かけた．特にHARIO）．ふらっと歩いていても美味いコーヒー屋にぶつかるなんて最高だなと思った．Take outしてWashington Squareで謎の遊戯にいそしむ若者たちを眺めつつ飲んだ．
Blue Bottle Coffee．Mint Plaza．サードウェーブの代表格として有名．Tech系の企業からもたくさん投資を受けている（cf. ブルーボトルの夢：「コーヒー界のアップル」はいかに4,500万ドルを調達したか« WIRED.jp）．最近東京にも進出したので特に行く必要ないと思ってたけど，東京とは違った雰囲気があるかなと思い行ってみた．Dripを頼んだが，店内ではSiphonを多く見かけた．元々なのか今はそちらを推しているのか．何れにせよ次行くときはSiphonを試したい．店内は洗練されていて素敵だった．
Coffee Bar．Bryant St.にある（Four Barrelの近く）．Mr. Espressoのプロデュースとして有名．時間がなくて店内で飲むことはできなかったがいくつか豆を購入した．後日スペイン料理を食べに行ったときにちょうど食後の飲み物としてMr. Espressoの豆を使ったEspressoが提供されていて飲むことができた．こんなに飲みやすいEspressoは初めてだ！と思うくらいすっきりした味わいで美味しかった．
他にもHaus Coffeeやcentoにも行った．
行きたくていけなかったのはWrecking Ball CoffeeとかEquator CoffeeとかCafe 3016とか．次回来るときは絶対挑戦したい．
日本 日本にもサードウェーブに影響を受けたロースターがいくつかある．以下で軽くまとめた．
 東京サードウェーブコーヒー  参考 サードウェーブのコーヒー文化は茶太郎豆央さんの一連の書籍から学んだ（1冊目はkindleで出版されていてさらっと読めるので興味のあるひとは是非）．
 サードウェーブ！：サンフランシスコ周辺で体験した最新コーヒーカルチャー サードウェーブ・コーヒー読本  以下の書籍はコーヒーの味について科学的な視点から知れて良い．自分でコーヒー入れるひとは読んでみてもいいかもしれない（例えば酸味成分と苦味成分がどのように抽出されるのか，それはなぜかを知れたりする）．</description>
    </item>
    
    <item>
      <title>Docker社を訪問した</title>
      <link>https://deeeet.com/writing/2015/06/03/visit-docker-inc/</link>
      <pubDate>Wed, 03 Jun 2015 16:56:34 -0700</pubDate>
      
      <guid>https://deeeet.com/writing/2015/06/03/visit-docker-inc/</guid>
      <description>Google I/O 2015のためにサンフランシスコを訪れたついでにDocker社に遊びに行った．Docker社はサンフランシスコのダウンタウンを南に下った475 Brannan St.にある（ちなみに275にはGitHub社がある）．
迎えてくれたのはNathan．Nathanとは昨年東京で開催されたCommunities meetup Chef, Docker, Openstack, Puppetで出会った．その後もtwitterで絡んでおり今回訪問させてもらうことになった．
まず，近くにカフェ（Blue Bottleで焙煎された豆を使っていた）がありコーヒーを片手に近況などをゆっくり話した．NathanはDocker Machineをメインに担当していて，最近追加した機能や今後の予定などについて語ってくれた．今はv0.3.0に向けてRCを出して絶賛テスト中とのこと．Docker Machineは他社のサービスに依存するのでテストはなかなか大変らしい．
Docker Machineは今後Dockerデーモンの煩雑な設定を楽にする方向に向かうとのこと（詳しくはどこまで言っていいのかわからないので書きません）．Docker Machineでデーモン層をDocker Composeでコンテナ層をと担当を分けるてDockerを使った開発・運用を楽にしていく．Dockerを使った開発環境の構築を楽にしたいという思いがめちゃ伝わって良かった．
もともとWebエンジニアでPHPとかを書いてたけどGo言語に興味もって書いててGo言語ならDockerっしょとなりDockerに就職できたという夢のある話も聞いた．Go言語書くぞ！
その後はオフィスの様子を見せてもらった．
オフィスの様子 建物の入り口（他にもいくつかの企業が入っていた）
受付
オフィス
Nathanの（自作）スタンディングデスク
キッチン
非公式キャラクター（クジラは飼えない）
任天堂との繋がり
コンテナ
まとめ Nathanは今年のYAPC::2015で発表する予定になっているので絶対聞きに行きましょう！（&amp;ldquo;Docker For Polyglots : Where We&amp;rsquo;ve Come From, and Where We Can Go&amp;rdquo;）．日本でDockerのエンジニアの話を聞けるのはめちゃめちゃ貴重だと思います．
Thanks, Nathan :)</description>
    </item>
    
    <item>
      <title>Golang Cross Compiler on Heroku (with Docker)</title>
      <link>https://deeeet.com/writing/2015/05/11/gox-server/</link>
      <pubDate>Mon, 11 May 2015 22:35:55 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/05/11/gox-server/</guid>
      <description>Heroku unveils new CLI functionality heroku docker:release (cf. &amp;ldquo;Heroku | Introducing &amp;lsquo;heroku docker:release&amp;rsquo;: Build &amp;amp; Deploy Heroku Apps with Docker&amp;rdquo;). You can run Heroku&amp;rsquo;s Cedar environment on Docker container and test your application in local environment (Environment parity). In addition to that, you can create Slug from that docker image and deploy it directly to Heroku.
Before this release, Heroku provided the way to create Slug by Buildpack. Buildpack is powerful but for me it&amp;rsquo;s a little bit complex and hard to write from scratch.</description>
    </item>
    
    <item>
      <title>Herokuの&#39;docker:release&#39;の動き</title>
      <link>https://deeeet.com/writing/2015/05/07/heroku-docker/</link>
      <pubDate>Thu, 07 May 2015 09:08:55 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/05/07/heroku-docker/</guid>
      <description>Introducing &amp;lsquo;heroku docker:release&amp;rsquo;: Build &amp;amp; Deploy Heroku Apps with Docker
HerokuがDockerを使ったツールを提供し始めた．一通り触ってコードもちょっと読んでみたので現時点でできること，内部の動きについてまとめる．
TL;DR  Herokuのデプロイ環境とおなじものをDockerでつくれる Buildpackを使わないでDockerfileからSlugを作れる  自分の好きなDockerイメージをHeroku上で動かせるようになるわけではない．
何ができるのか まず何ができるようになったのかについて簡単に書く．プラグインをインストールするとDockerコマンドが使えるようになる．
$ heroku plugins:install heroku-docker  カレントディレクトリの言語/フレームワークに応じた専用のDockerfileを生成する（これはなんでも好きに書けるDockerfileではないことに注意）．
$ heroku docker:init Wrote Dockerfile (ruby)  上記で作成したDockerfileをもとにDockerコンテナを起動してコンテナ内でアプリケーションを起動する．
$ heroku docker:start ... web process will be available at http://192.168.59.103:3000/  起動したコンテナ内でOne-offコマンドを実行する．
$ heroku docker:exec bundle exec rake db:migrate  開発が終わったら上記のDockerイメージからSlug（アプリケーションのソースとその依存関係を全て含めた.tgz）を作成してそれをそのままHeroku上にデプロイすることができる．デプロイされるとそれは通常通りにDynoになりアクセスできるようになる．
$ heroku docker:release $ heroku open  以下の制約を満たせば生成されたDockerfileを自分なりに編集することもできる．
 heroku:cedarイメージをベースにする /appディレクトリ以下に変更が入るようにする  詳しくは以下で説明する．
内部の仕組み 上記のワークフローで実際に何が行われているのかをコード/コマンドレベルで追ってみる．プラグインのソースはhttps://github.</description>
    </item>
    
    <item>
      <title>Go言語でプラグイン機構をつくる</title>
      <link>https://deeeet.com/writing/2015/04/28/pingo/</link>
      <pubDate>Tue, 28 Apr 2015 13:18:07 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/04/28/pingo/</guid>
      <description>dullgiulio/pingo
Go言語でのプラグイン機構の提供方法は実装者の好みによると思う（cf. fluentd の go 実装におけるプラグイン構想）．Go言語はクロスコンパイルも含めビルドは楽なのでプラグインを含めて再ビルドでも良いと思う．が，使う人がみなGo言語の環境を準備しているとも限らないし，使い始めてもらう障壁はなるべく下げたい．プラグインのバイナリだけを持ってこればすぐに使えるという機構は魅力的だと思う．
Go言語によるプラグイン機構はHashicorpの一連のプロダクトやCloudFoundryのCLIなどが既に提供していてかっこいい．net/rpcを使っているのは見ていてこれを自分で1から実装するのは面倒だなと思っていた．
dullgiulio/pingoを使うと実装の面倒な部分を受け持ってくれて気軽にプラグイン機構を作れる．
使い方 サンプルに従ってプラグインを呼び出す本体とプラグインを実装してみる．
まず，プラグイン側の実装（plugins/hello-world/main.go）は以下．
package main import ( &amp;quot;github.com/dullgiulio/pingo&amp;quot; ) type HelloPlugin struct{} func (p *HelloPlugin) Say(name string, msg *string) error { *msg = &amp;quot;Hello, &amp;quot; + name return nil } func main() { plugin := &amp;amp;HelloPlugin{} pingo.Register(plugin) pingo.Run() }  structとしてプラグインを定義し，メソッドを定義する．メイン関数はそれを登録（Register）して起動（Run）するだけ．
プラグインはあらかじめビルドしておく．
$ cd plugins/hello-world $ go build  次にプラグインを呼び出す本体の実装は以下．上のプラグインで実装したSay()を呼び出す．
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/dullgiulio/pingo&amp;quot; ) func main() { p := pingo.</description>
    </item>
    
    <item>
      <title>Content Addressable DockerイメージとRegistry2.0</title>
      <link>https://deeeet.com/writing/2015/04/20/docker-1_6_distribution/</link>
      <pubDate>Mon, 20 Apr 2015 22:55:35 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/04/20/docker-1_6_distribution/</guid>
      <description>Docker 1.6: Engine &amp;amp; Orchestration Updates, Registry 2.0, &amp;amp; Windows Client Preview | Docker Blog
Docker1.6が出た．コンテナやイメージのラベリング（RancherOSの&amp;ldquo;Adding Label Support to Docker 1.6&amp;rdquo;がわかりやすい）や，Logging Driversといった新機能が追加された．今回のリリースで自分的に嬉しいのはDockerイメージがContent-addressableになったこと（#11109）．
今までDocker Regitryを介したイメージのやりとりはイメージの名前とタグ（e.g., tcnksm/golang:1.2）しか使うことができなかった．タグはイメージの作成者によって付与されるのもであり，同じタグであっても必ず同じイメージが利用できるという保証はなかった（Gitでいうとコミットハッシュが使えず，タグのみしか使えないという状況）．
Docker1.6と同時に発表されたRegistry2.0（docker/distribution）によりイメージにユニークなID（digest）が付与されるようになり，確実に同じイメージを参照することができるようになった（immutable image references）．
使ってみる DockerHubはすでにRegistry2.0になっているのですぐにこの機能は使える．が，今回は自分でPrivate Registryを立ててこの機能を試してみる（環境はboot2docker on OSX）．
まずはRegistryを立てる．v1と同じようにDockerイメージが提供されている．
$ docker run -p 5000:5000 registry:2.0  簡単なDockerfileを準備してtcnksm/test-digestイメージをビルドする．
FROM busybox  $ docker build -t $(boot2docker ip):5000/tcnksm/test-digest:latest .  imagesコマンドで確認する．--digestsオプションをつけるとdigestが表示されるようになる．Gitと同じように考えると直感とズレるかもしれないがbuildするだけではdigestは生成されない．
$ docker images --digests REPOSITORY TAG DIGEST IMAGE ID CREATED VIRTUAL SIZE 192.168.59.103:5000/tcnksm/test-digest latest &amp;lt;none&amp;gt; 8c2e06607696 3 days ago 2.</description>
    </item>
    
    <item>
      <title>Go言語のCLIツールのpanicをラップしてクラッシュレポートをつくる</title>
      <link>https://deeeet.com/writing/2015/04/17/panicwrap/</link>
      <pubDate>Fri, 17 Apr 2015 00:18:38 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/04/17/panicwrap/</guid>
      <description>mitchellh/panicwrap
Go言語でpanicが発生したらどうしようもない．普通はちゃんとテストをしてそもそもpanicが発生しないようにする（もしくはトップレベルでrecoverする）．しかし，クロスコンパイルして様々な環境に配布することを，もしくはユーザが作者が思ってもいない使いかたをすることを考慮すると，すべてを作者の想像力の範疇のテストでカバーし，panicをゼロにできるとは限らない．
panicが発生した場合，ユーザからすると何が起こったか分からない（Go言語を使ったことがあるユーザなら「あの表示」を見て，panicが起こったことがわかるかもしれない）．適切なエラーメッセージを表示できると良い．開発者からすると，そのpanicの詳しい発生状況を基に修正を行い，新たなテストケースを追加して二度とそのバグが発生しないようにしておきたい．
mitchellh/panicwrapを使うと，panicが発生したときにそれ（バイナリ）を再び実行し，設定したhandlerを実行することで，その標準出力/エラー出力を取得することができる．このパッケージを使えばpanicが起こったときに詳細なクラッシュレポートを作成し，ユーザにそれを報告してもらうことができる．
使い方 使い方は簡単でトップレベルにhandlerを登録するだけ．まず簡単に動作例を説明する．以下の例はpanicが発生したときにそのpanicの出力をcrash.logに書き込む例．
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;os&amp;quot; &amp;quot;github.com/mitchellh/panicwrap&amp;quot; ) func main() { // (1) exitStatus, _ := panicwrap.BasicWrap(handler) // (2) if exitStatus &amp;gt;= 0 { os.Exit(exitStatus) } // (3) panic(&amp;quot;Panic happend here...&amp;quot;) } func handler(output string) { f, _ := os.Create(&amp;quot;crash.log&amp;quot;) fmt.Fprintf(f, &amp;quot;The child panicked!\n\n%s&amp;quot;,output) os.Exit(1) }  以下のように動作する．
 (1)ではhandlerをBasicWrapに登録する (2)はpanicが発生した場合．この場合exitStatusは0以上の値になる (3)は通常の実行．ここでpanicを発生させている．  動作としては(2)でpanicが発生し(0)で登録したhandler（ここではcrash.logへの書き込み）を実行し，(1)で終了する．
内部の仕組み panicwrapが何をしているかを簡単に見てみる．BasicWrapからWrapが呼ばれ中で自分自身（バイナリ）を実行している．
exePath, err := osext.Executable() .... cmd := exec.</description>
    </item>
    
    <item>
      <title>CoreOS Meetup Tokyo #1 を開催した</title>
      <link>https://deeeet.com/writing/2015/04/13/coreos-meetup-tokyo-1/</link>
      <pubDate>Mon, 13 Apr 2015 22:03:28 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/04/13/coreos-meetup-tokyo-1/</guid>
      <description>CoreOS Meetup Tokyo #1 - connpass
今回のMeetupは，etcd2.0のリリースやrktの登場，5月のCoreOS Fest 2015，また各社のCoreOSの導入事例の兆しを受けての開催．といってもCoreOSの利用事例はまだ少ないと感じたため，CoreOSだけではなくその関連技術やプラットフォームをテーマとした．それでも20分の発表8本というとても濃いMeetupとなり非常に勉強になった．またそこまで人は集まらないと思っていたところ100人枠に350人の応募があり，注目の高さにも驚いた（次回は抽選にするなど考慮します）．
発表資料は全て，CoreOS Meetup Tokyo #1 - 資料一覧 - connpassにまとめてある．が，簡単にMeetupの内容をまとめておく．各種テーマが散っているので自分なりにまとめておく．
概要 まず，自分からはCoreOSについて知らない人でもMeetupにキャッチアップできるできるようにCoreOSの概要を簡単に紹介した．
 Introduction of CoreOS
CoreOSとは何か，モチベーションは何か，どんな特徴がありどんな技術が使われているのかについて話した．CoreOSについてはRebuildでも簡単にしゃべらせてもらったので参考にしてください．
 Rebuild: 83: Living In A Container (deeeet)  要素技術 次にCoreOSで使われている技術について．今回はコンテナの話が2本とFleetの概要の発表が1本だった．
 @mopemope, &amp;ldquo;CoreOS/Rocket&amp;rdquo; - rktとは何か，どのような思想で作られているのか，Dockerとは何が違うのか，現状何ができるのかについて．特にDockerとの比較はとてもわかりやすくて良かった．「Dockerほど高機能ではないが，その分シンプルで組み合わせ可能」という説明が確かにという感じ． @kawamuray, &amp;ldquo;Docker + Checkpoint/Restore&amp;rdquo; - CoreOS関係ないw CRIUの応用はLive migrationくらいだと思っていたけど，コンテナの中身（アプリケーション）に依存せずに起動を高速化するという応用は今後大切になりそう（複数の役割のコンテナをMicroservices的に立てるときに1つだけ起動が遅くてそれを考慮しようとすると管理側に複雑さが入り込む．特に大規模な分散システムだと問題が顕著になりそう）． @spesnova, &amp;ldquo;Understanding fleet&amp;rdquo; - Fleetの仕組みについて．FleetのスケジューリングはMesosほど複雑なことをしてなくて，もっとも少ないUnitを実行しているAgentに均等にタスクを割り振っているだけ．自分的にはこのシンプルさが好きだし，これで十分なことも多いと思う．  IaaS CoreOS OEMの話をNIFTY Cloudの@higebuさんにしていただいた（&amp;ldquo;CoreOS OEM in NIFTY Cloud&amp;rdquo;）．CoreOSのOEMとは，IaaSプロバイダーがそのプラットフォームでCoreOSを利用可能にするために，プラットフォームの独自の設定などをCoreOSに取り込んでもらうための仕組みでCloudに特化したCoreOSならではのもの．NIFTY CloudもCoreOSが使えるようになっており，その経緯について話してももらった．
普通の人はIaaSサービスでCoreOSを使うだけなので，これは万人向けの話ではない．が，CoreOSがどのように提供されるのかを知ることは使う側としてとても大事だと思うので，非常に勉強になった．特にたくさんあるCoreOSのレポジトリがどんな役割があるのかを知れて良かった．
プロダクション運用 実際にCoreOSで実運用を始めている話が2本あった．
 @spesnova, &amp;ldquo;CoreOS 運用の所感&amp;rdquo; - Wantedlyでの運用について．etcd/fleetをオフにして運用しているとのこと．ダイナミックデプロイを犠牲にしても得られるCoreOS利点として，ホストマシンの構築タスクがほぼない．AMI焼く必要もない，脆弱性対応（Ghostとか）が楽，起動が速い，コンテナとOSの分離（ホストの抽象化）の促進はそうだなと思うし，導入の参考になると思う．Channelの選びかたも参考になった．Docker後の世界でOpsとDevがいかに協調するか，という話はとても面白かったし賛同できた @harukasan, &amp;ldquo;CoreOSで運用する際に考えないといけないこと&amp;rdquo; - pixivでの運用について．こちらはFleetとetcdも使っているとのこと．とにかく導入時に考えるべきことが網羅されていて良かった．自分的にはCoreOSをどう捉えているのか，もしくはどのような場合に選択肢になるのか，k8sが必要になるのはどのラインか考え方がとても共感できた（cf.</description>
    </item>
    
    <item>
      <title>Go言語のツールが最新バージョンであるかをユーザに伝えるためのgo-latestというパッケージをつくった</title>
      <link>https://deeeet.com/writing/2015/04/07/go-latest/</link>
      <pubDate>Tue, 07 Apr 2015 00:02:56 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/04/07/go-latest/</guid>
      <description>tcnksm/go-latest
Webアプリケーションとは異なり，コマンドラインツールやモバイルアプリはバージョンアップがユーザに委ねられる．そのため一度リリースしてしまうとバージョンアップをしてもらうのが難しくなる（バグを含めてしまった場合にロールバックもできない cf. &amp;ldquo;Mobile First Development at COOKPAD #deploygate&amp;rdquo;）．とにかくしっかりテストをしてそもそもバクを含めないというのも大切だが，完璧なソフトウェアは存在しないので，アップデートは常に必要になる．
モバイルアプリとは異なり，Go言語でツールを書いきバイナリとして配布した場合は，最新のバージョンがすでに存在していることをユーザに伝える仕組みはそもそもない．ので，最新のバージョンをリリースしたことをユーザに伝えることが難しくなる．go-latestを使うと，ユーザが使っているツールが最新バージョンであるかをチェックし，古い場合にそれを伝えバージョンアップを促すということが可能になる．
インストール go getでインストールできる．
$ go get -d github.com/tcnksm/go-latest  使い方 go-latestにはSourceという概念がある．Sourceは最新バージョンの問い合わせ先である．デフォルトではGitHub上のタグ，HTMLのmetaタグ（もしくはオリジナルのスクレイピング），JSONレスポンスを利用することができる（Sourceはただのインターフェースなので自分で実装することもできる）．
これらの使い方を簡単に説明する．
GithubTag まず，GitHub上のタグを使う方法．例えば，https://github.com/tcnksm/ghrというツールにおいて，バージョン0.1.0が最新であるかをチェックするには以下のようにする．
githubTag := &amp;amp;latest.GithubTag{ Owner: &amp;quot;tcnksm&amp;quot;, Repository: &amp;quot;ghr&amp;quot;, } res, _ := latest.Check(githubTag, &amp;quot;0.1.0&amp;quot;) if res.Outdated { fmt.Printf(&amp;quot;0.1.0 is not latest, you should upgrade to %s&amp;quot;, res.Current) }  レポジトリのユーザ名とレポジトリ名を指定してCheck()を呼ぶだけ．レスポンスの詳細は，https://godoc.org/github.com/tcnksm/go-latestを参照．
HTML metaタグ 次に，特定のmetaタグをHTMLに仕込む方法．例えば，reduce-workerというツールがあるとする．この場合は，まず，以下のようなバージョン情報を含んだmetaタグをHTMLに仕込んでおく，
&amp;lt;meta name=&amp;quot;go-latest&amp;quot; content=&amp;quot;reduce-worker 0.1.2 New version include security update&amp;quot;&amp;gt;  バージョン0.1.0が最新のバージョンであるかをチェックするには以下のようにする．
html := &amp;amp;latest.</description>
    </item>
    
    <item>
      <title>デプロイ自動化とServerspec</title>
      <link>https://deeeet.com/writing/2015/03/17/serverspec-for-automation/</link>
      <pubDate>Tue, 17 Mar 2015 16:16:17 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/03/17/serverspec-for-automation/</guid>
      <description>Serverspec本の献本ありがとうございました．とても面白かったです．詳しい書評はすでに素晴らしい記事がいくつかあるので，僕は現チームでどのようにServerspecを導入したか，どのように使っているかについて書きたいと思います．
Serverspec導入の背景 今のチームではサーバーのセッアップおよびデプロイにChefを使っている．本にも書かれているようにこのような構成管理ツールを使っている場合はそのツールを信頼するべきであり，Serverspecのようなテストツールは必要ない．僕らのチームもそのような理由でServerspecの導入には至っていなかった．
しかしアプリケーションが複雑になりChefのレシピも混沌とするようになるとそれは成立しなくなる．見通しの悪いレシピはChefへの信頼度を落とす．信頼度の低下はデプロイ不信に繋がり人手（筋肉）によるテストが始まる．
サーバーの数がそこまで多くなければなんとか運用できるかもしれない．しかしサーバーの数が膨大になるとデプロイ担当者が登場し，人手によるテストに時間がかかり，本来やるべき仕事が失われる．
このような状況を解決するためにServerspecを導入した．具体的には以下を行うことを目的とした．
 Chefレシピのリファクタリング デプロイ時の人手による確認作業の自動化  1つ目に関しては現在インフラCI環境の構築中でまだ成果は出ていない（し，他に良い資料がたくさんある）．2つ目のデプロイ自動化への組み込みに関しては成果が出ているのでそちらについて工夫したことなどを簡単に書いておく．
デプロイの3ステップ デプロイは以下の3つのステップに分割できる．
 サービスアウト セットアップ サービスイン  まず，サービスアウトではサーバーをロードバランサから切り離してユーザ影響をなくし，デーモン化したジョブを停止してセットアップの準備を行う．次に，セットアップではChefなどを用いてミドルウェアのインストールやアプリケーションのアップデート，設定値の更新などを行う．最後に，サービスインでは遮断したアクセスの復帰，デーモンジョブの再開を行いサーバーを本来のあるべき状態に戻す．
うまく自動化できてない場合，ステップごとに人手のテストを行うことになる．これをなくすために各ステップで専用のServerspecを組み込むようにした．例えばサービスアウトをテストする場合は，以下のようにする．
$ rake spec:service_out:all  テストが失敗した場合は，そのステップでデプロイを停止するようにした．これによりステップが確実に成功した上で次のステップ進んでいることを保証できるし，失敗した場合は問題を切り分けられるようになった．
ディレクトリの構造は以下のようになる．
├── Rakefile └── spec ├── role1 ├── role2 ├── role3 ├── service_in │ ├── role1 │ ├── role2 │ └── role3 └── service_out ├── role1 ├── role2 └── role3  サービスが大きいとサービスインのやり方でさえも異なることがあるので，以下で説明するロールごとにディレクトリを分けるという戦略をサービスイン，サービスアウトでも使えるようなディレクトリ構造を採用した．
失敗を書き出す サーバーの数が多いとホストごとにディレクトリを準備するServerspecデフォルトのやり方では限界がある．そういう場合は，ロール毎，モジュール毎ににspecをまとめ，ホストとそのロール情報を別ファイル（JSON形式など）で準備し，それを読み込みRakeタスクを定義するのが良い．
今のチームではそもそもホストとそのロールのリストを準備しそれをもとにChefを実行するという運用があったので，そのリストをそのまま利用することにした．
さらにどのホストでSpecが失敗したかを知るのも大切である．これは以下のように新しくタスククラスを作って失敗したホスト情報をファイルに書き出すようにした．
class ServerspecTask &amp;lt; RSpec::Core::RakeTask attr_accessor :target attr_accessor :failed_list def run_task(verbose) success = system(&amp;quot;#{spec_command}&amp;quot;) save_failed_vm if not success end def save_failed_vm file = File.</description>
    </item>
    
    <item>
      <title>ACIのディスカバリーの仕様</title>
      <link>https://deeeet.com/writing/2015/03/12/appc-discover/</link>
      <pubDate>Thu, 12 Mar 2015 23:20:39 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/03/12/appc-discover/</guid>
      <description>App Container Image Discovery
&amp;ldquo;AppcとCoreOS/Rocket&amp;rdquo; に書いたようにAppc Specではインターネット上に配置したApp Container Image（ACI）のURLとそれを検証するための署名のURLをACIの名前から解決する方法も仕様として定めている．その仕様がなかなか面白いので簡単にまとめておく．
なぜ必要か Dockerではイメージの配布にDockerHubやDocker Registryを使う．使うしかない．イメージ取得をするにはRegistryと話す必要があり拡張性がない．.tar形式にして自分の好きなストレージに置くこともできるが気軽さは失われる．これらを踏まえてAppc specでは普通に使われているWebの技術/仕様（HTTPS+HTML）を用いて誰でもそれを実装できるようにしている．
ディスカバリーの仕様 ACIの名前はDNS RFC1123で受け入れられる小文字と/から構成する．
ACIの名前はURLのような構造になる．例えばexample.com/reduce-workerのようになる．しかし，これには明確なスキーマがないので直接イメージのURLを解決することができない．さらにACIをコンテナとして動かす場合は名前だけではなくバージョンやOS/アーキテクチャといった値も必要になる．Appc specはこの仕様をGo言語のRemote import pathを参考に作成している．
解決するべきURLは以下の3つである．
 イメージの場所を示すURL（.aci） 署名のURL（.asc） 公開鍵のURL  解決方法は，Simple discoveryとMeta discoveryの2つがあり，どちらもテンプレートを使ってイメージのURLを解決する．これらの2つ方法について簡単に説明する．
Simple discovery Simple discoveryは以下のテンプレートを用いる．
https://{name}-{version}-{os}-{arch}.{ext}  例えば，バージョンが1.0.0でプラットフォームがlinux/amd64であるexample.com/reduce-workerという名前のACIのURLは以下のように解決する．
$ https://example.com/reduce-worker-1.0.0-linux-amd64.aci  上記のURLによるACIの取得が失敗した場合はMeta discoveryを行う．成功した場合は，以下のURLで署名を取得する．
$ https://example.com/reduce-worker-1.0.0-linux-amd64.aci.asc  なおSimple discoveryで公開鍵を見つける方法はない．
Meta discovery Simple discoveryが失敗したらHTTPSとHTMLのmetaタグを用いてACIの名前から各種URLを解決する．
例えばexample.com/reduce-workerという名前のACIを見つける場合，まず以下のようなリクエストを送る．
https://example.com/reduce-worker?ac-discovery=1  そして，そのリクエストから得られるHTMLに以下のようなMetaタグを含ませる．
&amp;lt;meta name=&amp;quot;ac-discovery&amp;quot; content=&amp;quot;prefix-match url-tmpl&amp;quot;&amp;gt; &amp;lt;meta name=&amp;quot;ac-discovery-pubkeys&amp;quot; content=&amp;quot;prefix-match url&amp;quot;&amp;gt;  構成要素は以下のようになる．
 ac-discoveryにはACI，もしくは署名の場所を示すURLを記述する ac-disvovery-pubkeysにはACIの署名を検証するための公開鍵の場所を示すURLを記述する prefix-matchはACの名前と一致するかを確認するために利用する  例えば以下のようなmetaタグを含んだHTMLを準備する．http以外のスキーマでも良い．</description>
    </item>
    
    <item>
      <title>AppcとCoreOS/Rocket</title>
      <link>https://deeeet.com/writing/2015/03/12/rocket/</link>
      <pubDate>Thu, 12 Mar 2015 00:58:54 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/03/12/rocket/</guid>
      <description>Dockerの諸問題とRocket登場の経緯
Rocketはリリースした直後にちょっと触ってそのまま放置していた．App containerの一連のツールとRocketが現状どんな感じかをざっと触ってみる．まだまだ全然使えると思えないが今後差分だけ追えるようにしておく．
なお，今回試した一連のツールをすぐに試せるVagrantfileをつくったので触ってみたいひとはどうぞ．
https://github.com/tcnksm/vagrant-appc
概要 App Container SpecやRocketが登場の経緯は前回書いたのでここでは省略し，これらは一体何なのかを簡単に書いておく．
まず，App Container（appc）Specはコンテナで動くアプリケーションの&amp;rdquo;仕様&amp;rdquo;である．なぜ仕様が必要かというと，コンテナという概念は今まで存在したが曖昧なものだったため．namespaceやcgroupを使った..という何となくのものはあったが，統一的なものは存在しなかったため．appc specはOpenかつSecure，Composable，Simpleであることを理念に掲げて作成されている．
appcには仕様だけではなくいくつかのツールも提供されている．例えば，appcの元になるApp Container Image (ACI)の構築と検証を行うactoolや，DockerイメージからACIをつくるdocker2aci，Go言語のバイナリからACIをつくるgoaciなどがある．
では，Rocketは何かというと，そのappcを動かすruntimeの実装の1つである．つまりappcとRocketは別のものであり実装は他にも存在する．例えば，現時点ではFreeBSDのJail/ZFSとGo言語で実装されたJetpackや，C++のライブラリとしてlibappcとそれを使ったruntimeであるNose Coneなどがある．
今回はこれらのappc関連ツールとRocketを実際に触ってみる．
Appc tools まず，https://github.com/appcにあるAppcの一連のツールを触ってみる．
actoolによるイメージのbuild actoolはRootファイルシステムとjsonで既述されるmanifestファイルを基にACIをビルドするツール．ビルドだけではなく，manifestやACIが仕様通りであるかの検証を行うこともできる．
例として，以下のGo言語で書かれてサンプルWebアプリケーションを動かすためのACIを作成する．
package main import ( &amp;quot;log&amp;quot; &amp;quot;net/http&amp;quot; ) func main() http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r *http.Request) { log.Printf(&amp;quot;Request from %vn&amp;quot;, r.RemoteAddr) w.Write([]byte(&amp;quot;Hello from App Container&amp;quot;)) }) log.Fatal(http.ListenAndServe(&amp;quot;:5000&amp;quot;, nil)) }  ルートファイルシステムを準備する．
$ mkdir hello $ mkdir hello/rootfs $ mkdir hello/rootfs  サンプルアプリケーションを静的リンクでビルドする（go1.4の場合は-installsuffixが必要）．
$ CGO_ENABLED=0 GOOS=linux go build -a -tags netgo -ldflags &#39;-w&#39; -o hello-web  $ file hello-web hello-web: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, not stripped  バイナリをRootFS内に配置する．</description>
    </item>
    
    <item>
      <title>Dockerの諸問題とRocket登場の経緯</title>
      <link>https://deeeet.com/writing/2015/02/17/docker-bad-points/</link>
      <pubDate>Tue, 17 Feb 2015 23:32:16 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/02/17/docker-bad-points/</guid>
      <description>2014年の後半あたりからDocker，Docker Inc.への批判を多く見かけるようになった（もちろんもともと懸念や嫌悪を表明するひとはいた）．それを象徴する出来事としてCoreOSチームによる新しいコンテナのRuntimeであるRocketのリリースと，オープンなアプリケーションコンテナの仕様の策定を目指したApp Containerプロジェクトの開始があった．
 CoreOS is building a container runtime, Rocket  批判は，セキュリティであったり，ドキュメントされていない謎の仕様やバグだったり，コミュニティの運営だったり，と多方面にわたる．これらは具体的にどういうことなのか？なぜRocketが必要なのか？は具体的に整理されていないと思う．これらは，今後コンテナ技術を使っていく上で，オーケストレーションとかと同じくらい重要な部分だと思うので，ここで一度まとめておきたい．
なお僕自身は，コンテナ技術に初めて触れたのがDockerであり，かつ長い間Dockerに触れているので，Docker派的な思考が強いと思う．またセキュリティに関しても専門ではない．なので，以下の記事はなるべく引用を多くすることを意識した．また，あくまで僕の観測範囲であり，深追いしていないところもある，気になるひとは自分で掘ってみて欲しい．
セキュリティ問題 Dockerを使ったことがあるひとならわかると思うがDockerを使うにはルート権限が必須である．デーモンが常に動いており，それにクライアントがコマンドを発行するアーキテクチャになっているので，Dockerコンテナが動いているホストでは常にルートのプロセスが動き続けることになる．クライアントとデーモンはHTTPでやりとりするため，外部ホストからコマンドを叩くこともできてしまう．
これは怖い．コンテナはカーネルを共有しているので，もし特権昇格の脆弱性であるコンテナがハイジャックされたら，他の全てのコンテナとホストも攻撃されることになる（Container Security: Isolation Heaven or Dependency Hell | Red Hat Security）．
実際Docker 1.3.1以前のバージョンでは脆弱性も見つかっている．
 CVE-2014-6407 CVE-2014-6408  docker pullは安全なの？ 上記の脆弱性では悪意のあるイメージによる攻撃が指摘されており，攻撃を受けやすいのはdocker pullで外部からイメージを取得/展開するところである．ではここはちゃんと安全になっているのか？答えは「No」で，あまりよろしくないモデルになっており，よく批判されるところでもある．
 Docker Image Insecurity · Jonathan Rudenberg  これはFlynnの開発者が現在のdocker pullの危険性を指摘したブログ記事．要約すると，Dockerは署名されたManifestなるもので公式のDockerイメージの信頼性を確認していると主張しているがそれが全く動作していない，モデルとして危ないということを言っている．具体的には，
 イメージの検証は，[decompress] -&amp;gt; [tarsum] -&amp;gt; [unpack]処理の後に実行されるが，そもそもここに脆弱性が入り込む余地がある キーはDockerのコードには存在しておらず，イメージをダウンロードする前にCDNからHTTPSで取得するようになっており，これは&amp;hellip;  この問題を回避する方法が以下で紹介されている．
 Before you initiate a &amp;ldquo;docker pull&amp;rdquo; | Red Hat Security  ここで紹介されているのはdocker pullを使わない方法．具体的には，信頼できるサイトから，イメージの.</description>
    </item>
    
    <item>
      <title>Docker 1.5の変更点</title>
      <link>https://deeeet.com/writing/2015/02/11/docker-1_5/</link>
      <pubDate>Wed, 11 Feb 2015 22:04:50 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/02/11/docker-1_5/</guid>
      <description>Docker 1.5.0-rc1 Docker 1.5: IPv6 support, read-only containers, stats, “named Dockerfiles” and more | Docker Blog  Docker 1.5が出た．IPv6のサポートやstatsコマンドによるコンテナのメトリクス表示などが追加された．ユーザ的に一番嬉しいのはDockerfileの名前を自由に決められるようになったことだろうと思う．
今までDockerfileはDockefileという名前しか受け付けなかった，というかまともに動かなかった．やりようはあって，標準入力からぶっ込むことはできた．例えばbaseとう名前のDockerfileを作って以下のようにbuildを実行することができた．
$ docker build -t tcnksm/test - &amp;lt; base  しかし，ADDもしくはCOPYインストラクションを使っている場合に，そのソースはURLでないといけないという制限があった．ソースにローカルのファイルを指定していると，buildのコンテキストが伝わらずno such file or directoryエラーが発生するという面白い状況だった．
1.5からは-fオプションが追加され，Dockerfileという名前以外のDockerfileを指定することができるようになった．
$ docker build -t tcnksm/test -f base .  ADDやCOPYインストラクションのソースにローカルファイルを指定していてもちゃんと動く．
しかし，今までのようにカレントディレクトリのDockerfileをビルドすることに慣れていると，ハマるところがある．カレントディレクトリ以外のDockerfileをビルドするときは，そのbuildの起点となるディレクトリをちゃんと指定する必要がある．例えば，filesディレクトリ内のbaseという名前のDocekerfileをその外からビルドするときは，以下のようにする．
$ docker build -t tcnksm/test -f files/base files  末尾のfilesをいつも通りに.にするとADDやCOPYインストラクションでno such file or directoryエラーが発生する．
DockerHub ではDockerHubのAutomated buildはどうか．現時点（2015年2月11日）ではDockerfileという名前以外は受け付けていない．ので，今まで通りにディレクトリごとにDockerifileを準備する必要がある．
ただ，自分はディレクトリごとにDockerfileを分けるという慣習には適応しすぎているので，もうこのままでも良いかなって気持ちはある&amp;hellip;
その他の変更 他にもいくつか自分的に気になった機能をいくつか．
Read-onlyコンテナ runに--read-onlyオプションがつき，コンテナのファイルシステムを書き込み不可にすることができるようになった．</description>
    </item>
    
    <item>
      <title>etcd/consulに認証情報を安全に保存する</title>
      <link>https://deeeet.com/writing/2015/02/03/crypt/</link>
      <pubDate>Tue, 03 Feb 2015 23:09:51 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/02/03/crypt/</guid>
      <description>分散Key-Valueストアとしてetcdやconsulの利用が増えている．ここにアプリケーションの設定値などを保存し，各ホストからそれらを購読して利用する．
また，X-as-a-Serviceといった外部サービスの利用も多くなってきた．その場合API Tokenやパスワードといった認証情報が必要になる．PaaSやTwelve-factor的なアーキテクチャを採用する場合は，それらの値を環境変数に保存して利用することが多い（危険であるという意見はある．cf. http://techlife.cookpad.com/entry/envchain）．etcdやconsulといった分散Key-Valueストアの利用を前提としたアーキテクチャでは，そこに外部に漏らしたくない設定値も一緒に保存してしまうのがシンプルになる．
しかし，そういった設定値をプレインテキストのまま保存するのは望ましい状態ではない．暗号化して保存できるとよい．
xordataexchange/cryptを使うとetcdやconsulに暗号化して値を保存できるようになる．具体的にはGNU Privacy Guard（GnuPG）で秘密鍵と公開鍵をつくり，それらを使った値のやりとりを行う．本記事ではその使い方とCoreOSでの実例を簡単に紹介する．
インストール cryptはGoで書かれている．バイナリをダウンロードすることもできるが，go getでも良い．
$ go install github.com/xordataexchange/crypt/bin/crypt  鍵の準備 gpg2を使って秘密鍵と公開鍵のペアを生成する．以下のファイルapp.batchを作り，バッチとして生成する．
%echo Generating a configuration OpenPGP key Key-Type: default Subkey-Type: default Name-Real: app Name-Comment: app configuration key Name-Email: app@example.com Expire-Date: 0 %pubring .pubring.gpg %secring .secring.gpg %commit %echo done  $ gpg2 --batch --armor --gen-key app.batch  これで秘密鍵.secring.gpgと公開鍵.pubring.gpgが生成される．
値の保存/取得 ここでは例としてetcdを利用する．使い方は簡単でetcdctlと同じ感覚で使える．例えば，以下のようなパスワード情報config.jsonをetcdの/app/configに保存する．保存には，上記で生成した公開鍵.pubring.gpgを利用する．
$ cat &amp;lt;&amp;lt;EOF &amp;gt; config.json {&amp;quot;password&amp;quot;: &amp;quot;passw0rd&amp;quot;} EOF $ crypt set -endpoint=${ETCD_PEERS} -keyring .</description>
    </item>
    
    <item>
      <title>&#34;Orchesrating Docker&#34;という本をレビューした</title>
      <link>https://deeeet.com/writing/2015/01/26/review-orchestrating-docker/</link>
      <pubDate>Mon, 26 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2015/01/26/review-orchestrating-docker/</guid>
      <description>  Orchestrating Docker | Packt  Orchestrating Docker: Amazon.com: Books  Packt Publishingから1月22日に出版された&amp;ldquo;Orchestrating Docker&amp;rdquo;という本にレビュアーとして参加した．本の中身は，Dockerの基礎とCoreOSやdokkuといった周辺ツールをサンプルコードとともに幅広く紹介するという内容になっている．日本の技術界隈では見かけなかった話題もちょくちょく含まれていて面白い．
英語の本をレビュー依頼を受けるのはよくあるらしいが，実際にやったひとの話は見かけないので簡単にどんな感じだったかを簡単に書いておく．
経緯 9月あたりにPacktの編集者からDocker本のレビューに興味がないかとメールが届いた．日本人の自分が選ばれた理由はcodarwallへの投稿を見たからとのこと．ネイティブスピーカーではないことを伝え，それでもとの返答をもらったので，何事も経験だと思い了承した．
報酬 報酬はレビューした本とPackt内の電子書籍をどれか1つの贈呈（&amp;ldquo;Mastering Concurrency in Go&amp;rdquo;を希望した）．今回は自分の経験のためという名目があったので得に不満はない．仕事もそれに見合った働きしかしていないと思う．
レビューの流れ レビューする原稿は一度に2章づつメールで送られてくる．.doc形式で送られてくるので，MS wordもしくはGoogle docsのコメント機能を使って気になるところや，技術的におかしなところにコメントを加えていく．文法的な誤りの指摘は不要とあらかじめ言われていたので，その辺は無視する．また，自分が試したことがないサンプルがあれば，実際にコマンドを叩きながらちゃんと動くかを確認する．
毎回章ごとにアンケートが添付されているので，そこで章の総評を行う．例えば，
 この章で著者が逃しているトピックはないか？あればそれは何か？ 点数をつけるなら10点中何点か？満点にするにはどうすれば良いか？ 次章に登場するべきトピックは何か？  など，ざっくりとした内容の質問が並ぶ．
場合によっては，著者と直接やりとりをすることもあるらしいが，自分の場合はそれはなかった．レビューが終わったらコメントした原稿とアンケートをメールに添付して返信する．これを繰り返す．
英語 普段から自分が好きで追っかけてるテーマなのでそれほど大変ではなかった．著者にもよるけど，技術文章の場合は調べないといけないような複雑な文法表現は少ないし，専門分野であれば単語も分かっている．
コメントも長文を書くわけでないので，それほど苦ではなかった．伝わりにくいと思えば，参考リンクとして外部のブログを貼ってそちらに任せたりした．
良かったこと このテーマだったら自分で買って読んでいただろうし，それをいち早く読めたのは良かった．また自分ならこうやって説明するのになあという視点で読めたのは良かった．これは今後ブログを書くときなどにためになったと思う．
不満だったこと とにかく締め切りが厳しかった．こちらの都合関係なしに1章1日でやれと言われたりした．にも関わらず，原稿が送られ来るのは大幅に遅れてきたりして雑な感じだった（ポジティブに捉えるならば，本作りとはこういうものなのだなと思うことができた）．
まとめ 最後に本の総評をしておくと5.0点満点中3.5点だと思う．めっちゃ良い本！とは言えない．書籍の良さは多くの情報を集約/構造化してわかりやすく伝えてくれることだと思うが，本書は集約/構造化が弱く，散漫に感じるかもしれない．そもそもDocker界隈は流れが早いため，まだまだそれを理想的な形で実現するのは難しいので仕方のないことだとは思う．流れが早いのを分かった上で周辺技術を広く浅く伝えようという意図はあるので，その辺を掴みたいひとには良い本かと思う．
オライリーから出版される以下はどうなるのか楽しみ．
 Docker: Up and Runnin Docker Cookbook Using Docker  参考  Show HN: My first book: Orchestrating Docker  </description>
    </item>
    
    <item>
      <title>DockerHubのAutomated Buildsをフックして最新のDockerコンテナをデプロイする</title>
      <link>https://deeeet.com/writing/2015/01/08/dockerhub-hook/</link>
      <pubDate>Thu, 08 Jan 2015 20:29:53 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2015/01/08/dockerhub-hook/</guid>
      <description>DockerHubのAutomated Buildsは，GithubやBitbucketへのgit pushをフックしてレポジトリ内のDockerfileを元にDockerイメージをビルドする機能である．
イメージを使う側からすれば，それがどのようなDockfileから作られているか可視化され，常に新しいイメージがあることが保証されるので安心感がある．イメージを提供する側からすればDockerfileを更新してgit pushすれば自動でビルドしてくれくれるので楽という利点がある．そのためDockerHubにイメージを上げる場合は，docker pushを使うことはほとんどなくてこのAutomated Buildsを使うのが普通である．
このAutomated BuildsはWeb hookを提供しており，イメージのビルドが終了したら，好きなところにHTTP POSTをぶん投げるということができる．この仕組みを使えば，git pushしたら，DockerHubで最新のイメージをビルドして，終わったらそのイメージをデプロイ！といったことが可能になる．
bketelsen/captainhookを使えば，Web Hookを受け取ってコマンドを実行ということが簡単にできる（Go製なのでデプロイも楽）．これを使って，DockerHubのAutomated BuildsをフックしてDockerコンテナのデプロイする．以下ではCoreOS以外のホストOS（シングルホスト）の場合とCoreOSの場合をそれぞれ説明する．
Captainhookとは bketelsen/captainhookについて簡単に説明する．captainhookはHTTP POSTを受け取って設定したコマンドを実行するツールである．
まずconfigdirを作成し，その中にJSONで設定ファイルを準備する．例えばendpoint.jsonというファイルを作る．
$ mkdir ~/captainhook  { &amp;quot;scripts&amp;quot;: [ { &amp;quot;command&amp;quot;: &amp;quot;echo&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;hello&amp;quot; ] } ] }  以下のようにサービスを起動する．
$ captainhook -configdir ~/captainhook  あとは，以下のようにHTTP POSTを投げると，JSONで指定したコマンドが実行される．シンプルで良い．
$ curl -X POST http://localhost:8080/endpoint1  CoreOS以外の場合 まず，CoreOS以外のホストOS（シングルホスト）でAutomated Buildsをフックして最新のDockerコンテナをデプロイする方法について説明する．これは，Gopher Academyのブログのデプロイ方法が参考になる．まさにこの方法で最新のブログ記事をデプロイしている．以下のブログに詳しく説明されている．
 Easy Docker Deployment with Hooks and Captain Hook  Gopher Academyのブログはbketelsen/gopheracademy-webとうDockerイメージとして動いている．Automated Buildsでビルドされており，git pushされる度に最新のイメージが作られる．Captainhookには以下のようなスクリプトを登録している．</description>
    </item>
    
    <item>
      <title>TerraformでCoreOSクラスタを構築する</title>
      <link>https://deeeet.com/writing/2015/01/07/terraform-coreos/</link>
      <pubDate>Wed, 07 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2015/01/07/terraform-coreos/</guid>
      <description>CoreOSはDigitalOceanやAmazon EC2，OpenStackなどあらゆるクラウドサービスやプラットフォームで動かすことができる．1つのCoreOSクラスタを複数のクラウドサービスや自社のベアメタルサーバーにまたがって構築することもできるし，それが奨励されている．また，クラスタのマシンの数はサービスの成長や負荷状況によって増減させる必要もある．
このようなCoreOSクラスタの構築を簡単に，かつInfrastructure as Code的に再現可能な形で行いたい場合，HashicorpのTerraformを使うのがよさそう（個人的に試しているだけなので数百規模のマシンではなく，数十規模の話．もし膨大なマシン数になったときにどうするのがよいのか知見があれば知りたい）．
以下では，Terraformを使ってDigitalOcean上にCoreOSクラスタを構築する方法について書く．コードは全て以下のレポジトリにある．
 tcnksm/deeeet.com/terraform  CoreOSの設定ファイル まず，CoreOSの設定ファイルであるcloud-configを準備する．
#cloud-config coreos: etcd: discovery: https://discovery.etcd.io/XXXX addr: $private_ipv4:4001 peer-addr: $private_ipv4:7001 fleet: public-ip: $private_ipv4 metadata: role=lb,provider=digitalocean units: - name: etcd.service command: start - name: fleet.service command: start update: group: alpha reboot-strategy: best-effort  cloud-configの設定項目についてはweb上に良い記事がたくさんあるのでここでは詳しくは書かない．ただしfleet.metadataにroleやproviderといった値を書いておくと，fleetでスケジューリングを行うときにより柔軟な設定ができるようになるので，状況に合わせて既述しておくとよい．
Terraformの設定ファイル 次にTerraformの設定ファイルである.tfファイルを準備する．ここでは以下の2つのファイルを準備する．
 variables.tf - 外部から与える設定値を定義する main.tf - ProviderとResourceを定義する  （Terraformは実行時にカレントディレクトリの.tfファイルを全て読み込むのでファイルの分割は自由にやってよい．すべてを1つの.tfファイルに書いてしまうことも可能．ただし，Providerやマシンの数が増えると管理がキツくなるので適宜分けるのがよい）
variable.tf まず，外部から与える設定値をvariables.tfに定義する．ここには，例えばDigitalOceanのAPI Tokenのような設定ファイルには直接書きたくない設定値を定義し，コマンド引数-varでそれを受け取れるようにする．
variable &amp;quot;digitalocean_token&amp;quot; { description = &amp;quot;DigitalOcean API token&amp;quot; } variable &amp;quot;ssh_key_id&amp;quot; { description = &amp;quot;ID of the SSH key to use DigitalOcean&amp;quot; }  ssh_key_idはDigitalOceanに登録してあるSSH KeyのIDで以下で取得できる．</description>
    </item>
    
    <item>
      <title>Best Music 2014</title>
      <link>https://deeeet.com/writing/2014/12/31/music-2014/</link>
      <pubDate>Wed, 31 Dec 2014 13:09:00 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/31/music-2014/</guid>
      <description> 今年は例年と比べてあまり多く聴けなかったと思う．その中でも良かったものをピックアップした．
 Millie &amp;amp; Andrea &amp;ldquo;Drop The Vowels&amp;rdquo; Tessela &amp;ldquo;Nancy&amp;rsquo;s Pantry&amp;rdquo; Aphex twin &amp;ldquo;Syro&amp;rdquo; Flying Lotus &amp;ldquo;You&amp;rsquo;re Dead!&amp;rdquo; Shackleton &amp;ldquo;Freezing Opening Thawing&amp;rdquo; Jon Hopkins &amp;ldquo;Asleep Versions&amp;rdquo; Clap! Clap! &amp;ldquo;Tayi Bebba&amp;rdquo; Caribou &amp;ldquo;Our Love&amp;rdquo; Lone &amp;ldquo;Reality Testing&amp;rdquo; patten &amp;ldquo;ESTOILE NAIANT&amp;rdquo;  参考  Best Music 2013 | SOTA Best Music 2012 | SOTA  </description>
    </item>
    
    <item>
      <title>OctopressからHugoへ移行した</title>
      <link>https://deeeet.com/writing/2014/12/25/hugo/</link>
      <pubDate>Thu, 25 Dec 2014 01:34:57 +0900</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/25/hugo/</guid>
      <description>このブログは2年ほどOctopressを使って生成してきたが，不満が限界に達したので，Go言語で作られたHugoに移行した．
Octopressへの不満は，とにかく生成が遅いこと．100記事を超えた辺から耐えられない遅さになり，最終的には約150記事の生成に40秒もかかっていた．ブログは頻繁に書くのでかなりストレスになっていた．
Hugoのうりは生成速度．試しに使ったところ，明らかに速く，すぐに移行を決めた．最終的な生成時間は以下．爆速．
$ time hugo hugo 0.30s user 0.06s system 296% cpu 0.121 total  他に良いところを挙げると，まずとてもシンプル．Octopressと比べても圧倒的に必要なファイルは少ない．また，後発だけあって嬉しい機能もいくつかある．例えば，draftタグを記事のヘッダに書いておけば，ローカルでは生成されても，本番用の生成からは外されるなどなど．
インストール Go言語で書かれているのでgo getして，デザインテーマをCloneするだけで動かせる．バイナリも配布されてるので，Go言語の環境がなくても使える（この楽さもRuby製のOctopressと比べて良い）．
$ go get -v github.com/spf13/hugo  $ git clone --recursive https://github.com/spf13/hugoThemes themes  使いかたは公式に十分なドキュメントがある．
移行方法 Octopressからの移行はとても簡単だった．source/_posts内の記事を移せばとりあえず動く．以下ではこれ以外の移行作業を簡単にまとめておく．
まず，設定ファイルは，yamlもしくは，toml，json形式で書く．ブログの移行でめんどくさいのはURLの維持だが，Octopressと同様にpermalinkを設定できる．例えば，tomlを使う場合は以下のように書く．
[permalinks] post = &amp;quot;/:year/:month/:day/:filename/&amp;quot;  次に記事のヘッダ．OctopressとHugoでは日付フォーマットが若干異なる．Octopressの場合は，2014-12-25 01:34でHugoの場合は2014-12-25T01:34:57となる．これでも動くが，うまくパースされない．とりあえず，時刻を消せばちゃんとパースされるので，以下のようなワンライナーを書く．
$ find . -type f -exec sed -i &amp;quot;&amp;quot; -e &#39;s/date: \([0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\).*$/date: \1/g&#39; {} \;  最後にOctopressのタグ（e.g., {% img ...%}）．OctopressのタグはHugoでは使えない．これもワンライナーを使ってHTMLタグに変換する．例えば，イメージタグを変換は以下のようにした．
$ find . -type f -exec sed -i &amp;quot;&amp;quot; -e &#39;s/{%.</description>
    </item>
    
    <item>
      <title>Go言語でテストしやすいコマンドラインツールをつくる</title>
      <link>https://deeeet.com/writing/2014/12/18/golang-cli-test/</link>
      <pubDate>Thu, 18 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/18/golang-cli-test/</guid>
      <description>本記事はGo Advent Calendar 2014の18日目の記事です．
Go言語は，クロスコンパイルや配布のしやすさからコマンドラインツールの作成に採用されることが多い．自分もGo言語でいくつかのコマンドラインツールを作成してきた．例えば，GitHub Releaseへのツールのアップロードを簡単に行うghrというコマンドラインツールを開発をしている．
コマンドラインツールをつくるときもテストは重要である．Go言語では標準テストパッケージだけで十分なテストを書くことができる．しかし，コマンドラインツールは標準出力や標準入力といったI/O処理が多く発生する．そのテスト，例えばある引数を受けたらこの出力を返し，この終了ステータスで終了するといったテストは，ちゃんとした手法が確立されているわけではなく，迷うことが多い（少なくとも自分は結構悩んだ）．
本記事では，いくつかのOSSツール（得にhashicorp/atlas-upload-cli）を参考に，Go言語によるコマンドラインツールおいてI/O処理に関するテストを書きやすくし，すっきりとしたコードを既述する方法について解説する．
なお，特別なパッケージは使用せず，標準パッケージのみを利用する．
TL;DR io.Writerを入力とするメソッドをつくり，そこに実処理を書く．main関数やテストからはio.Writerを書き換えて，それを呼び出すようにする（文脈によりioの向き先を変える）．
実例 ここでは，簡単な例としてawesome-cliというコマンドラインツールを作成し，その出力結果と終了コードのテストを書く．
awesome-cliは-versionオプションを与えると，以下のような出力と，終了コードが得られるとする．
$ awesome-cli -version awesome-cli version v0.1.0  $ echo $? 0  以下では，この挙動のテストをどのように書くかを，awesome-cliのコードそのものと共に解説する．
コード awesome-cliは以下の2つのソースで構成する．
 cli.go - オプション引数処理を含めた具体的な処理 main.go - main関数  そしてcli_test.goにI/Oに関わるテスト，ここでは-versionオプション引数を与えたときの出力とその終了コードのテスト，を既述する．以下ではこれらを具体的に説明する．
cli.go まず，引数処理を含めた具体的な処理を行うcli.goは以下のように既述する．引数処理には標準のflagパッケージを利用する．
package main import ( &amp;quot;flag&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;io&amp;quot; ) // 終了コード const ( ExitCodeOK = 0 ExitCodeParseFlagError = 1 ) type CLI struct outStream, errStream io.Writer } // 引数処理を含めた具体的な処理 func (c *CLI) Run(args []string) int { // オプション引数のパース var version bool flags := flag.</description>
    </item>
    
    <item>
      <title>Dockerコンテナ接続パターン (2014年冬)</title>
      <link>https://deeeet.com/writing/2014/12/01/docker-link-pattern/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/01/docker-link-pattern/</guid>
      <description>本記事はDocker Advent Calendar 2014の1日目の記事です．
Dockerによるコンテナ化はリソース隔離として素晴らしい技術である．しかし，通常は1つのコンテナに全ての機能を詰め込むようなことはしない．マイクロサービス的にコンテナごとに役割を分け，それらを接続し，協調させ，全体として1つのサービスを作り上げるのが通常の使い方になっている．
コンテナ同士の接続と言っても，シングルホスト内ではどうするのか，マルチホストになったときにどうするのかなど様々なパターンが考えられる．Dockerが注目された2014年だけでも，とても多くの手法や考え方が登場している．
僕の観測範囲で全てを追いきれているかは分からないが，現状見られるDockerコンテナの接続パターンを実例と共にまとめておく．
なお今回利用するコードは全て以下のレポジトリをcloneして自分で試せるようになっている．
 tcnksm/docker-link-pattern  概要 本記事では以下について説明する．
 link機能（シングルホスト） fig（シングルホスト） Ambassadorパターン（マルチホスト） 動的Ambassadorパターン（マルチホスト） weaveによる独自ネットワークの構築（マルチホスト） Kubernetes（マルチホスト）  事前知識 事前知識として，Dockerがそのネットワークをどのように制御しているかを知っていると良い．それに関しては以下で書いた．
 Dockerのネットワークの基礎 | SOTA  利用する状況 以下ではすべてのパターンを，同じ状況で説明する．redisコンテナ（crosbymichael/redis）を立て．それにresdis-cliコンテナ（relateiq/redis-cli）で接続するという状況を考える．
link機能（シングルホスト） まず，基礎．DockerはLinksというコンテナ同士の連携を簡単に行う仕組みを標準でもっている．これは，--link &amp;lt;連携したいコンテナ名&amp;gt;:&amp;lt;エイリアス名&amp;gt;オプションで新しいコンテナを起動すると，そのコンテナ内で連携したいコンテナのポート番号やIPを環境変数として利用できるという機能である．
今回の例でいうと，まず，redisという名前でredisコンテナを立てておく．
$ docker run -d --name redis crosbymichael/redis  これに接続するには，以下のようにする．
$ docker run -it --rm --link redis:redis relateiq/redis-cli redis 172.17.0.42:6379&amp;gt; ping PONG  relateiq/redis-cliコンテナの起動スクリプトは以下のようになっている．
# !/bin/bash if [ $# -eq 0 ]; then /redis/src/redis-cli -h $REDIS_PORT_6379_TCP_ADDR -p $REDIS_PORT_6379_TCP_PORT else /redis/src/redis-cli &amp;quot;$@&amp;quot; fi  引数なしで起動すると，relateiq/redis-cliは環境変数， $REDIS_PORT_6379_TCP_ADDRに接続しようとする．--link redis:redisでこれを起動することで，この環境変数が設定され，接続できる．</description>
    </item>
    
    <item>
      <title>Go Conference 2014 Autumnの手伝いをした</title>
      <link>https://deeeet.com/writing/2014/12/01/go-conference-2014-autumn/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/12/01/go-conference-2014-autumn/</guid>
      <description> Go Conference 2014 autumn - connpass
自分の所属チームをはじめ弊社でもGolangの導入は始まっているため，合コンの会場の提供及び，運営の手伝いをした．会議の内容については他に良いまとめ記事があるので，そちらに任せ，普段あまり語られない会場について軽く書いておく．
今回やってみてこういう大規模なカンファレンスを余裕でやる設備あるなと思った．運営とかの&amp;rdquo;複雑さを隠蔽して&amp;rdquo;良いところを挙げると，
 500人以上は余裕で入れるキャパシティがある プロジェクターが全面に配置されている（どの席からでもスライドちゃんと見える） Wifiがめちゃしっかりしてる（普段から何千人が普通に使えてる） 音響もめちゃしっかりしてる  しかも設備は，タッチパネルで余裕の操作ができる．毎週全世界の支社を含めた，全社員が参加する会をやってるくらいなので，それに耐えうる設備がある．
逆にしんどい部分を挙げると，
 セキュリティが厳重（当たり前だけど柔軟さとのトレードオフ） パイプ椅子なのでケツが死ぬ 電源不足（でもこれは僕の怠惰による準備不足） 会場の自販機がEdyしか使えない  今回他の会場を探す機会があったが，費用を考えた場合に，300人以上の会場はなかなかない．今後，大規模なカンファレンスをやる機会があれば，少し考慮に入れてもらっても良いかもしれない．僕はしばらくやりたくないけど，社員に知り合いがいればなんとかなるかもしれません．
謝辞 @hyoshiokさんの協力がなければ，何もできませんでした．ありがとうございました．あと会場の関係上，当日は社員スタッフに手伝っていただきました．本当にありがとうございました！
主催の@tenntennさん，@jxck_さん，@ymotongpooさん，めちゃおもろいカンファレンスを開いて頂いてありがとうございました！またスタッフの方々もお疲れ様でした！
あと運営の手伝いをして，普段自分が気軽に参加している勉強会やカンファレンスのありがたさを実感した．
次回は発表枠で参加したい．
参考  Go Conference 2014 autumn を終えて #gocon 私のGopherコレクション2014 #golang  </description>
    </item>
    
    <item>
      <title>CoreOSクラスタ内のDockerコンテナの動的リンク</title>
      <link>https://deeeet.com/writing/2014/11/26/coreos-etcd-docker-link/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/11/26/coreos-etcd-docker-link/</guid>
      <description>Dynamic Docker links with an ambassador powered by etcd
上記の記事を参考にCoreOSのクラスタ内で複数ホスト間にまたがりDockerコンテナを連携させる方法について検証した．
背景と問題 複数ホストにまたがりDockerのコンテナを接続する方法としてはAmbassador パターンが有名である．これはトラフィックを別ホストへforwardすることに特化したコンテナを立てる方法で，ホストに無駄な設定なし，かつDockerコンテナのみで行えるシンプルな方法である．例えば，あるホストからredis-cliを使って，別ホストで動くredisに接続する場合は以下のように接続する．
(redis-cli) --&amp;gt; (ambassador) ---network---&amp;gt; (ambassador) --&amp;gt; (redis)  redis-cliコンテナとambassadorコンテナ，redisコンテナとambassadorコンテナはdockerのlink機能で接続し，ambassadorコンテナはトラフィックをネットワーク越しにフォワードする．
この方法は，接続側がその相手先のホストを知っている必要がある．例えば上記の場合，redis-cliコンテナ側のambassadorコンテナは以下のように相手先のホストのIP（e.g., 192.168.1.52）を指定して起動しなければならない．
$ docker run -d --name ambassador --expose 6379 -e REDIS_PORT_6379_TCP=tcp://192.168.1.52:6379 svendowideit/ambassador  ホストが固定されている場合は問題ないが，CoreOSのように動的にホストが変わる可能性がある場合は問題になる．接続先のホスト情報を直接既述すると，ホストが変わる度に設定を更新する必要があり，かなり億劫な感じになる．
CoreOSにおける1つの解法 CoreOSはクラスタの形成に分散Key-Valueストアであるetcdを使っている．このetcdを使うと動的なambassadorパターンを作り上げることができる．つまり，以下のようなことをする．
 接続される側は接続情報をetcdに書き込み続けるコンテナを立てる 接続する側はその情報を読み込み続ける動的なambassadorコンテナを立てる  あとは，この動的なambassadorコンテナとlink接続すれば，相手先の情報を環境変数として取得するとができる．これで接続する側は接続相手のホスト情報を知らなくてもよくなる．
検証ストーリー これを実際にCoreOSクラスタを立てて検証してみる．
ここでは，Docker公式のドキュメント&amp;ldquo;Link via an Ambassador Container&amp;rdquo;と同様の例を用いる．クラスタ内のあるホストよりredis-cliコンテナを使って，別ホストのredisコンテナに接続するという状況を考える．
CoreOSクラスタを立てる 利用するCoreOSクラスタはtcnksm/vagrant-digitalocean-coreosを使って，VagrantでDigitalOcean上に立てる．
$ export NUM_INSTANCES=3 $ vagrant up --provider=digital_ocean  これで，DigitalOcean上に3つのCoreOSインスタンスが立ち上がる．
利用するコンテナ 全部で5つのコンテナを用いる．
https://coreos.com/assets/images/media/etcd-ambassador-hosts.png
HostA（redisを動かすホスト）では以下のコンテナを立てる．
 crosbymichael/redis - Ambassador パターンで使われているものと同様のRedisコンテナ polvi/simple-amb - socatコマンドを使って，特定のポートへのトラフィックを与えられたホストにforwardするだけのコンテナ．etcdへのフォーワードに利用する． polvi/docker-register - docker portコマンドを使って与えられたDockerコンテナのIPとPortを取得し，etcdにそれを登録するコンテナ  HostB（redis-cliを動かすホスト）では以下のコンテナを立てる．</description>
    </item>
    
    <item>
      <title>Fleetの使い方，Unitファイルの書き方</title>
      <link>https://deeeet.com/writing/2014/11/20/fleet/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/11/20/fleet/</guid>
      <description>CoreOSに入門した | SOTA
CoreOSではすべてのアプリケーションをDockerで動かす．このとき，コンテナによるサービスをCoreOSクラスタのどのマシンで起動するかをいちいち人手で決めるわけにはいけない．クラスタ内のリソースの状態や動いているサービスに基づき，適切なマシンでコンテナを動かすスケジューリングの仕組みが必要になる．
このスケジューリングとコンテナの管理にCoreOSはfleetを用いる． fleetを使うとCoreOSクラスタが1つのinit systemで動いているかのようにそれを扱うことができるようになる．開発者はどのマシンでどのDockerコンテナが動いているかを気にする必要がなくなる．
例えば，5つのコンテナを動かす必要があれば，fleetはクラスタのどこかでその5つのコンテナが動いてることを保証する．もしコンテナが動いているマシンに障害があっても，fleetはそのコンテナを別のマシンにスケジューリングしなおす（フェイルオーバー）．
スケジューリングは柔軟で，マシンのRegionやRoleによって振り分けることもできるし，同じサービスを同じマシンでは動かさないようにするといった設定もできる．例えば，複数のDBコンテナを別々のマシンに分散させるといったこともできる．
DigitalOceanの&amp;ldquo;Getting Started with CoreOS&amp;rdquo;シリーズの
 How To Use Fleet and Fleetctl to Manage your CoreOS Cluster How to Create Flexible Services for a CoreOS Cluster with Fleet Unit Files  において，fleetを操作するためのfleettclコマンドの使い方と，その設定ファイルであるUnitファイルの書き方を良い感じに解説していたので，それらを参考にfleetの使い方をまとめておく．
まずfleetの技術的概要をまとめる，次にfleetctlコマンドによるサービスの管理方法を書く．最後にUnitファイルの書き方について説明する．
fleetの技術的概要 fleetはクラスタレベルのsystemdと捉えることができる（単一マシンのinit systemがsystemdで，クラスタのinit systemがfleet）．
https://coreos.com/assets/images/media/fleet-schedule-diagram.png
fleetはengineとagentという大きく2つのコンポーネントから構成される．engineはジョブスケジューリングとクラスタサイズの変更を管理する．agentはマシンの代わりにジョブを引き受ける．Unitがクラスタに割り当てられると，agentはUnitファイルを読み込み，それを開始する．そして，systemdの状態をfleetに通知する．
バックエンドではetcdクラスタが動いており，engineとagentの協調に使われる．
fleetctlによるサービスの管理 fleetの設定ファイルは，systemdのunitファイルにfleet特有の設定（e.g., クラスタ内での分散方法など）を加えたものを利用する． このファイルの詳細は後述するとして，ここでは以下のようなHello Worldを出力しつづけるhello.serviceを利用する．
[Unit] Description=My Service After=docker.service [Service] TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill hello ExecStartPre=-/usr/bin/docker rm hello ExecStartPre=/usr/bin/docker pull busybox ExecStart=/usr/bin/docker run --name hello busybox /bin/sh -c &amp;quot;while true; do echo Hello World; sleep 1; done&amp;quot; ExecStop=/usr/bin/docker stop hello  fleetによるサービス管理はfleetctlコマンドを使って行う．サービスの起動は以下の流れで行われる．</description>
    </item>
    
    <item>
      <title>CoreOSに入門した</title>
      <link>https://deeeet.com/writing/2014/11/17/coreos/</link>
      <pubDate>Mon, 17 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/11/17/coreos/</guid>
      <description>CoreOS is Linux for Massive Server Deployments · CoreOS
CoreOS + Docker Meetup Tokyo #1に参加してCoreOSにめっちゃ感動したので，CoreOSに入門していろいろ触ってみた．
まず，CoreOSの概要とそれを支える技術について説明する．次に実際にDigitalOcenan上にVagrantを使って実際にCoreOSクラスタを立てて，CoreOSで遊ぶ方法について書く．
CoreOSとは何か CoreOSは，GoogleやFacebook，Twitterといった企業が実現している柔軟かつスケーラブル，耐障害性の高いインフラの構築を目的としたLinuxディストリビューションである．軽量かつ使い捨てを前提にしており，クラウドなアーキテクチャのベストプラクティスを取り入れている．CoreOSの特徴は大きく4つ挙げられる．
 ミニマルなデザイン 容易かつ安全なOSアップデート Dockerコンテナによるアプリケーションの起動 クラスタリング  CoreOSはとてもミニマルである．従来のLinuxディストリビューションが機能を追加することでその価値を高めていったのに対して，CoreOSは必要最低限まで機能を削ぎ落としていることに価値がある（&amp;ldquo;CoreOS の調査：足し算から引き算へと，Linux ディストリビューションを再編する&amp;rdquo;）．
CoreOSは安全かつ容易なOSアップデート機構を持っている．これにはOmahaというChromeOSやChromeの更新に利用されているUpdate Engineを使っており，RootFSを丸ごと入れ替えることでアップデートを行う．これによりShellShockのような脆弱性が発見されても，いちいちパッチを当てるといったことやらずに済む．
CoreOSは専用のパッケージマネージャーをもたない．またRubyやPythonといった言語のRuntimeも持たない．全てのアプリケーションをDockerコンテナとして動作させる．これによりプロセスの隔離と，安全なマシンリソースの共有，アプリケーションのポータビリティという恩恵を受けることができる．
CoreOSはクラスタリングの機構を標準で持っている．クラスタリングについては，先週来日していたCoreOSのKelsey氏は&amp;rdquo;Datacenter as a Computer&amp;rdquo;という言葉を使っていた．データセンターの大量のサーバー群からクラスタを構築してまるでそれが1つのコンピュータとして扱えるようにすることをゴールとしているといった説明をしていた．
CoreOSはクラウドネイティブなOSである．Amazon EC2，DigitalOcean，Rackspace，OpenStack，QEMU/KVMといったあらゆるプラットフォームが対応を始めている．1つのクラスタを異なる2つのクラウドサーバにまたがって構築することもできるし，クラウドと自社のベアメタルサーバーを使って構築することもできる．
CoreOSの特徴については，@mopemopeさんの &amp;ldquo;CoreOS入門 - Qiita&amp;rdquo;や，@yungsangさんの&amp;ldquo;CoreOS とその関連技術に関するここ半年間の私の活動まとめ&amp;rdquo;が詳しい．
CoreOSを支える技術 CoreOSを支える技術キーワードを挙げるとすれば以下の3つになる．
 Docker etcd fleet  これらについてざっと説明する．
Docker CoreOSは専用のパッケージマネージャーをもたない．またRubyやPythonといった言語のRuntimeも持たない．全てのアプリケーションをDockerコンテナとして動作させる．
https://coreos.com/assets/images/media/Host-Diagram.png
Dockerを使うことで上図のようにコンテナによるプロセスの隔離と，安全なマシンリソースの共有，アプリケーションのポータビリティという恩恵を受けることができる．
etcd CoreOSは複数のマシンからクラスタを形成する．クラスタを形成するために，CoreOSはetcdという分散Key-Valuesストアを使い，各種設定をノード間で共有する（etcdってのは&amp;rdquo;/etc distributed&amp;rdquo;という意味）．
https://coreos.com/assets/images/media/Three-Tier-Webapp.png
etcdはクラスタのサービスディスカバリーとしても利用される．クラスタのメンバーの状態などを共有し，共有情報に基づき動的にアプリケーションの設定を行う．これらを行うetcdのコアはRaftのコンセンサスアルゴリズムである．Raftについては，&amp;ldquo;Raft - The Secret Lives of Data&amp;rdquo;を見るとビジュアルにその動作を見ることができる．
etcdはlocksmithというクラスタの再起動時のリブートマネジャーにも使われている．
fleet コンテナによるサービスをクラスタ内のどのマシンで起動するかをいちいち人手で決めるわけにはいけない．クラスタ内のリソースの状態や動いているサービスに基づき，適切なマシンでコンテナを動かすスケジューリングの仕組みが必要になる．
このスケジューリングとコンテナの管理にCoreOSはfleetを用いる．fleetはクラスタ全体のinit systemとして，クラスタのプロセス管理を行う．fleetはこれを各マシンのsystemdを束ねることでこれを実現している．fleetで管理するサービスはsystemdのUnitファイルを改良したものを用いる．</description>
    </item>
    
    <item>
      <title>PaaSエンジニアになった</title>
      <link>https://deeeet.com/writing/2014/11/14/work-as-paas-engineer/</link>
      <pubDate>Fri, 14 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/11/14/work-as-paas-engineer/</guid>
      <description>今まではモバイルアプリ向けのAPIの開発に携わりつつ，CIやデプロイ自動化といったDeveloper Productivity的なことをメインとしていたが，PaaSチームにジョインした．後に自分がどういうことを考えて舵を取ったかを見返すために簡単に今思っていることを綴ってみる．
PaaSという選択 &amp;ldquo;実践Heroku入門&amp;rdquo;の&amp;rsquo;はじめに&amp;rsquo;にしつこく書かれているようにPaaSの大きな目標は「アプリケーション開発者の効率を最大化」することにある．もともとDeveloper Productivity的なことが好きでいろいろやってきたが，その究極的な形がPaaSではないかと思う．
PaaSといってもプライベートPaaSだが，素晴らしいアイディアがあり，それを簡単にリリースでき，かつスケールもできる，そういうプラットフォームを社内にもっているのは大きな強みになると思う．どうすれば開発者にとって使いやすいプラットフォームになるのか，それがいかにビジネスとしてうまくスケールできるのか，といったことをどんどん突き詰めていきたい．
PaaSへの興味は，FlynnやDeisのコミュニティやその成長を見てきたことも大きい．この辺の技術の動きは本当に面白い．
技術的な興味 自分が関わり始めたPaaSは結構なサービスを動かしつつも，パフォーマンスや運用上の問題を抱えている．それらを解決するために，Dockerや各種DevOpsツール，そして言語としてGolangに舵をとろうとしてる．
昨年あたりから個人的に興味をもってDocker等のツールやGolangを触ってきたが，個人レベルと実際の運用ではかなりの乖離がある．気に入った技術をガチな環境で試していける機会はなかなかない．実運用でこそ，大変さがわかるし，本当の良さがわかると思う．
PaaSというと今はポリグロット（他言語）対応が当たり前だが，それを運用するにはその対応しているプログラミング言語に精通する必要がある．自分のメイン言語以外も，ちゃんと運用できるくらいの知識はもっていたいので，その辺も魅力に感じている．
また，自分は開発の知識はあっても運用の経験は全くない．現在のチームは開発だけでなく，数千台規模のサーバの運用も自分たちでこなしているため，しっかりした運用の知識をつけることもできる．言葉としてはなく実感として．
まだチームに入って2週間程度だが，毎日知らないことが入ってくるのでとても新鮮！
環境 少数精鋭かつ，フラット，スクラム体制などが良い点として挙げられるが，一番気に入っているのが，OSSにどんどんコミットしていこうという姿勢．良いツールができれば公開するし，使っているツールに問題があればどんどんプルリクエストを送る．仕事としてそれが奨励されている．
また，日本人がほとんどいない．完全に英語．日本という住みやすい場所で英語使って働けるのはとても良いと思っている（いろいろ意見はあると思うけど）．
まとめ こういう機会が得られたのもブログ書いたり，外で発表してた結果だと思う．巡り巡って現在のチームのひとに知られることになった．こういうことを良くいろんな人が言ってて本当かよ！と思ってたけど実際本当だった．これからもどんどん続けていきたい．</description>
    </item>
    
    <item>
      <title>DockerHub公式の言語StackをCentOSに移植した</title>
      <link>https://deeeet.com/writing/2014/11/04/dockerfile-centos/</link>
      <pubDate>Tue, 04 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/11/04/dockerfile-centos/</guid>
      <description>DockerHub公式の言語Stackが出て非常に便利になった．が，これらは全てdebianベースである．やんごとなき理由でCentOSを使わざるを得ないこともあるので，公式言語Stackの一部をCentOSに移植した．
とりあえず，ruby，rails，perl，node，javaを作成した．すべて公式の言語Stackをフォークして作成しているので，公式と同様の使い方ができる．また全てAutomated Buildしているので，DockerHubからインストールしてすぐに使える．
上の全てのイメージは，HerokuのStack的なイメージであるtcnksm/dockerfile-centos-buildpack-depsをベースにしている．もし他の言語のイメージを作成したい場合も，これをベースにすることができる．
ただ，どうしもイメージの容量は大きくなってしまった．その辺は注意してください&amp;hellip; またCentOSは慣れてないのでおかしなところがあればIssueかtwitterで指摘してください．</description>
    </item>
    
    <item>
      <title>複数プロジェクトを抱えるチームでのデプロイ自動化</title>
      <link>https://deeeet.com/writing/2014/10/30/fabric/</link>
      <pubDate>Thu, 30 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/10/30/fabric/</guid>
      <description>1つのチームで，10以上のプロジェクト，コードベースを抱える場合にどのようにデプロイの自動化を進めたか，工夫したこと，考慮したことなどをまとめておく．
デプロイツールには，Python製のfabricを採用しているが，他のツールでも同様のことはできそう．なお，fabricの基本的な使い方などは既にインターネット上に良い記事がたくさんあるので書かない（最後の参考の項を見てください）．
fabricの選択 シェルスクリプトとCapistranoを考慮した．
まず，シェルスクリプトは人によって書き方が違うため，統一が難しくメンテナンスコストも高い．また共通化も難しい．
次に，Capistranoは，裏でやってくれることが多く，学習コストも高い．プロジェクトによってはかなり特殊な環境へのデプロイも抱えているため，Capistranoの前提から外れることがあった．また，自分独りなら問題ないが，チームの負担が大きくなると感じた．
fabricは全てを自分で記述する必要がある．それは欠点である一方利点でもある．例えば，特殊な要件であっても柔軟に対応することができる．また，最初に自分が頑張れば，既述の統一や共通化により最小のフレームをつくり，チームメンバーへの負担も少なくできると感じた．
このような理由からfabricを採用した．
戦略 デプロイスクリプト（ここではfabfile.py）は以下のように1つの専用のレポジトリ内にプロジェクトごとに作成した．理由としては，プロジェクトの数が多い場合，コードベースごとにデプロイスクリプトを抱えるのは，自動化の推進の弊害になるため．デプロイスクリプトをバラバラに管理すると，プロジェクトごとに独自文化が生まれ，統一を失うと感じたため．また，後述する共通化が困難になるため．
. ├── README.md ├── projectA │ ├── README.md │ └── fabfile.py ├── projectB │ ├── README.md │ └── fabfile.py ├── projectC │ ├── README.md │ └── fabfile.py . .  デプロイは開発者の手元からではなく，デプロイサーバから行う．これによりワークフローもシンプルになる．デプロイ時には専用のサーバにログインし，プロジェクトのディレクトリに移動して，fabコマンドを叩けば良い．またそのプロジェクトに入っていないひとでもデプロイを行うこともできる（ChatOpsの実現も容易）．
README.mdも重要である．プロジェクトごとに考慮するべきことは全てREADME.mdを見れば良いようにした（参考，&amp;ldquo;わかりやすいREADME.mdを書く&amp;rdquo;）．
共通化 プロジェクトは違っても，例えば，チャットツールへの通知やコードのチェックアウト，ビルドは共通化できる．共通化しておくことで，新しいプロジェクトの自動化の展開が容易になる．そのプロジェクト特有なものだけ既述して，あとは共通化したものを呼び出すだけでよくなる．
以下のように，レポジトリのトップレベルにcommonディレクトリを作成し，共通作業をまとめるようにする．
common ├── __init__ ├── git.py ├── hipchat.py ├── build.py . .  使うときは，各プロジェクトのfabfile.pyからそれを呼び出すようにする．
sys.path.append(os.pardir) from common import git,hipchat,java  用語の統一 複数のプロジェクトを扱う場合，用語の統一は，属人性を廃して複数の開発者がどのプロジェクトでもデプロイできるようにするために非常に重要である．用語とは，例えば，fabコマンドと共に叩く関数名のことである．せっかく自動化しても，プロジェクトごとに様々なコマンドが存在したり，叩くべきコマンドが複雑になっては意味がない．
例えば，リリースは全て以下のように統一している（stg環境にfeature/ABCブランチをデプロイするという意味）．
$ fab stg release:feature/ABC  用語の統一は大変で，ドキュメント等でチームに周知し，コードレビューをしっかりやるしかない（できれば簡単なlintツールとかを作りたい）．</description>
    </item>
    
    <item>
      <title>CI-as-a-ServiceでGo言語プロジェクトの最新ビルドを継続的に提供する</title>
      <link>https://deeeet.com/writing/2014/10/16/golang-in-ci-as-a-service/</link>
      <pubDate>Thu, 16 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/10/16/golang-in-ci-as-a-service/</guid>
      <description>Go言語で作成したツールのリリース方法について，最近実践していることを書く．
リリースは，ローカルから人手で行っている．具体的には，自分のローカル環境でクロスコンパイルし，セマンティック バージョニングによるタグをつけ，CHANGELOG.mdを丁寧に書いた上でリリースをしている．クロスコンパイルにはmitchellh/gox，リリースには自分で作成したtcnksm/ghrを使っている（ghrについては，&amp;ldquo;高速に自作パッケージをGithubにリリースするghrというツールをつくった&amp;rdquo;を参考）．
その一方で，開発中の最新のビルドも提供するようにしている．例えば，こんな感じで，Pre-Releaseとして提供している．Go言語での開発なので，go getしてくださいと言える．しかし，環境によってビルドが失敗することもあるし，そもそもGo言語を使っていないユーザもいる．新機能をいち早く使うことにはワクワク感がある（少なくとも自分にはある）．ユーザに負担なくそれを提供したい．
頻繁に開発を行っているときに，これを上記のように人手で毎回やるのは厳しい．WebアプリケーションのようにGit pushを契機にCI-as-a-Serviceでテストが通ったものを自動でリリースするのが美しい．
しかし，今ままでこれをやるのは意外と面倒だった．毎回違った名前でリリースするとリリースだらけになるし，リリースを消すにも新たなシェルスクリプトを頑張って書くしかなかった．
ghrの最新バージョンでは，--replaceオプションをサポートしている．このオプションを使うと，一度リリースしたものを入れ替えてリリースすることができるようになる．もともとは誤ってリリースしてしまったものを入れ替えたいという要望から作ったが，上記のようなCI-as-a-Serviceとの連携でも威力を発揮する．
人によって好みのCI-as-a-Serviceは違う．無料かつ知名度のあるWercker，TravisCI，drone.ioを使い，上記のようにGo言語プロジェクトの最新のビルドを継続的にリリースする方法について書く．
Wercker Werckerには専用のステップを準備した（tcnksm/wercker-step-ghr）．以下のようなwercker.ymlを準備すればよい．これで，テストが通ったあとに，goxによりクロスコンパイルが行われ，zipで圧縮，Githubへのリリースが行われる．リリースはPre-Releaseとして行われる．実際に動いているサンプルは，tcnksm-sample/wercker-golangで確認できる．
box: tcnksm/gox build: steps: - setup-go-workspace - script: name: go get code: | go get -t ./... - tcnksm/goveralls: token: $COVERALLS_TOKEN - tcnksm/gox - tcnksm/zip: input: $WERCKER_OUTPUT_DIR/pkg output: $WERCKER_OUTPUT_DIR/dist deploy: steps: - tcnksm/ghr: token: $GITHUB_TOKEN input: dist replace: true  ちなみに自分はWerckerを採用している．Werckerの仕組みや，stepの自作の方法は別に記事を書いたので参考にしてください．
 Werckerの仕組み，独自のboxとstepのつくりかた | SOTA  TravisCI TravisCIの場合は，以下のような.travis.ymlを準備すればよい．テスト，ビルド，リリースが行われる．実際に動いているサンプルは，tcnksm-sample/travis-golangで確認できる．
language: go go: - 1.3 env: - &amp;quot;PATH=/home/travis/gopath/bin:$PATH&amp;quot; before_install: - go get github.</description>
    </item>
    
    <item>
      <title>Werckerの仕組み，独自のboxとstepのつくりかた</title>
      <link>https://deeeet.com/writing/2014/10/16/wercker/</link>
      <pubDate>Thu, 16 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/10/16/wercker/</guid>
      <description>WerckerはTravisCIやDrone.ioのようなCI-as-a-Serviceのひとつ．GitHubへのコードのPushをフックしてアプリケーションのテスト，ビルド，デプロイを行うことができる．
Werckerは，TravisCIのように，レポジトリのルートにwercker.ymlを準備し，そこに記述された実行環境と実行コマンドをもとにテスト/ビルドを走らせる．
Werckerには，その実行環境をbox，実行コマンド（の集合）をstepとして自作し，あらかじめWercker Directoryに登録しておくことで，様々なテストからそれらを呼び出して使うという仕組みがある．実際，Werkcerで標準とされているboxやstepも同様の仕組みで作成されている（wercker · GitHub）．
今回，WerkcerでのGolangのCross-compileとリリースのために，いくつかboxとstepを自作した．
 tcnksm/wercker-box-gox tcnksm/wercker-step-ghr tcnksm/wercker-step-zip tcnksm/wercker-step-gox tcnksm/wercker-step-goveralls  これらの作り方を簡単にまとめておく．まず，大まかなWerckerの仕組み説明し，次に具体的なboxとstepの作り方をそれぞれ説明する．
Werckerの仕組み Werckerにはpipelineという概念がある．pipelineは，BuildフェーズとDeployフェーズに分けられ，フェーズは複数のstepで構成される．すべてのフェーズは1つのboxと呼ばれる環境上で実行される．
How wercker works
1つのpipelineは1つのwercker.ymlに記述する．例えば，以下のようにBuildフェーズとDeployフェーズ，それらの具体的なstepを記述する．
box: box build: steps: - stepA - stepB deploy: steps: - step1 - step2  Buildフェーズ Buildフェーズは，GitHubやBitbucketへのコードのpushを契機に始まり，アプリケーションのビルド，テスト，コンパイルを行う．生成物がある場合は，PackegeとしてDeployフェーズに渡す．
Deployフェーズ Deployフェーズは，Packegeを受け取り，それを外部サービスへデプロイする．例えば，Webアプリケーションであれば，Herokuへデプロイし，バイナリであれば，Github Releaseやbintray.comへリリースする．
box 各フェーズはboxと呼ばれる同一の環境上で実行される．boxはOSと一連のパッケージがインストールされたVMである．例えば，rubyがインストールされたbox，Golangがインストールされたboxなどがある．
boxはWerckerが提供するもの，もしくは自分でプロビジョニングを定義してWercker Directoryに登録したものを利用することができる．
例えば，Werckerが提供するGolangの実行環境が整ったboxを使いたい場合は，以下のようにwerker.ymlを記述する．
box: wercker/golang  Step フェーズを構成するのが複数のstepであり，stepは名前がつけられた一連のコマンドの集合である．
stepは，Werckerが提供するもの，自分で定義してWercker Directoryに登録したもの，もしくはscriptとしてwercker.ymlに直接定義したものを使うことができる．
例えば，Jekyllで静的サイトを生成するBuildフェーズは以下のようにwercker.ymlを記述する．
build: - bundle-install - script: name: generate static site code: |- bundle exec jekyll build --trace --destination &amp;quot;$WERCKER_OUTPUT_DIR&amp;quot;  この場合bundle-installはWerckerが提供する標準のstepであり，bundlerのインストールや，Gemfileを元に依存gemのインストールを行う（wercker/step-bundle-install）．scriptはgenerate static siteと名付けられたstepであり，codeに実行したいコマンドを直接記述している．</description>
    </item>
    
    <item>
      <title>boot2dockerでのVolume問題が解決しそう</title>
      <link>https://deeeet.com/writing/2014/10/08/boot2docker-guest-additions/</link>
      <pubDate>Wed, 08 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/10/08/boot2docker-guest-additions/</guid>
      <description>（追記）Docker 1.3がリリースされた．boot2dockerはデフォルトでVirtualBox Guest Additionsをサポートし，boot2docker-cliはinitのときにホストのディレクトリをboot2docker-vm上にマウントするようになった（Docker 1.3: signed images, process injection, security options, Mac shared directories | Docker Blog）．
TL;DR OSXやWindowsでboot2dockerを使う場合に特別な操作をしなくても-vオプション（Volume）が使えるようになる．
背景 OSXやWindowsでboot2dockerを使うひとが最も不満に感じるのは-vオプション（Volume）が使えないことだと思う．例えば，以下のようにカレントディレクトリをマウントし，そのファイルを参照しようとしてもファイルはないなどと言われる．
$ echo &#39;hello from OSX&#39; &amp;gt; hello $ docker run -v &amp;quot;$(pwd)&amp;quot;:/osx busybox cat /osx/hello cat: can&#39;t open &#39;/osx/hello&#39;: No such file or directory  boot2dockerを使う場合，Dockerデーモンはboot2docker-vm上で動き，OSXやWindowsから叩くdockerコマンドはそれに対するリモートクライアントとして動作する．Dockerはリモートクライアントからのvolumeをまだサポートしていないため，上記のコマンドはboot2docker-vm内のディレクトリをマウントする．よって，ローカルにあるファイルは発見されない．
現時点でそれを解決するには，OSXやwindowsのディレクトリをboot2docker内にマウントするしかない．しかし，boot2dockerはVirtualBox Guest Additionsをサポートしていないため，独自スクリプトでisoイメージを1から作るか，他人がつくった非公式のisoを使うしかなかった（誰もが一度はググっていろいろ回った結果VBox guest additions #284にたどりついては面倒くせえと思っていたと思う）．
VirtualBox Guest Additionsをサポートが進まなかったのは，boot2dockerのシンプルさが失われること，またパフォーマンスへの危惧が大きい．
どうなるのか まず，そもそもDocker自体がリモートクライアントからのvolumeに対応しようとしている（FUSEが検討されている）．が，まだ議論が進んでいる．
 Proposal: Remote Shared Volumes #7249  その間の穴埋めをboot2dockerがすることになった．理由として，OSXやWindowsでDockerを使う場合には公式的にboot2dockerを使うことになっている以上，-vオプション（Volume）を使えないのはユーザビリティに影響があるため．
 VirtualBox Guest Additions #534 Add VirtualBox shared folders creation #258  上記のPull Requestにより，boot2docker-vmにはVirtualbox Guest Additionsがデフォルトでインストールされるようになった．かつ，boot2docker-cliはinitの際に，OSXの場合は/UsersをWindowsの場合は，/c/Usersを自動でマウントするようになった（オプションで無効にすることもできる）．</description>
    </item>
    
    <item>
      <title>認証付きのDocker Private registryを立てる</title>
      <link>https://deeeet.com/writing/2014/10/02/docker-private-registry-auth/</link>
      <pubDate>Thu, 02 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/10/02/docker-private-registry-auth/</guid>
      <description>DockerHub（Public registry）を使えない場合は，Private Registryを立てる必要がある．DockerはPrivate registry用のDockerイメージを提供しているため，コンテナを立てるだけですぐに使い始めることができる．
$ docker run -p 5000:5000 registry $ docker push docker-private.com:5000/test-image:latest  ただ，これだとURLを知っていれば誰でも好きにイメージをpushできてしまうので，認証を行う必要がある．認証には，Dockerクライアント（docker login）が対応しているBasic認証を利用する．Docker registryには認証機構がないため，nginxやApacheをリバースプロキシとして配置して，Basic認証を行う．
このとき，（当たり前だが）以下の2つの制限がある．
 DockerクライアントのBasic認証はSSLが必須である Dockerクライアントは証明書の正当性をちゃんとチェックする（無視できない）  気軽さを求めて自己署名証明書を使うと，いくつか面倒な部分があるのでまとめておく．環境としては，サーバーをUbuntu，リバースプロキシをnginx，クライアントをOSX+boot2dockerとする．
サーバー側の設定 サーバー側では以下の3つの設定を行う．
 nginxの設定 認証するユーザのパスワードの設定 自己署名証明書の作成  nginxの設定 リバースプロキシにはnginxを用いる．Docker registryはBasic認証を行うためのnginxの設定例を提供している（docker-registry/contrib/nginx）ので，それをそのまま利用する．
$ git clone https://github.com/docker/docker-registry $ cp docker-registry/contrib/nginx/nginx_1-3-9.conf /etc/nginx/conf.d/. $ cp docker-registry/contrib/nginx/docker-registry.conf /etc/nginx/.  パスワードの設定 Docker Registryを利用するユーザの設定を行う（apache2-utilsパッケージを利用する）．
$ htpasswd -bc /etc/nginx/docker-registry.htpasswd USERNAME PASSWORD  自己署名証明書の作成 自己署名（オレオレ）証明書を作る．まず，CAの秘密鍵と公開鍵を作成しておく．
$ echo 01 &amp;gt; ca.srl $ openssl genrsa -des3 -out ca-key.</description>
    </item>
    
    <item>
      <title>DockerHub公式の言語Stack</title>
      <link>https://deeeet.com/writing/2014/09/25/dockerhub-official-language-stacks/</link>
      <pubDate>Thu, 25 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/09/25/dockerhub-official-language-stacks/</guid>
      <description>DockerHub Official Repos: Announcing Language Stacks | Docker Blog
DockerHubには公式のレポジトリがある．そこにはUbuntuやCentos，MySQLやPostgres，MongoといったDockerイメージがコミュニティーベースで，つまりより汎用的に使える形で開発され集められており，ベースイメージとして簡単に使えるようになっている．
今までは，OSのディストリビューションや，Webサーバ，DBなどがメインだったが，公式として各種プログラミング言語のベースイメージも公開された．現状（2014年9月時点）では，c/c++(gcc)，clojure，golang，hylang，java，node，perl，PHP，python，rails，rubyがある．
特徴 この公式の言語stackには以下の3つの特徴がある．
 buildpack-depsイメージをベースにしている 各Versionをサポートしている ONBUILDをイメージもサポートしている  これらを簡単に説明する．
Buildpack-deps buildpack-depsイメージというのは，HerokuのStackのようなイメージで，各言語を動かすために必要な基本的な依存関係等がインストールされている．
Dockerfileは以下．
FROM debian:wheezy RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \ autoconf \ build-essential \ imagemagick \ libbz2-dev \ libcurl4-openssl-dev \ libevent-dev \ libffi-dev \ libglib2.0-dev \ libjpeg-dev \ libmagickcore-dev \ libmagickwand-dev \ libmysqlclient-dev \ libncurses-dev \ libpq-dev \ libpq-dev \ libreadline-dev \ libsqlite3-dev \ libssl-dev \ libxml2-dev \ libxslt-dev \ libyaml-dev \ zlib1g-dev \ &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \ bzr \ cvs \ git \ mercurial \ subversion \ &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*  よく言語をインストールする際に依存で入れるべきものが揃っている．これを元に各言語スタックは作成されるので，これらの依存のインストールし忘れなどを防ぐことができる．</description>
    </item>
    
    <item>
      <title>Dockerの再起動オプション</title>
      <link>https://deeeet.com/writing/2014/09/17/docker-1-2-restart/</link>
      <pubDate>Wed, 17 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/09/17/docker-1-2-restart/</guid>
      <description>Announcing Docker 1.2.0 | Docker Blog
v1.2でもいくつかの面白い機能が追加された．例えば，今まで--privilegedオプションを使うと全権限を与えてしまっていたが--cap-addや--cap-dropオプションでそれを制限できるようになったり，–deviceオプションで利用したいデバイスを指定できたり，コンテナ起動時に/etc/hostsを編集できたり&amp;hellip;など．
中でも再起動オプションが良さげなので，実際に触ってみた．docker runを実行するときに--restartオプションに以下を指定するとコンテナの再起動の挙動を変更できる:
 no - 再起動しない（デフォルト） on-failure - 終了ステータスがnon-zeroの場合に再起動する on-failure:X - 終了ステータスがnon-zeroの場合にX回だけ再起動する always - 終了ステータスがなんであろうと再起動する  no これはデフォルトの挙動で，再起動は行わない．
$ docker run --restart=no busybox /bin/sh -c &#39;date; exit 1&#39; Wed Sep 17 08:13:15 UTC 2014  $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e3389974b4ef busybox:latest &amp;quot;/bin/sh -c &#39;date; e 5 seconds ago Exited (1) 4 seconds ago jolly_hoover  on-failure これは終了ステータスがnon-zeroの場合に再起動し続ける．</description>
    </item>
    
    <item>
      <title>&#34;Microservices&#34;を読んだ</title>
      <link>https://deeeet.com/writing/2014/09/10/microservices/</link>
      <pubDate>Wed, 10 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/09/10/microservices/</guid>
      <description>James Lewis氏とMartin Fowler氏による&amp;ldquo;Microservices&amp;rdquo;を読んだ．以前ざっと目を通したが，最近よく耳にするようになったのでちゃんと読んだ．以下はそのメモ．
概要  &amp;ldquo;Microservices&amp;rdquo; とはソフトウェアシステムの開発スタイルである  近年このスタイルでの開発を見てきて良い結果が出ている 初出は2012年の3月の&amp;ldquo;Micro services - Java, the Unix Way&amp;rdquo;  Microserviceは一連の小さなサービスで1つのアプリケーションを開発する手法  それぞれのサービスは自身のプロセスで動いており，軽量な機構（e.g., HTTP API）を通じて情報をやりとりする これらのサービスは独立して自動デプロイされる  一枚岩として構築されるMonolithicスタイルのアプリケーションと比較すると分かりやすい  一般的なエンタープライズのアプリケーションは，クライアントサイドのユーザインターフェース，データベース，サーバーサイドのアプリケーションの3つで構成される サーバーサイドのアプリケーションは，HTTPリクエストを受け，データベースとやりとりし，クライアントにHTMLを返す  このようなサーバーサイドアプリケーションはMonolithicであり，システムへの変更は新しいバージョンのアプリケーションのビルドとデプロイを要する   Monolithicシステムの構築は一般的には成功したスタイルである  リクエストを処理するロジックは単一のプロセスで動く ロードバランサを配置しスケールアウトさせることもできる  クラウドに多くのアプリケーションがデプロイされ始めるとMonolithicアプリケーションはフラストレーションになってきた  システムの変更サイクルは，全て結びついている モジュール構造の維持や影響範囲の限定が困難になる アプリケーションの一部だけスケールが必要なのに全体をスケールしなければならない  これらのフラストレーションがMicroservicesアーキテクチャーを導きだした，Monolithicなアプリケーションと比較してMicroservicesは:  独立してデプロイできる 独立してスケールできる しっかりしたモジュールの境界をもつ（影響範囲の限定） 様々なプログラミング言語を利用できる 異なるチームで運用できる  Microservicesは新しい考え方ではない  少なくともその根源はUNIXのデザイン哲学に立ち戻っている   Microserviceの特徴  正式な定義はないが，共通の特徴を述べる  すべてのMicroservicesが全ての特徴を満たすわけではない   サービスによるコンポーネント化  コンポーネントを組み合わせてシステムを作りたい コンポーネントを入れ替え可能/アップグレード可能な独立したソフトウェアと定義する Microservicesはライブラリを使うが，主要なコンポーネント化はサービスへ分割することで行う  ライブラリを1つのプログラム内で連結し，インメモリーで関数呼び出しを行うコンポーネントと定義する サービスを別プロセス動作し，HTTPリクエストやRPCなどで連携するコンポーネントと定義する  サービスをコンポーネントとして扱う主要な理由の1つは独立してデプロイできること  良いMicroservicesアーキテクチャーはサービス間をなるべく粗結合にして，変更時のデプロイを少なくする  サービスをコンポーネントとして扱うとインターフェースがより明確になる プロセス内のコールと比べてリモートのコールはコストが高いのでAPIはなるべく粗くある必要がある  ビジネス能力に基づく組織化  巨大なアプリケーションを分割するとき普通は技術レイヤーでそれを区切る（図）  e.</description>
    </item>
    
    <item>
      <title>Tmux Plugin Manager（TPM）を使う</title>
      <link>https://deeeet.com/writing/2014/09/09/tmux-plugin-manager/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/09/09/tmux-plugin-manager/</guid>
      <description> TL;DR tmux-plugins/tpmを使うと，Gemfileやpackage.jsonのように，tmux用のpluginを~/.tmux.confに書いてインストール/有効化することができる．
使い方 まず，tpmをインストールする．
$ git clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm  次に，以下のように~/.tmux.confに利用したいプラグインを記述する．プラグインはtmux-pluginsにまとまっている．
set -g @tpm_plugins &amp;quot; \ tmux-plugins/tpm \ tmux-plugins/tmux-sidebar \ tmux-plugins/tmux-copycat \ tmux-plugins/tmux-open \ tmux-plugins/tmux-resurrect \ tmux-yank/tmux-yank \ tmux-plugins/tmux-battery \ tmux-plugins/tmux-online-status \ &amp;quot; # Initialize tpm run-shell ~/.tmux/plugins/tpm/tpm  あとは，tmuxを起動してPrefix+Iを実行すれば，プラグインがインストールされる．
プラグイン 以下のようなものを使い始めた．
 tmux-resurrect - マシンを再起動しても，tmux-serverが死んでも，保存しておいたsessionやpane，プロセスを復活させられるやつ（デモ）． tmux-sidebar - ディレクトリツリーを表示する．emacsのdirexのtmux版． tmux-open - ハイライトしているファイルやURLを開く． tmux-yank - システムのクリップボードへコピー可能にする．OSXとLinuxの両方で同じように使える． tmux-battery - ステータスバーにバッテリーの残量を表示する．ステータスバーの設定項目に#{battery_icon}や#{battery_percentage}を記述するだけ．  プラグインは自作することができる．作り方は&amp;ldquo;How to create Tmux plugins&amp;rdquo;にまとまっている．プラグインはとてもシンプルで.tmuxファイルと，いくつかのシェルスクリプトを準備するだけで良い．動作としてはtpmが.tmuxを呼び出して，.tmuxがrun-shellでシェルスクリプトを実行する．
まとめ 今はまだtpmの作者が独りでゴリゴリとプラグインを作ってる段階で，それほどプラグインは多くない．そのうち面白いものも出てきそう．
プラグインを一括管理できるのも良いけど，今までいろんなブログのツギハギかつ無理矢理だったもの，例えばコピーの設定などが，OSSベースのより洗練された設定になっていきそうなのが嬉しい．プラグイン作成は簡単そうなので自分でもそのうち作ってみようかと．
Emacsのパッケージ管理もCaskに移行したし，dotfilesが整理されつつある．
参考  tcnksm/dotfiles  </description>
    </item>
    
    <item>
      <title>YAPC::Asia 2014でコマンドラインツールについて語ってきた</title>
      <link>https://deeeet.com/writing/2014/08/31/yapc-2014/</link>
      <pubDate>Sun, 31 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/31/yapc-2014/</guid>
      <description>コマンドラインツールについて語るときに僕の語ること #yapcasia
語ってきました．言いたいことはすべてスライドに詰め込んだし，参考文献もまとめておいたので興味のあるひとは参考にしてください．また，gihyo.jpさんに素晴らしいレポートを書いて頂いたのでそちらもご覧下さい．
 コマンドラインツールを作るときに参考にしている資料 YAPC:: Asia 2014 1日目レポート  以下，簡単に雑感を書いておきます．
YAPC初参加・初トーク 自分は去年東京に来たばかりです．YAPCの盛り上がりは毎年インターネット越しに眺めており，自分もいつか参加したいなと憧れていました．
初めは参加さえできれば良いと思っていたのですが，インターネットのすごい方々と肩を並べて話す機会が誰にでも開かれてるならぶっ込むぞ！と思いトークに応募しました．また，&amp;ldquo;YAPC 初心者ほど YAPC にトーク応募すべき10の理由&amp;rdquo; にも影響を受けました．
トークが採択された後は，必至に準備しました．寝ても覚めてもYAPCのことを考えるという生活を続けました．そもそも参加者がお金を払って観に来るというイベントで喋るという経験が初めてだったので，トーク来てくれたひとに少しでも満足してもらうためにいつも以上に構成を練ったし，いつも以上にスライドをつくり直したし，いつも以上に練習もしました．スライドは，せっかくの機会なので今まで試したことのないスタイルで作成しました．挑戦もちゃんとしました．
トークが始まる前は，こんな内容聞きたいひといるのかなと若干不安でした．それでも立ち見でまで聴いていただけるひとがいたのは非常に嬉しかった．トークは，緊張してたけど，十分に準備していたし，話すのはとても楽しかった．
トーク後に何人かのひとに良かったと声を書けて頂いたのが本当に嬉しかった．ありがとうございます．それだけでも話したかいがあったと思います．
ベストトーク賞3位 めちゃめちゃ恐縮でした．会場とか「誰？」ってなってたと思います．でも，自分のトークが一定数のひとから評価していただいたのは本当に嬉しかった．大きな励みになったし，これからもっと頑張ろうと思えました．これで満足せず，次につなげていきます．もっとコード書きます．
最後に @yusukebeさん，運営の方々，素晴らしい機会を頂きありがとうございました．来年も是非開催してください！またトークしたいです．
懇親会やHUBでいつもブログ等を読んでて参考にしてる方々とお話しする機会があって楽しかった．また次に機会があれば絡んで下さい．
#deeeet_sushi ってのは奢ってもらえるやつですか？</description>
    </item>
    
    <item>
      <title>コマンドラインツールを作るときに参考にしている資料</title>
      <link>https://deeeet.com/writing/2014/08/27/cli-reference/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/27/cli-reference/</guid>
      <description>コマンドラインツールについて語るときに僕の語ること - YAPC::Asia Tokyo 2014
コマンドラインツールが好きで昔からつくってきた． 今年のYAPCで，そのコマンドラインツールをつくるときにどういうことを意識して作っているのか？どのような流れで開発しているのか？といったことを語る機会をもらえた． 具体的な内容については，是非トークを聴きに来てもらうとして， スライドをつくるにあったって過去に読んだ資料や，よく参考にしている記事を集め直したので，その一部を参考資料としてまとめておく．
UNIXという考え方 UNIXという考え方
Mike GancarzによるUNIXの思想や哲学をまとめた本．古いが全然色あせてない． コマンドラインツールの作り方を書いた本ではないが，これらの思想の上で動くツールはこの思想に準拠して作られるべきだと思う．何度も読んで考え方を染み付かせた．
 小さいものは美しい 一つのプログラムには一つのことをうまくやらせる できるだけ早く試作する 効率より移植性を優先する データをフラットなテキストデータとして保存する ソフトウェアを梃子（てこ）として使う シェルスクリプトによって梃子の効果と移植性を高める 過度の対話インターフェースを避ける 全てのプログラムをフィルタとして設計する  小定理  好みに応じて自分で環境を調整できるようにする オペレーティングシステムのカーネルを小さく軽くする 小文字を使い，短く 木を守る（ドキュメント） 沈黙は金（エラーメッセージの出力について) 同時に考える（並列処理） 部分の総和は全体よりも大きい（小さな部品を集めて大きなアプリケーションを作る） 90パーセントの解を目指す 劣る方が優れている 階層的に考える  GNU標準インターフェース Standards for Command Line Interfaces
コマンドラインツールには長い歴史がある．つまり慣習がある．慣習を外れない簡単な方法は，標準に従うこと． 普段からコマンドラインツールは使っているので，インターフェースはわかりきっていると思うかも知れないが，いざ自分がつくるとなると見落としていることは多い．
また，オプションは短オプション（e.g., -f）と長オプション（e.g., --force）の両方を準備するべきだが，長オプションの名前に迷うときがある．そういうときのために，GNUでよく使われている長オプションが以下にまとめられている．
Table of Long Options
Build Awesome Command-line tool Build Awesome Command-Line Applications in Ruby 2: Control Your Computer, Simplify Your Life</description>
    </item>
    
    <item>
      <title>UNIXのワイルドカードがワイルド</title>
      <link>https://deeeet.com/writing/2014/08/18/unix-wildcards-gone-wild/</link>
      <pubDate>Mon, 18 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/18/unix-wildcards-gone-wild/</guid>
      <description>Back To The Future: Unix Wildcards Gone Wild
面白かったので．また気をつけようと思ったので．
ワイルドな実例 例えば，以下のようなファイルとディレクトリがあるとする．
$ ls -al total 0 drwxr-xr-x 9 taichi staff 306 8 18 22:31 . drwxr-xr-x 6 taichi staff 204 8 18 22:27 .. drwxr-xr-x 2 taichi staff 68 8 18 22:26 DIR1 drwxr-xr-x 2 taichi staff 68 8 18 22:26 DIR2 drwxr-xr-x 2 taichi staff 68 8 18 22:26 DIR3 -rw-r--r-- 1 taichi staff 0 8 18 22:26 file1 -rw-r--r-- 1 taichi staff 0 8 18 22:26 file2 -rw-r--r-- 1 taichi staff 0 8 18 22:26 file3 -rw-r--r-- 1 taichi staff 0 8 18 22:30 -rf  ここで，以下のようにワイルドカードでファイル指定しファイル削除を実行する．</description>
    </item>
    
    <item>
      <title>golang勉強会でGo製ツールの配布方法について話してきた</title>
      <link>https://deeeet.com/writing/2014/08/11/golang-study/</link>
      <pubDate>Mon, 11 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/11/golang-study/</guid>
      <description>  &amp;ldquo;Ship your CLI tool built by golang to your user #golangstudy&amp;rdquo;
&amp;ldquo;Golang勉強会&amp;rdquo;で発表してきた．Go言語で作成したツールをクロスコンパイルして，複数プラットフォームに配布する方法について話してきた．自分がGoをはじめた理由の一つがクロスコンパイルによる配布のしやすさであり，いろいろ実践したりそれ用のツールを作ったりしてきたのでそれをまとめた．
以下の視点で話したつもり，
 自動化により開発者の負担を減らす ユーザがツールを使うまでの負担を減らす  &amp;ldquo;わかりやすいREADME.mdを書く&amp;rdquo;にも似たようなことを書いたけど，自分のような無名なエンジニアの作ったツールであってもユーザに使ってもらうには，2点目のような視点を大切にしないといけないと思う．
発表は以下の記事をもとにしている．
 &amp;ldquo;HerokuとGithubを使った統一的なツール配布&amp;rdquo; &amp;ldquo;高速に自作パッケージをGithubにリリースするghrというツールをつくった&amp;rdquo; &amp;ldquo;Go言語のツールをクロスコンパイルしてGithubにリリースする&amp;rdquo; &amp;ldquo;HomeBrewで自作ツールを配布する&amp;rdquo;  感想 特に以下の2つの発表が面白かった．
 &amp;ldquo;GOとライセンス&amp;rdquo; &amp;ldquo;How To Think Go&amp;rdquo;  まず，ライセンスの話．自分でバイナリ配布のことをいろいろやっておきながら，このことを全く考慮してなかった．ソースコードとバイナリではライセンス異なることを知らなかった．めんどくせえけど大事だと思います．参考文献読みます．
あと@lestrratさんの&amp;rdquo;How to Think Go&amp;rdquo;．最高でした． rebuild.fmの&amp;ldquo;Rebuild: 42: When in Golang, Do as the Gophers Do (lestrrat)&amp;rdquo;で話していたことを発表としてさらにパワーアップさせたという印象（これ5回以上聴いたので）． 特に自分のためになったのが「Goで構造体設計」の話．今まで見たGoでのモデリングの説明で一番しっくりきた． 「オブジェクトの階層を作ろうという考え方をしない（&amp;rsquo;動物&amp;rsquo;を作ろうとしない）」，「&amp;rsquo;草食動物&amp;rsquo;ではなく&amp;rsquo;草を食べる&amp;rsquo;というinterfaceを考えてメソッドをそろえる」 などなど，いかにオブジェクト思考的な考え方からGo的な思考に変えていくかという説明の仕方がとてもわかりやすかった． 今までの書いたコードは完全に失敗してるのでちゃんと書き直していきたい．
最後に 発表する機会を与えて下さった@btoさん，ありがとうございました． さらなるモチベーションに繋がるとても良い勉強会でした．質疑の質もとても高かった．次に機会があれば是非参加したいです．
参考  HDE Incで開催のGo勉強会で話してきた : D-7  Go lang勉強会でgo-socket.ioの話してきた - from scratch Go lang勉強会に参加した感想 - きょこみのーと  </description>
    </item>
    
    <item>
      <title>HerokuとGithubを使った統一的なツール配布</title>
      <link>https://deeeet.com/writing/2014/08/07/github-heroku-dist/</link>
      <pubDate>Thu, 07 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/07/github-heroku-dist/</guid>
      <description>Go言語ではクロスコンパイルがとても簡単で，複数プラットフォーム向けのバイナリをつくってそれを配布するというのがさらっとできる．
単純にやるなら，
 クロスコンパイルした各バイナリをzip等に固める Github Releaseやbintray，Dorone.ioなどにホストする  そして，ユーザには自分のプラットフォームに合ったものをダウンロード／展開してPATHの通ったところに置いてもらう．
開発者からすると，すごい簡単．ホストするまで完全に自動化できる．でも，ユーザからすると若干めんどくさい．
もっとツールを使い初めてもらうまでの敷居を下げたい．
TL;DR 全プラットフォーム共通で以下のようにツールをインストールできるようにする．若干長いが1コマンド！
$ L=/usr/local/bin/ghr &amp;amp;&amp;amp; curl -sL -A &amp;quot;`uname -sp`&amp;quot; http://ghr.herokuapp.com/ghr.zip | zcat &amp;gt;$L &amp;amp;&amp;amp; chmod +x $L  このような配布をHerokuとGithubを使ってできるようにする．
実例 このようなツール配布を行っている例はいくつかある．
 heroku/hk flynn/cli  例えば，Herokuのhkは，以下のようにインストールできる．
$ L=/usr/local/bin/hk &amp;amp;&amp;amp; curl -sL -A &amp;quot;`uname -sp`&amp;quot; https://hk.heroku.com/hk.gz | zcat &amp;gt;$L &amp;amp;&amp;amp; chmod +x $L  動作の概要 Githubにリリースを作成し，各プラットフォーム向けのパッケージがホストされているとする．
動作の流れは以下のようになる．
 ユーザがHerokuアプリに対してリクエストを投げる アプリはリクエストに基づきプラットフォームを判定し，それに合ったGithub Release上のパッケージへのリダイレクトを返す ユーザはプラットフォームに合ったパッケージをGithub Releaseから得る  具体的な動作 Githubリリースの作り方，ワンライナーの動作，Herokuアプリについて簡単に説明する．
Github Release まず，Github Releaseページに作成したパッケージをホストしておく．パッケージ名は以下のルールに従うようにする．</description>
    </item>
    
    <item>
      <title>好きなPodcast</title>
      <link>https://deeeet.com/writing/2014/08/06/podcast-2014/</link>
      <pubDate>Wed, 06 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/06/podcast-2014/</guid>
      <description>twitterでちょっとつぶやいてたけど，最近自分がよく聴いてるPodcastをまとめてみる．Tech系以外もすこし混じってる．他にオススメあれば教えてください．
日本語  Rebuild - Podcast by Tatsuhiko Miyagawa - Podcastを聴くという習慣はここから始まった．大学院生のころからずっと聴いてる．Liveもできる限り聴いてる．大ファン．取り上げる技術もすごい尖っていて面白い．全エピソード好きだけど，敢えてあげるなら，&amp;ldquo;3: MessagePack&amp;rdquo;，&amp;ldquo;14: DevOps with Docker, chef and serverspec&amp;rdquo;，&amp;ldquo;27: Dragon Quest, Docker and AngularJS&amp;rdquo;，&amp;ldquo;35: You Don&amp;rsquo;t Need API Version 2&amp;rdquo;, &amp;ldquo;37: N Factor Auth&amp;rdquo;，&amp;ldquo;42: When in Golang, Do as the Gophers Do&amp;rdquo;，&amp;ldquo;45: Remembering WSDL&amp;rdquo;&amp;hellip; mozaic.fm - #1から全て聴いてる．ある特定の技術テーマについてものすごい深い，仕様策定レベルの話が聴ける．自分が全く知らない世界で，こんな話が聴けるのかーって毎回思ってる．#6 WebRTC，#4 Security (protocol)，#2 HTTP2が面白かった． backspace.fm - 最近聴きはじめた．その週のガジェット系のニュースが聴ける．とにかく話がうまくて面白い．編集のテンポもとても良くて，さらっと聴ける．自分だとこんな製品出たんだーで終わるけど，それに対する深い洞察/実際に使った意見が聴けるのが素晴らしい． だんごゆっけの平和な話 - たまに聴く．日曜日とかに買い物行きながら聴く．タイトル通りにyusukebeさんとkamadangoさん等のすごい平和な話が聴ける．境界線の哲学大好き． ライムスター宇多丸のウィークエンド・シャッフル - Tech系ではない．映画が大好きなので，週間映画批評を毎週欠かさず聴いている．毎週ガチャで当たった映画を観に行ってそれを批評するというコーナー．宇多丸さんがすごいのは，つまらない映画をつまらないで終わらせないこと，いかにつまらないかを面白く語るところ．映画の観方はこれで学んだ． たまむすび - これも映画関係．町山さんの回を聴く．全部聴いてるわけではなくて，見た映画で検索して聴いてる．  英語 おそらく自分より，&amp;ldquo;にわか Podcast ファン - steps to phantasien&amp;rdquo;が参考になると思う．めっちゃ聴いてる訳ではなくてつまみ食いが多い．</description>
    </item>
    
    <item>
      <title>TerraformでHerokuアプリのセットアップ</title>
      <link>https://deeeet.com/writing/2014/08/04/terraform-heroku/</link>
      <pubDate>Mon, 04 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/08/04/terraform-heroku/</guid>
      <description>ちょうど新しくHerokuでアプリケーションを作り始めたので，Terraformを使ってセットアップをしてみた．
Terraformとは TerraformはHashicorpの新作．インフラの構成をコード（テンプレートファイル）に落とし込んで，構築/変更することができる．インフラの構成は，複数のプロバイダやツール，例えば，AWSやConsul，DigitalOcean，Herokuなどにまたがって記述することができる．
Terraformが良いのは，各設定値を変数としてサービス間で共有できるところ．例えば，Herokuでアプリケーションを立ち上げた際に自動で割り振られるホスト名を，DNSimpleの設定項目に渡してCNAMEを設定するといったことが1つのファイルに書けてしまう（Cross Provider - Terraform）
他に良い点は，
 依存関係をグラフで管理しており，依存がない部分を並列で実行するため速い 実行する前にDry-run的に実行計画を出力できる ワークフロー（コマンド）がとてもシンプルである  簡単な例 tcnksm/re-dist-ghr・Github
実際に，Terraformを使ってHerokuに新規アプリケーションをセットアップし，作成中のGo言語のWebアプリをデプロイしてみた．
まず，設定ファイルであるheroku.tfは以下．
variable &amp;quot;heroku_email&amp;quot; {} variable &amp;quot;heroku_api_key&amp;quot; {} provider &amp;quot;heroku&amp;quot; { email = &amp;quot;${var.heroku_email}&amp;quot; api_key = &amp;quot;${var.heroku_api_key}&amp;quot; } resource &amp;quot;heroku_app&amp;quot; &amp;quot;default&amp;quot; { name = &amp;quot;ghr&amp;quot; stack = &amp;quot;cedar&amp;quot; config_vars { BUILDPACK_URL=&amp;quot;https://github.com/kr/heroku-buildpack-go.git&amp;quot; } }  やっているのは以下．
 providerでherokuを指定し，APIを利用するための設定を記述する resourceでheroku_appを指定しdefaultアプリケーションを作成し，アプリケーションの名前，利用するStack，環境変数（今回は利用するbuildpack）を記述する  作成前に以下で実行計画（どんな変数が設定されるかなど）を確認することができる．
$ terraform plan \ -var heroku_email=$HEROKU_EMAIL \ -var heroku_api_key=$HEROKU_API_KEY  例えば，今回だと以下のような出力が得られる．
+ heroku_app.default config_vars: &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot; config_vars.</description>
    </item>
    
    <item>
      <title>わかりやすいREADME.mdを書く</title>
      <link>https://deeeet.com/writing/2014/07/31/readme/</link>
      <pubDate>Thu, 31 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/31/readme/</guid>
      <description>Githubなどに自分のツールやライブラリを公開するとき，README.mdは重要な役割を担っている．レポジトリを訪れたユーザが自分のツールを使ってくれるか否かの第一歩はREADME.mdにかかっている，と言っても過言ではない．実際自分が使う側になったときも，まずREADME.mdを読んで判断していると思う．
成功しているプロジェクトを参考にしつつ，自分が実践していることをまとめておく．ここに書いていることはあくまで（自分の中で）最低限的なものである．プロジェクトが成長していくにつれてREADMEはあるべき姿に成長していくべきだと思う．
READMEの役割 README.mdには大きく2つの役割がある．
 プロジェクト，ツールの使い方，インストール方法 プロジェクト，ツールの宣伝  元々READMEは前者の役割しかなかったが，Githubの仕組み上，後者の役割も徐々に重要になっている．
さらに自分の場合は，README.mdを簡単な設計書としても使う．新しくツールやライブラリを書き始めるときは，まずREADME.mdを書く．Usageを書くことでツールの簡単なインターフェース，オプションを定義する．Installを書くことで配布の仕方を定義する．これにより作りたいツールのゴールが明確になる．
以下で詳しく書くが，自分は社内プロジェクトでもREADMEを準備する，準備するようにチームに呼びかけている．その場合は，基本的な使い方に加えて，プロジェクトに新たに参加したメンバーに対してその道しるべになるようにREADMEを使ってる．
テンプレート 自分は以下のテンプレートを使ってる．
Name ==== Overview ## Description ## Demo ## VS. ## Requirement ## Usage ## Install ## Contribution ## Licence [MIT](https://github.com/tcnksm/tool/blob/master/LICENCE) ## Author [tcnksm](https://github.com/tcnksm)  何ができるのか？ Name まず，一番上には名前を書く．かっこいい名前を考える．
Overview 名前のすぐ下にこのツールの概要を一言で書く．レポジトリを訪れたユーザがまず最初に目にし，このツールは何ができるのかを判断する．例えば，
 kennethreitz/requests - Requests is an Apache2 Licensed HTTP library, written in Python, for human beings. progrium/dokku - Docker powered mini-Heroku. The smallest PaaS implementation you&amp;rsquo;ve ever seen.</description>
    </item>
    
    <item>
      <title>Go言語でCPU数に応じて並列処理数を制限する</title>
      <link>https://deeeet.com/writing/2014/07/30/golang-parallel-by-cpu/</link>
      <pubDate>Wed, 30 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/30/golang-parallel-by-cpu/</guid>
      <description>負荷のかかる処理を制限なしに並列化しても意味ない．処理の並列数を予測可能な場合は，当たりをつけて最適化するのもよいが，不明確な場合は，CPU数による制限が単純な1つの解になる．
TL;DR CPU数に応じたバッファ長のChannelを使ってセマフォを実装する．
実例  mitchellh/gox  goxはGo言語製のツールを並列コンパイルするツール．コンパイルの処理は重いため，デフォルトで並列処理数をCPU数で制限している．
簡単な例 例えば，以下のような単純な並列処理を考える．heavy()（重い処理）を並列で実行する．
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;sync&amp;quot; &amp;quot;time&amp;quot; ) func heavy(i int) { fmt.Println(i) time.Sleep(5 * time.Second) } func main() { var wg sync.WaitGroup for i := 0; i &amp;lt;= 100; i++ { wg.Add(1) go func(i int) { defer wg.Done() heavy(i) }(i) } wg.Wait() }  この並列処理の同時実行数をCPU数で制限する．
まず，利用可能なCPUのコア数は，runtimeパッケージのNumCPU()で取得できる．
func NumCPU() int  次に，CPU数をバッファ長としたChannelを作成する．
cpus := runtime.NumCPU() semaphore := make(chan int, cpus)  後は，heavy()をChannelへの送受信で囲む．これで，CPU数だけバッファが溜まると，Channelへの送信がブロックされ，新しい並列処理の開始もブロックされる．</description>
    </item>
    
    <item>
      <title>高速に自作パッケージをGithubにリリースするghrというツールをつくった</title>
      <link>https://deeeet.com/writing/2014/07/29/ghr/</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/29/ghr/</guid>
      <description>tcnksm/ghr・Github
ghrを使えば，1コマンドでGithubにリリースページの作成とそこへのパッケージのアップロードが可能になる．複数パッケージのアップロードは並列で実行される．
デモ 以下は簡単な動作例．
上のデモでは，v0.1.0タグでリリースを作成し，pkg/dist/v0.1.0以下の6つのファイルを並列でアップロードしている（ghrをghrでリリースしている）．1ファイルあたり，2.0M程度なのでまあま速いかと．アップロード結果は，ここで見られる．
背景 &amp;ldquo;Go言語のツールをクロスコンパイルしてGithubにリリースする&amp;rdquo;
上で書いたようにcurl使って頑張ってAPIを叩いていたが，やっぱシェルスクリプトは嫌だし，アップロードが遅い．
Githubへのリリースを行う専用ツールでaktau/github-releaseというのもあるが，オプションが多くて，curlを使うのと大差ない．Descriptionなどは後でページから編集した方がよい．
とういことで，シンプルなインターフェース，かつ高速にリリース可能なものをつくった．
使い方 事前準備としてGithubのAPI Tokenを環境変数にセットしておく．
$ export GITHUB_TOKEN=&amp;quot;....&amp;quot;  あとは，プロジェクトのディレクトリで以下を実行するだけ．
$ ghr &amp;lt;tag&amp;gt; &amp;lt;package&amp;gt;  例えば，上のデモでは，以下を実行している．
$ ghr v0.1.0 pkg/dist/v0.1.0 --&amp;gt; Uploading: pkg/dist/v0.1.0/ghr_0.1.0_darwin_386.zip --&amp;gt; Uploading: pkg/dist/v0.1.0/ghr_0.1.0_darwin_amd64.zip --&amp;gt; Uploading: pkg/dist/v0.1.0/ghr_0.1.0_linux_386.zip --&amp;gt; Uploading: pkg/dist/v0.1.0/ghr_0.1.0_linux_amd64.zip --&amp;gt; Uploading: pkg/dist/v0.1.0/ghr_0.1.0_windows_386.zip --&amp;gt; Uploading: pkg/dist/v0.1.0/ghr_0.1.0_windows_amd64.zip  ディレクトリを指定すれば，そのディレクトリ以下の全てのファイルが，ファイルを指定すれば，そのファイルのみがアップロードされる．
Go言語プロジェクトの場合は，mitchellh/goxで並列クロスコンパイルすれば，もっと幸せになる．
インストール OSXの場合は，[Homebrew]()でインストールできる．
$ brew tap tcnksm/ghr $ brew install ghr  他のプラットフォームの場合は，リリースページからパッケージをダウンロードして，$PATHの通ったところに配置する．
実装 Go言語で実装している．
並列アップロードはgoroutineを使って以下のように書いている．
var errorLock sync.Mutex var wg sync.</description>
    </item>
    
    <item>
      <title>GithubのGo言語プロジェクトにPull Requestを送るときのimport問題</title>
      <link>https://deeeet.com/writing/2014/07/23/golang-pull-request/</link>
      <pubDate>Wed, 23 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/23/golang-pull-request/</guid>
      <description>TL;DR fork元（オリジナル）をgo getしてその中で作業，forkした自分のレポジトリにpushしてPull Requestを送る．
問題 Github上のGo言語のプロジェクトにコミットするとき，cloneの仕方で若干ハマることがある．普通のOSSプロジェクトの場合は，forkしてそれをcloneしてpush，Pull Requestとすればよい．Go言語のプロジェクトでは，同じレポジトリの中でパッケージを分け，それをimportして使ってるものがある．そういう場合にforkしたものをそのままcloneすると，importの参照先がfork元の名前になりハマる．
例えば，[github.com/someone/tool]()があるとする．このレポジトリは[github.com/someone/tool/utils]()という別パッケージを持っており，mainがそれを使っているとする．つまり以下のようになっているとする．
package main import ( &amp;quot;github.com/someone/tool/utils&amp;quot; ) ...  この場合に，通常のやりかたでforkしてソースを取得する．
$ go get -d github.com/you/tool/...  するとソースは，$GOPATH/src/github.com/youに，importしてるutilsパッケージは$GOPATH/src/github.com/someone/tool/utilsにあるといったことがおこる．で，$GOPATH/src/github.com/you/utils直しても反映されない，import書き換えないと！とかなる．
良さげなやりかた [@mopemope]()さんが言及していたり，&amp;ldquo;GitHub and Go: forking, pull requests, and go-getting&amp;rdquo;に書かれているやり方が今のところ良さそう．
まず，fork元（オリジナル）のソースを取得する．
$ go get -d github.com/someone/tool/...  作業は，$GOPATH/src/github.com/someone/tool内でブランチを切って行う．
pushはforkした自分のレポジトリにする．
$ git remote add fork https://github.com/you/tool.git $ git push fork  あとは，そこからPull Requestを送る．
他のやりかた forkして以下のようにcloneするというやり方も見かけた．
$ git clone https://github.com/you/tool.git $GOPATH/src/github.com/someone/tool  他にベストなやり方があれば教えてほしい．</description>
    </item>
    
    <item>
      <title>シェルスクリプトでGo言語のツールをクロスコンパイルしてGithubにリリースする</title>
      <link>https://deeeet.com/writing/2014/07/23/github-release/</link>
      <pubDate>Wed, 23 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/23/github-release/</guid>
      <description>[@motemen]()さんの&amp;ldquo;Wercker で Go のプロジェクトをクロスコンパイルし，GitHub にリリースする - 詩と創作・思索のひろば (Poetry, Writing and Contemplation)&amp;rdquo;を手元からやる．
Werckerからリリース良いと思うけど，自分はリリースは手元で管理したい．その辺は毎回同じスクリプトでやってるのでまとめておく．なお，コードは全てtcnksm/go-distribution-scriptsにある．
クロスコンパイル 基本はHashicorpのやり方を真似してる．
まず，クロスコンパイルはmitchellh/goxを使う．goxは複数プラットフォームの並列コンパイルと出力先の設定の自由度が気に入ってずっと使ってる．何よりシンプルで良い．以下のようなスクリプトを書いている．
# compile.sh gox \ -os=&amp;quot;darwin linux windows&amp;quot; \ -arch=&amp;quot;386 amd64&amp;quot; \ -output &amp;quot;pkg/{{.OS}}_{{.Arch}}/{{.Dir}}&amp;quot;  あとは，これらをzipでアーカイブする（package.sh）．
Githubへのリリース Github APIを使ってリリースの作成，ファイルのアップロードを行う．werkcerはこれらをstepとしてGithubに公開しているのでそれを簡略化して使っている．
 wercker/step-github-create-release wercker/step-github-upload-asset  まず，リリースの作成．以下のようなスクリプトを準備する．
# github-create-release.sh INPUT=&amp;quot; { \&amp;quot;tag_name\&amp;quot;: \&amp;quot;${VERSION}\&amp;quot;, \&amp;quot;target_commitish\&amp;quot;: \&amp;quot;master\&amp;quot;, \&amp;quot;draft\&amp;quot;: false, \&amp;quot;prerelease\&amp;quot;: false }&amp;quot; RELEASE_RESPONSE=$( curl --fail -X POST https://api.github.com/repos/${OWNER}/${REPO}/releases \ -H &amp;quot;Accept: application/vnd.github.v3+json&amp;quot; \ -H &amp;quot;Authorization: token ${GITHUB_TOKEN}&amp;quot; \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d &amp;quot;${INPUT}&amp;quot;)  $OWNERはGithubのユーザ名，$REPOはレポジトリ名，$GITHUB_TOKENはGithub APIのAPI Token（ここから取得できる）を指定する．</description>
    </item>
    
    <item>
      <title>Dockerコンテナのおもしろい名前</title>
      <link>https://deeeet.com/writing/2014/07/15/docker-container-name/</link>
      <pubDate>Tue, 15 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/15/docker-container-name/</guid>
      <description>Dockerコンテナを立ち上げるときに，--nameオプションで名前を指定しないと勝手に名前がつけられる．
$ docker run -d dockerfile/nginx  $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1f29f753eaf6 dockerfile/nginx:latest nginx 2 days ago Up 11 hours 80/tcp, 443/tcp elegant_feynma  例えば，上ではelegant_feynmaという名前がつけられている．
で，これどうやってやってるのかなーと思ってソースを眺めていると，docker/pkg/namesgeneratorというパッケージが名前を生成していた．
名前の生成方法はとても単純で，49個の形容詞と68名の著名な科学者もしくはハッカーの名前をランダムに組み合せているだけ．ソースを見ると，科学者もしくはハッカーの名前と簡単な紹介文，wikipediaへのリンクがコメントに書かれている．
以下が，生成部分の実装．注意深くみると，異変に気づく．
func GetRandomName(retry int) string { rand.Seed(time.Now().UnixNano()) begin: name := fmt.Sprintf(&amp;quot;%s_%s&amp;quot;, left[rand.Intn(len(left))], right[rand.Intn(len(right))]) if name == &amp;quot;boring_wozniak&amp;quot; /* Steve Wozniak is not boring */ { goto begin } if retry &amp;gt; 0 { name = fmt.</description>
    </item>
    
    <item>
      <title>libswarmの現状と将来</title>
      <link>https://deeeet.com/writing/2014/07/14/libswarm/</link>
      <pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/14/libswarm/</guid>
      <description>DockerCon14で新たに発表されたDockerによる新しいOSSであるlibswarmをざっと触ってみたので，現状何ができて，将来的にどういったことができそうになるかを簡単にまとめておく．
TL;DR libswarmを使うと複数ホストやサービス（自社サーバー，DigitalOcean，Amazon EC2，Orchardなど）に存在するDockerコンテナを，１つのホストに存在しているかのように扱うことができるようになる．Dockerがホストを抽象化したのに対して，libswarmは複数ホストを抽象化する．
libswarmを使ったswarmdコマンドを使って，UNIXのパイプのように複数ホストやサービスを連鎖的につなげる．
デモ libswarmで何ができるのかは，DockerCon14でのデモ動画&amp;ldquo;Orchard + libswarm demo from DockerCon&amp;rdquo;を観るのが一番わかりやすい．
ここでは，異なるホスト（ローカルホストとDigitalOcean，Orchard）に対して，swarmdを立ち上げるだけで，それらに個別にログインすることなく，同様のコマンドを発行してaanand/hello-worldコンテナを立ち上げ，かつそれら全てのコンテナの情報を一気に取得している様子が観られる．
libswarmの動作 libswarmのプロジェクトをみると，backendsというディレクトリがある．ここに，様々ななバックエンドサービス，例えば標準的なDocker server/clientやAmazon EC2，Orchardなど，が定義されており，libswarmはこれらのサービスの間の情報のやりとりを受け持つ．
例えば，一番単純なコマンドは以下のようになる．
$ swarmd &#39;dockerserver unix:///var/run/docker.sock&#39; &#39;dockerclient tcp://192.168.59.103:2375&#39;  これを立ち上げたまま，ローカルでDockerコマンドを実行すると，
 クライアントによるHTTPリクエストをdockerserverが受ける dockerserverはリクエストをdockerclientにフォワードする 193.168.59.103のDockerデーモンでコマンドが実行される  この動作は以下の図がわかりやすい．
Libswarm (in a nutshell) | Tech&amp;rsquo;d
上の例は最小限で感動はない．libswarmがすごいのは，例えばdockerclientバックエンドをorchardバックエンドに変えればorchardにコマンドが発行されるし，EC2バックエンドに変えればEC2にコマンドが発行されるところ．さらに，複数のバックエンドサービスを束ねて同時にそれらを扱えるところ．
デモの再現 とりあえず，DockerCon14のデモをOSX上で再現してみる．まず，インストールは以下．Goがインストールされている必要がある．
$ go get github.com/docker/libswarm/... $ go install github.com/docker/libswarm/swarmd  dockerclient まず，dockerclientにboot2dockerを指定してみる．
$ swarmd &#39;dockerserver tcp://localhost:4567&#39; &#39;debug&#39; &amp;quot;dockerclient tcp://:2375&amp;quot;  別のウィンドウを立ち上げて，DOCKER_HOSTをdockerserverで指定した値にして，コマンドを発行する．
$ export DOCKER_HOST=tcp://:4567 $ docker run -d -p 80:80 dockerfile/nginx  すると，nginxコンテナが立ち上がる．</description>
    </item>
    
    <item>
      <title>DockerによるマルチホストのPaaS flynnの概要とそのアーキテクチャー</title>
      <link>https://deeeet.com/writing/2014/07/07/flynn/</link>
      <pubDate>Mon, 07 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/07/flynn/</guid>
      <description>&amp;ldquo;flynnの時代&amp;rdquo;
&amp;ldquo;Docker meetup tokyo #3&amp;rdquo;で発表してきた．内容は，Dockerの応用の１つであるOSSでPaaSをつくるflynnというプロジェクトの概要とそのアーキテクチャーの紹介．このflynnというプロジェクトの中には，Dockerの面白い使い方がたくさん詰まってるため，今後Dockerを使う人が，その応用の際の参考になればという思いで紹介させてもらった．
今回の発表のために資料を集めまくり，理解できない部分は出来る限りコードも読んだ．発表スライドの補完にもなると思うので，そのメモ書き（一応体裁は整えた）を公開しておく．
デモ 以下は，簡単なデモ．
やっていることは以下．
 nodeのアプリケーションをデプロイ ルーティングの追加 スケール  コマンドを含めた詳しい解説は以下で解説する．
前提知識 (Herokuの動作) まず，前提知識としてPaaS (ここではHeroku) がどのように動作しているのかをそのワークフローとともにまとめておく．
$ heroku create   Stackと呼ばれるベースとなるOSを準備する  e.g., Cedar stack   $ git push heroku master   アプリケーションがデプロイされる slug compilerでアプリケーションをビルドしてslugを作成する  slug compiler  各言語のBuildpackの集合 依存関係のインストール  e.g., RubyならGemfileをもとにrubygemsをインストール   slug  ソースと依存ライブラリ，言語のランタイムを含んだ圧縮されたファイルシステム(SquashFS)   アプリケーションの実行環境（Dyno）を準備する  Dyno  LXCをベースにしたContainer環境   Dynoにslugをロードする Procfileをもとにアプリケーションを起動する  Procfile  プロセスの起動コマンドを記述  e.</description>
    </item>
    
    <item>
      <title>Proxy環境下でDockerを動かす</title>
      <link>https://deeeet.com/writing/2014/07/01/docker-behind-proxy/</link>
      <pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/07/01/docker-behind-proxy/</guid>
      <description>Docker1.0がリリースされたことで，プロダクションレディ感もあり，企業でもDockerを使う機運が高まっている．でも，実際はまだまだ本番環境ではなく，テスト環境などで使われることが多い．
Dockerによるテスト環境構築でまず思い浮かぶのはdokku．dokkuはDockerを使ったbash実装のPaaS．プライベートPaaSを持たない，かつHerokuなどを気軽に使えない企業のテスト環境として今後使われる機会がありそう．
ただ，個人での利用とは違い企業などでDockerやdokkuを使う場合は，Proxyに阻まれることがある（というか今日阻まれた）．ので，Proxy環境下でのDocker，dokkuの使い方を簡単にまとめておく．まず，Docker全般に関して，次にdokku特有の問題についてProxyの問題を解決しなければならない状況とその解決方法を説明する．
Proxy環境下でのDocker Dockerを使う中で，外部ネットワークとのやりとりが必要になるのは，以下の3つの場合が考えられる．
 DockerHub（Docker Index）とのやりとり Dockerfile Dockerコンテナ  これらの解決方法をそれぞれ説明する．
DockerHub（Docker Index） まずは，DockerHub（Docker Index）とのやりとりを行う場合．例えば，docker pullなどでイメージを取得する場合など．
この場合は，dockerデーモンを起動する際にhttp_proxy環境変数を設定すればよい．例えば，Ubuntuの場合は，Upstartの設定ファイル/etc/default/dockerにexport http_proxy=&amp;lt;HTTP_PROXY&amp;gt;を記述すればよい．
Dockerfile 次に，Dockerfileで外部ネットワークとやりとりを行う場合．例えば，apt-getなどでパッケージをインストールする場合など．
この場合は，ENVコマンドを使ってDockerfile内で環境変数を設定すればよい．
FROM ubuntu:13.10 ENV http_proxy &amp;lt;HTTP_PROXY&amp;gt; ENV https_proxy &amp;lt;HTTPS_PROXY&amp;gt; RUN apt-get -y update  Dockerコンテナ 最後は，docker runでコンテナを起動した後に，コンテナ内から外部ネットワークとやりとりをする場合．例えば，サードパーティー製のDockerイメージをそのまま使う場合など．
この場合は，-eオプションを使ってhttp_proxy環境変数を設定してコンテナを起動すればよい．
$ docker run -d \ -e &amp;quot;http_proxy=&amp;lt;HTTP_PROXY&amp;gt;&amp;quot; \ progrium/buildstep /build/builder  Proxy環境下でのdokku dokkuのインストール以外で，dokkuが外部ネットワークとやりとりするのは以下の2カ所．
 dokku専用のDockerイメージprogrium/buildstepのpull Buildpackを使ったアプリケーションのビルド  1つ目は上記のDockerHubとのやりとりで示した方法で解決できる．2つ目は若干のハックが必要になる．
dokkuはHerokuと同様にアプリケーションのビルドにBuildpackを使用し，依存パッケージ等のインストールを行う．このビルドは，dokku専用のDockerイメージprogrium/buildstepを使い，そのコンテナ内で実行される．よって，そのときにProxyが設定されている必要がある．これは上記のDockerコンテナで示した方法で解決できる．
dokkuは，bash実装なので，/usr/local/bin/dokkuを直接編集してしまえばよい．編集するのは，build/builderコマンドと共にコンテナを起動するところ．そこで-eオプションを使って環境変数を設定すればよい．
具体的には，
id=$(docker run -d -v $CACHE_DIR:/cache $IMAGE /build/builder)  を以下のようにする．</description>
    </item>
    
    <item>
      <title>ブログにYoボタンを設置した</title>
      <link>https://deeeet.com/writing/2014/06/27/yo/</link>
      <pubDate>Fri, 27 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/06/27/yo/</guid>
      <description>最近はほとんどのコミュニケーションがYoで完結している．得に日本はハイコンテキスト文化なので助かる．だいたい伝わってると思う．最近だとYoに人生を救われたひともいるくらいだ．
さて，ブログを書いたあとにわざわざ「ブログ書いたので読んでください」などと言うのは，粋じゃない．さらっとYoするのが現代のスタイルだ．
ということで，Yoボタンをブログに設置した．

YoがインストールされたデバイスでこのYoボタンを押すと&amp;rdquo;SOTABLOG&amp;rdquo;というアカウントがYoのリストに登録される（PCとかだと手動で..）．すると，ブログ記事が更新される度に&amp;rdquo;SOTABLOG&amp;rdquo;からYoされるようになる．
仕組み といっても自分の個人アカウントが&amp;rdquo;SOTABLOG&amp;rdquo;で一人一人に手動でYoするわけではない．Yo Developers APIを利用している．やり方は以下．
 http://yoapi.justyo.co/から専用アカウントでAPIの登録する．しばらくするとメールで専用のAPI Tokenが送られてくる． http://button.justyo.co/でYoボタンを作成して，サイトに貼付ける．  あとは，以下のようにAPIを叩くと購読者全員に対してYoされる．
$ curl http://api.justyo.co/yoall/ -X POST -d &amp;quot;api_token=XXXXXX&amp;quot;  例えば，当ブログの場合は，Octopressを使っているので，rake gen_deployの度にこのAPIを叩くようにしている．
他の事例としては，YoApp/NFLがある．これはNFLの49erに対して，試合開始とともにYoするスクリプト．他にもサッカーのフランス代表チームも使っているっぽい．
これからもっといろいろな使われ方が出現しそう．
参考  Yo - It&amp;rsquo;s that simple. Yo | TechCrunch Yo App World Cup Feature Alerts Users When Goals Are Scored  俺は何をしてんだ&amp;hellip;</description>
    </item>
    
    <item>
      <title>高速にGo言語のCLIツールをつくるcli-initというツールをつくった</title>
      <link>https://deeeet.com/writing/2014/06/22/cli-init/</link>
      <pubDate>Sun, 22 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/06/22/cli-init/</guid>
      <description>tcnkms/cli-init・GitHub
cli-initを使えば，Go言語コマンドラインツール作成時のお決まりパターンをテンプレートとして自動生成し，コア機能の記述に集中することができる．
デモ 以下は簡単な動作例．
上のデモでは，addとlist，deleteというサブコマンドをもつtodoアプリケーションを生成している．生成結果は，tcnksm/sample-cli-initにある．
背景 Go言語で作られたコマンドラインツールを見ていると，codegangsta/cliというパッケージがよく使われている．
これは，コマンドラインツールのインターフェースを定義するためのライブラリで，これを使えば，サブコマンドをもつコマンドラインツールを簡単につくることができる（Usageを自動で生成してくれたり，bash補完関数をつくれたりするという便利機能もある）．
これを使って，自分もGo言語でコマンドラインツールをいくか作ってみた（e.g., Dockerとtmuxを連携するdmuxというツールをつくった）．で，自分で書いたり，他のプロジェクトを参考にしたりすると，codegangsta/cliを使ったプロジェクトは同様のパターンで記述されていることに気づいた．
このパターンを毎回記述するのはダルいので，それを自動生成することにした．
使い方 使い方は以下．
$ cli-init [options] application  例えば上のデモの場合は以下のようにしている．
$ cli-init -s add,list,delete todo  -sでサブコマンドを指定し，最後に作りたいコマンドラインアプケーションの名前を指定するだけ．
生成されるファイル 例えば，上記のコマンドでは以下のファイルが生成される．
 todo.go commands.go version.go README.md CHANGELOG.md  まず，todo.goの中身は以下．
func main() { app := cli.NewApp() app.Name = &amp;quot;todo&amp;quot; app.Version = Version app.Usage = &amp;quot;&amp;quot; app.Author = &amp;quot;tcnksm&amp;quot; app.Email = &amp;quot;nsd22843@gmail.com&amp;quot; app.Commands = Commands app.Run(os.Args) }  ここには，main()関数が生成され，その中でアプリケーションの基本的な情報が記述される．AuthorやEmailは.gitconfig，Versionはversion.goの値が使われる．Usageの中身だけ自分で記述する．
次に，commands.goには，サブコマンドの定義が記述される．例えば，サブコマンドlistに対しては，以下が生成される．
var commandList = cli.Command{ Name: &amp;quot;list&amp;quot;, Usage: &amp;quot;&amp;quot;, Description: ` `, Action: doList, } func doList(c *cli.</description>
    </item>
    
    <item>
      <title>hikarie.goでLTしてきた&#43;Hashicorpのクールなツール配布</title>
      <link>https://deeeet.com/writing/2014/06/17/hikarie-go1/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/06/17/hikarie-go1/</guid>
      <description>&amp;ldquo;複数プラットフォームにGo言語のツールを配布する&amp;rdquo;
hikarie.goでLTをしてきた．hikarie.goはA Tour of GoとGo研の溝を埋めるために，@7yan00さんと@yosuke_furukawaさんによって始まったイベント．今後Go言語を始めたばかりのGopher達の良い拠り所になっていきそう．
今回自分が話したのは，以下の記事がもとになっている．
 &amp;ldquo;複数プラットフォームにGoアプリケーションを配布する&amp;rdquo; &amp;ldquo;HomeBrewで自作ツールを配布する&amp;rdquo;  まとめると，Goはクロスコンパイルが簡単なので，バイナリでちゃんと配布して，自分のつくったツールを使ってもらうための敷居を下げていこう！という内容．
この辺のやり方は，Mitchell Hashimoto氏のHashicorpのやり方を参考にした．Hashicorp製のツールは基本的に公開当初からOSX，Linux，Windows，Debian，FreeBSDに向けて配布される．あれだけのツールを作っているのに，ユーザがすぐ使えるようにという視点を忘れてないところは本当に素晴らしい．Hashicorpのすごいところは，複数プラットフォームに対応する，始めから豊富なドキュメントを揃える（とくにあのVS.の項が素晴らしいと思う），といった当たり前のことを当然のようにやってくるところだと思う．
開発者としてそういうところと勝負していくには，すごいすごいと言っているだけではなく，良い部分はどんどん取り入れていかないといけないと思う（もちろんバイナリ配布はモバイルアプリの配布と同じようにどんどんアップデートしにくいなど考慮することは多いが）．得に自分はCLIツールをつくるのが好きで，Hashicorp製のツールはインターフェースや設定ファイルのあり方など参考になることがとても多い．
Go言語でいくつかツールはつくってみたけど，まだまだ書き方とかなってなくてクソなので，もっと精進していきたい．
最後に，発表の機会をつくっていただいた@7yan00さんと@yosuke_furukawaさん，ありがとうございました！</description>
    </item>
    
    <item>
      <title>HerokuのAPIデザイン</title>
      <link>https://deeeet.com/writing/2014/06/02/heroku-api-design/</link>
      <pubDate>Mon, 02 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/06/02/heroku-api-design/</guid>
      <description>Herokuが自ら実践しているAPIデザインガイドをGithubに公開した．
&amp;ldquo;HTTP API Design Guide&amp;rdquo;
このガイドは些細なデザイン上の議論を避けて，ビジネスロジックに集中すること目的としている．Heroku特有なものではなく，一般にも十分適用できる知見となっている．
最近は，モバイル向けにAPIをつくることも多いため，勉強もかねて抄訳した．なお内容は，HTTP+JSONのAPIについて基本的な知識があることが前提となっている．
適切なステータスコードを返す それぞれのレスポンスは適切なHTTPステータスコード返すこと．例えば，&amp;rdquo;成功&amp;rdquo;を示すステータスコードは以下に従う．
 200: GETやDELETE，PATCHリクエストが成功し，同時に処理が完了した場合 201: POSTリクエストが成功し，同時に処理が完了した場合 202: POSTやDELETE，PATCHリクエストが成功し，非同期で処理が完了する場合 206: GETのリクエストは成功したが，レスポンスがリソースに対して部分的である場合  その他のクライアントエラーやサーバエラーに関しては，RFC 2616を参照（日本語だと，このサイトや&amp;ldquo;Webを支える技術&amp;rdquo;が詳しい）．
可能な全てのリソースを提供する そのレスポンスで可能な全てのリソース表現（つまり，全ての要素とそのオブジェクト）を提供すること．ステータスコードが200もしくは201のときは常に全てのリソースを提供する．これはPUTやPATCH，DELETEリクエストでも同様．例えば，
$ curl -X DELETE \ https://service.com/apps/1f9b/domains/0fd4  HTTP/1.1 200 OK Content-Type: application/json;charset=utf-8 ... { &amp;quot;created_at&amp;quot;: &amp;quot;2012-01-01T12:00:00Z&amp;quot;, &amp;quot;hostname&amp;quot;: &amp;quot;subdomain.example.com&amp;quot;, &amp;quot;id&amp;quot;: &amp;quot;01234567-89ab-cdef-0123-456789abcdef&amp;quot;, &amp;quot;updated_at&amp;quot;: &amp;quot;2012-01-01T12:00:00Z&amp;quot; }  ステータスコードが202の場合は，完全なリソース表現は含めない．例えば，
$ curl -X DELETE \ https://service.com/apps/1f9b/dynos/05bd  HTTP/1.1 202 Accepted Content-Type: application/json;charset=utf-8 ... {}  リクエストボディ中のシリアル化されたJSONを受け入れる フォームデータに加えて，もしくは代わりに，PUTやPATCH，POSTのリクエストボディ中のシリアル化されたJSONを受け入れること．これにより，リクエストとレスポンスが対称になる．例えば，
$ curl -X POST https://service.</description>
    </item>
    
    <item>
      <title>カーネル読書会 #111でLTしてきた&#43;Dockerによる次世代のPaaS</title>
      <link>https://deeeet.com/writing/2014/05/30/ylug-111/</link>
      <pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/30/ylug-111/</guid>
      <description> &amp;ldquo;DockerでPaaSをつくる #ylug_111&amp;rdquo;
@hyoshiokさんにカーネル読書会でLTをする機会をいただいた．内容はDockerの応用の１つでOSSのPaaSをつくるというもの．Herokuの内部実装を説明しつつ，Dockerによりいかに簡単にPaaSを作れるようになったかを話した．
最後にちょっと話した，次世代のPaaSもしくはHeroku++を目指すFlynnは，野心的ですごく面白い．簡単にいうとFlynnはHerokuの簡便さとAmazon EC2のような自由度を兼ね備えたPaaSを目指している．Flynnは以下の2つのレイヤーで構成される．
 layer0：CoreOSのetcdによるサービスディスカバリー層 layor1：Herokuのようなアプリケーションのデプロイ+管理層  このプロジェクトにはdokkuの作者である@progriumさんも関わっている．dokkuは単にProof of conceptで，実際にこの辺のコミュニティが目指してるのはFlynnのような次世代のPaaSなんだろうなと思う（詳しくは&amp;ldquo;The Start of the Age of Flynn&amp;rdquo;を参考）．
Flynnについては最近いろいろ調べたりしているので，そのうちちゃんとまとめたい．とりあえず，参考文献だけ載せる．
 PaaSに何が起きているのか？ The Start of the Age of Flynn Flynn vs. Deis: The Tale of Two Docker Micro-PaaS Technologies | CenturyLink Labs Welcome to Flynn 5by5 | The Changelog #99: Flynn, Tent, open source PaaSes and more with Jeff Lindsay and Jonathan Rudenberg 5by5 | The Changelog #115: Flynn updates with Jonathan Rudenberg and Jeff Lindsay  </description>
    </item>
    
    <item>
      <title>Go言語のコードレビュー</title>
      <link>https://deeeet.com/writing/2014/05/26/go-code-review/</link>
      <pubDate>Mon, 26 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/26/go-code-review/</guid>
      <description>SoundCloudが2年半ほどGo言語を利用したプロダクトを本番で運用した知見をGopherConで発表していた（&amp;ldquo;Go: Best Practices for Production Environments&amp;rdquo;）．その中で&amp;ldquo;CodeReviewCommentsというGoogleでのGo言語のコードレビューにおいてよくあるコメントをまとめたサイトが紹介されていた．
最近Go言語を書くようになり，使えそうなのでざっと抄訳してみた．&amp;ldquo;リーダブルコード&amp;rdquo;的な視点も含まれており，Go以外の言語でも使えそう．
 gofmtでコードの整形をすること コメントは文章で書くこと．godocがいい感じに抜き出してくれる．対象となる関数（変数）名で初めて，ピリオドで終わること  // A Request represents a request to run a command. type Request struct { ...  // Encode writes the JSON encoding of req to w. func Encode(w io.Writer, req *Request) { ...   外から参照されるトップレベルの識別子にはコメントを書くべき 通常のエラー処理にpanicを使わないこと．errorと複数の戻り値を使うこと エラー文字列は他の出力で利用されることが多いので，（固有名詞や頭字語でない限り）大文字で始めたり，句読点で終わったりしないこと
 例えば，fmt.Errorf(&amp;quot;Something bad&amp;quot;)のように大文字で始めるのではなく，fmt.Errorf(&amp;quot;something bad&amp;quot;)のようにしておくことで，log.Print(&amp;quot;Reading %s: %v&amp;quot;, filename, err)としても，文の途中に大文字が入るようなことがなくなる   エラーの戻り値を_で破棄しないこと．関数がエラーを返すなら，関数が成功したかをチェックすること．エラーハンドリングをして，どうしようもないときにpanicとする
 パッケージのインポートは空行を入れることでグループとしてまとめるとよい
  import ( &amp;quot;fmt&amp;quot; &amp;quot;hash/adler32&amp;quot; &amp;quot;os&amp;quot; &amp;quot;appengine/user&amp;quot; &amp;quot;appengine/foo&amp;quot; &amp;quot;code.</description>
    </item>
    
    <item>
      <title>Heroku Meetup #12でLTしてきた&#43;Heroku on Docker</title>
      <link>https://deeeet.com/writing/2014/05/23/heroku-meetup-12/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/23/heroku-meetup-12/</guid>
      <description>&amp;ldquo;Go Web ApplicationをHerokuにデプロイ + Heroku on Docker #herokujp&amp;rdquo;
Heroku Meetup #12でLTをしてきた．MartiniをつかったGo Web ApplicationをHerokuにぶっ込んでみたという内容で，基本は&amp;ldquo;Martini(+Ginkgo)をWerckerでCIしてHerokuにデプロイ&amp;rdquo;が基になっている．
せっかく最近Dockerを使っているので，HerokuとDockerを絡めた話がしたいなと思い，&amp;ldquo;building&amp;rdquo;を使ってDocker Container上にHerokuと同じ環境を作るという話を追加した．以下はその補足．
Heroku on Docker Heroku on Docker | CenturyLink Labs
CenturyLink Labsが開発した&amp;ldquo;building&amp;rdquo;というツールを使えば，Herokuのbuildpackを使うアプケーション用のコンテナを簡単に立ち上げることができる．つまり，ローカルで気軽にHerokuと同様の環境をつくることができる．
似たようなツールに&amp;ldquo;dokku&amp;rdquo;というツールがある．dokkuはbuildpackとDockerを使ってmini Herokuを作ることができるツール．dokkuを立てたサーバに対してアプリケーションをgit pushすると，新しくDockerコンテナが起動し，アプリケーションのビルドが行われる（&amp;ldquo;Inside Dokku in 5 minutes&amp;rdquo;が詳しい）．
buildingは，dokkuをシンプルにしたツール．カレントディレクトリのアプリケーションをdokkuのbuildstepというスクリプトを使ってビルドしたDockerイメージをつくり，それを使ってコンテナを立ち上げるということをやってくれる．dokkuのようにサーバを立ててssh鍵を通すといった設定なしで使える．
buildingはdokkuをカジュアルに使えるようにしたツールであると言える．
buildingの使いどころ 以下のような場合に使える．
 ローカルにHerokuと同じ環境をつくりたい Cleanな環境でHerokuアプリケーションをビルドしたい buildpackのテストをしたい  buildingはHeroku同様にThird partyのbuildpackの追加も可能なので，それがちゃんと動作するかをテストすることもできる．
buildingを動かす まず，インストール．Rubygemsとして配布されている．
$ gem install building  あとは，動かしたHerokuアプリケーション（Rails，Node，HHVM，WordPress，Go）のディレクトリ内で以下を実行するだけ．
$ building -p 3000 tcnksm/app  -pで解放したいポート番号を指定する．tcnksm/appは作成したいDockerイメージ名．
これだけで，
 専用のDockerfileの作成 イメージのビルド コンテナの起動  をやってくれ，ローカルにHerokuと同じ環境でアプリケーションが立ち上がる．
buildingの動作 buildingがつくるDockerfileは以下のような感じ．</description>
    </item>
    
    <item>
      <title>HomeBrewで自作ツールを配布する</title>
      <link>https://deeeet.com/writing/2014/05/20/brew-tap/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/20/brew-tap/</guid>
      <description>複数プラットフォームにGoアプリケーションを配布する
上の記事で，Go言語で作ったツールを複数プラットフォーム向けにクロスコンパイルし，作成されたバイナリをBintrayでホストするまではできた．あとは，ダウンロード・展開・PATHを通す，をやってもらえば，自分の作ったツールを使ってもらえる．
OSX向けにツールを配布する場合はHomeBrewのFormulaを作っておけば，これをもっと簡単にできる．
TL;DR 以下でインストールできるようにする．
$ brew tap &amp;lt;ユーザ名&amp;gt;/&amp;lt;パッケージ名&amp;gt; $ brew install &amp;lt;パッケージ名&amp;gt;  ただし作成したツールが，GithubのリリースページやBintrayにホストされていることを前提とする．
Formulaの作成 Formulaとは，HomebrewでインストールするパッケージのURLやビルドの手順が書かれたスクリプト．Homebrewでインストールできるツールは全てこのFormulaが準備されていて，それらは全て/usr/local/Library/Formula以下にある．Formulaは単純なRubyのDSLで簡単に書ける．
まず，Formulaの雛形をつくる．
$ brew create &amp;lt;URL&amp;gt;  これを実行すると，URLに基づいた名前でFormulaの雛形が作られる．例えば，http://example.com/foo-0.1.tar.gzだと，foo.rbが作られる．雛形は以下のようになる．
require &amp;quot;formula&amp;quot; class Foo &amp;lt; Formula url &amp;quot;http://example.com/foo-0.1.tar.gz&amp;quot; homepage &amp;quot;&amp;quot; sha1 &amp;quot;1234567890ABCDEF1234567890ABCDEF&amp;quot; # depends_on &amp;quot;cmake&amp;quot; =&amp;gt; :build def install system &amp;quot;./configure&amp;quot;, &amp;quot;--prefix=#{prefix}&amp;quot;, &amp;quot;--disable-debug&amp;quot;, &amp;quot;--disable-dependency-tracking&amp;quot; #system &amp;quot;cmake&amp;quot;, &amp;quot;.&amp;quot;, *std_cmake_args system &amp;quot;make install&amp;quot; end end  あとは，これを編集するだけ．使えるDSLはここを見るとよい．systemが使えるのでやりたい放題といえばやりたい放題．
DSLを書く ここでは，最低限使えそうなものを紹介する．
Go言語のクロスコンパイルではamd64と386のバイナリをそれぞれ作れるので，それに応じてインストールURLを変更できるようにする．これにはHardware.is_64_bit?を使う．以下のように書ける．
if Hardware.is_64_bit? url &amp;quot;http://exmaple.com/foo_amd64.zip&amp;quot; sha1 &amp;quot;dce04210f14dcff0c7863e14695986e02ada4e02&amp;quot; else url &amp;quot;http://exmaple.</description>
    </item>
    
    <item>
      <title>複数プラットフォームにGoアプリケーションを配布する</title>
      <link>https://deeeet.com/writing/2014/05/19/gox/</link>
      <pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/19/gox/</guid>
      <description>tcnksm/jj
最近試しにGo言語でCLIアプリケーションを作成した．joelthelion/autojumpをシンプルにしただけのツールで，ディレクトリを保存して，どこからでもその保存したディレクトリへの移動を可能にする．
Goの環境さえあれば，このようなGo言語のアプリケーションの配布はとても簡単で，インストールは以下のようにするだけでよい．
$ go get github.com/tcnksm/jj_  これだけではなく，Goはクロスコンパイルが簡単で，様々なプラットフォーム向けにバイナリを生成することができる．つまり，Goがインストールされていない環境に対しても簡単にツールを配布することができる．
Packerなどの最近のHashicorp制のツールは，Go言語で書かれており，OSX，Linux，Windows，FreeBSDなど様々なプラットフォーム向けにそれらを配布している．レポジトリを見てると，その辺をいい感じに自動化している．それらを参考にして，今回作成したツールを複数プラットフォーム向けに配布してみた．
TL;DR 以下のようにOXSとLinux，そしてWindowsのそれぞれ386とamd64に対してツールを配布する（まだ不安定なので使わないでください）．
Download
やったことは，
 goxでクロスコンパイル bintrayからバイナリの配布 スクリプトによる自動化  ソースは全て，tcnksm/jjのscripts以下にある．
なぜGoを使い始めたか まず，簡単になぜGoを使い始めたか．理由は下のエントリと同じ．
 On Distributing Command line Applications: Why I switched from Ruby to Go - Code Gangsta Abandoning RubyGems | Mitchell Hashimoto  今まで簡単な便利コマンドラインツールは，Rubyを使ってさらっとつくってきた．他のひとも使えそうなものはRubyGemsで配布するようにつくった．しかし，いざチームの人に使ってもらう段階になると，そもそも自分の周りがジャバなので，gemって何？となり，Rubyのインストールから初めてもらうということが起こった．
自分ならruby-buildやruby-installでさらっと入れるが，Ruby使ったことないひとにとってはインストールさえも障壁が高い．その壁を超えてまで使ってくれるひとは実は少ない．
それはもったいない．たいしたツールしか作れないのであれば，せめて導入の障壁だけでも下げたい．使い手の環境にあったバイナリをつくって，はいどうぞ！としたい．Go言語の良さは，goroutineとかいろいろあるだろうが，自分の中では，このクロスコンパイルのやりやすさが一番大きい．
クロスコンパイル GO言語のクロスコンパイルはとても簡単で，以下のようにするだけでOSXでlinuxのamd64向けのバイナリをつくることができる．
$ GOOS=linux GOARCH=amd64 go build hello.go  複数プラットフォーム向けにクロスコンパイルする場合は，mitchellh/goxを使うともっと簡単にできる．Goxを使う利点は以下が挙げられる．
 シンプル 複数プラットフォームの並列ビルド 複数パッケージの並列ビルド  準備 まず，goxをインストールする．
$ go get github.com/mitchellh/gox  次にクロスコンパイル用のツールをインストールする．</description>
    </item>
    
    <item>
      <title>使いやすいシェルスクリプトを書く</title>
      <link>https://deeeet.com/writing/2014/05/18/shell-template/</link>
      <pubDate>Sun, 18 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/18/shell-template/</guid>
      <description>できればシェルスクリプトなんて書きたくないんだけど，まだまだ書く機会は多い．シェル芸やワンライナーのような凝ったことではなく，他のひとが使いやすいシェルスクリプトを書くために自分が実践していることをまとめておく．
ヘルプメッセージ 書いてるシェルスクリプトが使い捨てではなく何度も使うものである場合は，本体を書き始める前に，そのスクリプトの使い方を表示するusage関数を書いてしまう．
これを書いておくと，後々チームへ共有がしやすくなる．とりあえずusage見てくださいと言える．また，あらかじめ書くことで，単なるシェルスクリプトであっても自分の中で動作を整理してから書き始めることができる．関数として書くのは，usageを表示してあげるとよい場面がいくつかあり，使い回すことができるため．
以下のように書く．
function usage { cat &amp;lt;&amp;lt;EOF $(basename ${0}) is a tool for ... Usage: $(basename ${0}) [command] [&amp;lt;options&amp;gt;] Options: --version, -v print $(basename ${0}) version --help, -h print this EOF }  バージョンを書いたりもする．
function version { echo &amp;quot;$(basename ${0}) version 0.0.1 &amp;quot; }  出力に色をつける ErrorやWarningによって出力の色を変えて出力を目立たせられると良い．コンソールの出力への色づけはエスケープシーケンスを利用する．基本の構文は以下．
\033[{属性値}m{文字列}\033[m  属性値を変更するだけで，文字色や背景色，文字種を変更することができる．自分は以下のような関数を準備して使う．
red=31 green=32 yellow=33 blue=34 function cecho { color=$1 shift echo -e &amp;quot;\033[${color}m$@\033[m&amp;quot; }  以下のように使う．
cecho $red &amp;quot;hello&amp;quot;  対話処理　 例えば，以下のようにユーザ名やパスワードを対話的に入力させることはよくある．</description>
    </item>
    
    <item>
      <title>logspoutでDockerコンテナのログの集約・ルーティング</title>
      <link>https://deeeet.com/writing/2014/05/14/logspout/</link>
      <pubDate>Wed, 14 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/14/logspout/</guid>
      <description>progrium/logspout
logspoutは，ホスト内で動かした全てのDockerコンテナの出力を集約して，好きなところに飛ばす（ルーティングする）ためのツール．開発者はDokkのJeff Lindsay．
以下の2つの特徴がある
 コンテナとして起動（ステートレス） HTTP APIによるルーティングの設定  ログを貯めて管理したり，検索するといったことはできない．コンテナのログをリアルタイムで好きなところに飛ばすだけ．
これだけだが，Dockerのログの問題をいい感じに解決してくれそう．
Dockerのログのしくみ まず，簡単にDockerのログのしくみを説明する．
現時点（2014年5月）でDockerはコンテナ内で吐き出されたstdout/stderrを取得することができる．コンテナのプロセスがstdoutとstderrにログを吐き出し，Dockerはそれをホストにjsonとして保存する．docker logコマンドを使うとそれを取得することができる．
これはシンプルだけど欠点でもある．いずれディスクが圧迫されるし，毎回docker logを叩くわけにもいかない．そのため，Dockerのログをどうするかってのはいろいろ試みられている．
Dockerのログ収集の試み Dockerコンテナのログ収集の試みは，大きく分けて3つある．
 コンテナの内部で収集する：コンテナ内でログ収集のプロセスを同時に走らせる（&amp;ldquo;dockerなら5分で動く！ nginxのログをfluentdで集めてnorikraでストリーム分析&amp;rdquo;，&amp;ldquo;How To Run Rsyslog in a Docker Container for Logging&amp;rdquo;） コンテナの外部で収集する：ホスト側でログ収集のエージェントを走らせて，コンテナのログの書き出し先をホストからマウントする，もしくはjsonを直接読む（&amp;ldquo;Docker Log Management Using Fluentd&amp;rdquo;） 収集および配信用のコンテナを立てる：logstash-forwarderのようなログの収集および配信を担うエージェントをコンテナ内に立てる．そして各コンテナが起動の際に--volumes-fromでそのコンテナを指定する（&amp;ldquo;Docker And Logstash: Smarter Log management For Your Containers&amp;rdquo;）  やりようはいろいろあるが，少なくともDocker的に良いのは，
 コンテナに複数プロセスを立てない　 ホストに多くを設定しない  これを満たすのは，3番目の専用のコンテナを立てる方式．ただ，現状の方法は立てるコンテナごとに--volumes-fromを駆使しなといけないなど，少しめんどくさい．
logsoutの良い点 専用のコンテナ（progrium/logspout）を立てるだけ使える．
つまり，現状動いている他のコンテナになんの設定もなしに使える．当然，ホスト側に特別な設定をする必要がない．
logsoutを使う まず，インストール（以下でdocker runすればインストールもされるので実際は必要ない）
$ docker pull progrium/logspout  例として，&amp;rdquo;hello world&amp;rdquo;を出力し続ける単純なコンテナを立てておく．
$ docker run -d --name hello1 busybox /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot; $ docker run -d --name hello2 busybox /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot;  これらに対してログを収集するには以下を実行する．</description>
    </item>
    
    <item>
      <title>Brewfileでバージョンを指定する</title>
      <link>https://deeeet.com/writing/2014/05/13/brewfile-version/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/13/brewfile-version/</guid>
      <description> 個人用に使うBrewfileなら最新版をインストールするだけでいいと思うんだけど，プロジェクトやチームでBrewfileを共有する場合，ある程度特定のバージョンで揃えたい．
Brewfileだと，Gemfileのように特定のバージョンをがっちり指定してインストールすることはできない．
gem &#39;rails&#39;, &#39;4.0.5&#39;  例えば，深淵な理由によりtomcat 6で開発環境を揃えたいとする．Brewで複数バージョンを使うにはhomebrew/versionsをtapする．そして，以下でバージョンを検索する．
$ brew tap homebrew/versions $ brew search tomcat tomcat tomcat-native tomcat6  あとはそれをBrewfileに書いて共有するだけ．
# Brewfile update || true tap homebrew/versions || true install tomcat6 || true  欲しいバージョンがない場合はHomebrew/homebrew-versionsにPull Requestを投げてしまう（FomulaはただのDSL）．もしくは，自分たちでtapをつくってしまうのが良いかも．
最近見かけたチーム用のtapをつくるってエントリはBoxenよりさらっとできそうで良いなと思った．
 Homebrew vs Boxen を比較して，brewproj に着手 - ja.ngs.io  参考  BrewfileでHomebrewパッケージを管理する  </description>
    </item>
    
    <item>
      <title>DockerのHost networking機能</title>
      <link>https://deeeet.com/writing/2014/05/11/docker-host-networking/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/11/docker-host-networking/</guid>
      <description>DOCKER 0.11 IS THE RELEASE CANDIDATE FOR 1.0
1.0のRCである0.11はいくつかの新機能が追加された．例えば，SELinuxのサポートや，Host networking機能，Link機能でのホスト名，Docker deamonへのpingなど．
この中でもHost networking機能がなかなか面白いので，実際に手を動かして検証してみた．事前知識として&amp;ldquo;Dockerのネットワークの基礎&amp;rdquo;も書きました．ネットワークに関して不安があるひとが先にみると，Host Networing機能の利点／欠点もわかりやすいと思います．
TL;DR Host networking機能を使うと，異なるホスト間のコンテナの連携がちょっぴりやりやすくなる．SerfやConsulのようなサービスディスカバリーツールとの相性も良さそう．
まだ出たばかりの機能で実際に使ってるひとがいないので，あくまで個人の実感．HNのコメントで同様の発言は見かけた．
ネットワークモード コンテナを起動するとき，--netオプションで4つのネットワークモードを選択することができる．
 --net=bridge：仮想ブリッジdocker0に対して新しくネットワークスタックを作成する（default） --net=container:&amp;lt;コンテナ名|コンテナID&amp;gt;：他のコンテナのネットワークスタックを再利用する --net=host：ホストのネットワークスタックをコンテナ内で利用する --net=none：ネットワークスタックを作成しない  bridge ブリッジモードはデフォルトの挙動で，ループバックのloと仮想インターフェースのeth1がつくられる．eth1はホストのveth（Virtual Ethernet）とパイプされる．このモードは外部のネットワークとは隔離される．
$ docker run --net=bridge ubuntu ifconfig eth0 Link encap:Ethernet HWaddr 96:e7:26:24:69:55 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0  container コンテナモードでは既に起動しているコンテナのネットワークスタックが再利用される．以下の場合だと，あらかじめ起動したhelloコンテナで作成したネットワークスタックがそのまま利用される．つまり，helloコンテナを起動したときにホスト側でつくられたvethと仮想インターフェースeth0のパイプがそのまま利用される．
$ docker run -d --name hello ubuntu /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot; $ docker run --net=container:hello ubuntu ifconfig eth0 Link encap:Ethernet HWaddr b2:f4:26:c4:17:16 inet addr:172.</description>
    </item>
    
    <item>
      <title>Dockerのネットワークの基礎</title>
      <link>https://deeeet.com/writing/2014/05/11/docker-network/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/11/docker-network/</guid>
      <description>今までいろいろ触ってきて，Dockerネットワーク周りに関しては何となくは理解していたが，人に説明できるほど理解してなかったのでまとめておく．基本は，Advanced networking - Docker Documentationがベースになっている．
仮想ブリッジの仕組み Dockerのネットワークは，仮想ブリッジdocker0を通じて管理され，他のネットワークとは隔離された環境で動作する．
Dockerデーモンを起動すると，
 仮想ブリッジdocker0の作成 ホストの既存ルートからの空きのIPアドレス空間を検索 空きから特定の範囲のIPアドレス空間を取得 取得したIPアドレス空間をdocker0に割り当て  が行われる．
コンテナを起動すると，コンテナには以下が割り当てられる．
 docker0に紐づいたveth（Virtual Ethernet）インターフェース docker0に割り当てられたIPアドレス空間から専用のIPアドレス  そしてdocker0はコンテナのデフォルトのgatewayとして利用されるようになる．コンテナに付与されるvethは仮想NICで，コンテナ側からはeth0として見える．2つはチューブのように接続され，あらゆるやりとりはここを経由して行われるようになる．
実際にコンテナを起動して確認する．まず，インターフェースから．
$ brctl show bridge name bridge id STP enabled interfaces docker0 8000.000000000000 no  $ docker run -d ubuntu /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot; b9ffb0800ca5  $ docker run -d ubuntu /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot; 4c0d9b786e8f  $ brctl show bridge name bridge id STP enabled interfaces docker0 8000.</description>
    </item>
    
    <item>
      <title>Vagrant1.6のDocker provider</title>
      <link>https://deeeet.com/writing/2014/05/08/vagrant-docker-provider/</link>
      <pubDate>Thu, 08 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/08/vagrant-docker-provider/</guid>
      <description>Feature Preview: Docker-Based Development Environments
Vagrant 1.6からDocker providerがサポートされた．つまり，VagrantでVMだけでなくコンテナも管理できるようになった．
この機能はネイティブでDockerをサポートしてないOSXでも使え，この場合は裏側でProxy VM（boot2docker box）が勝手に立ち上がって，その上でコンテナが立ち上がる．つまり，以下のようになる．
OSX -&amp;gt; (Proxy VM) -&amp;gt; Docker Container  OSXの場合，これは今までboot2dockerを使ってやってきたのと変わらない．ただ，Docker providerを使うと，boot2dockerの立ち上げまで面倒を見てくれる．
何が嬉しいのか VagrantでDockerコンテナを立ち上げる利点はかなりあると思う，
 vagrant upだけで環境を立ち上げられる 同様のインターフェースでLinuxでもOSXでも動かせる コンテナの立ち上げの設定をVagrantfileに書ける Proxy VMの設定をVagranfileに書ける Vagrantの機能（syncd folder，ネットワーク設定，vagrant ssh，provisioner，vagrant share）が使える プラグインが書ける  これはそのままVagrantの利点だけど，それをDockerコンテナに持ち込めるのがよい．つまり，Vagrant道をDockerコンテナを使った開発にも適用できるようになる．
自分的には，Vagrantfileにコンテナの設定などを再現可能な状態で簡単に残せるのがよい．今までOSXでDocker使うときは，boot2docker initして，VBoxManage modifyvmでポートフォワードして，export DOCKER_HOSTして，などなど一手間あったが，vagrant upだけになる．それだけでも嬉しい．
また，v1.6から任意のディレクトリからVagrantのVMを操作できるようになった（Global Status and Control）ので，ほとんどDockerを扱うような感覚で扱える．
使ってみた OSX上でざっと触ってみた．最新版(1.6.1)をインストールしておく．
$ vagrant -v Vagrant 1.6.1  Dockerfile まずDockerfileの準備．ここでは例としてApacheコンテナを立ち上げる．
FROM ubuntu:12.04 RUN apt-get update RUN apt-get install -y apache2 ENV APACHE_RUN_USER www-data ENV APACHE_RUN_GROUP www-data ENV APACHE_LOG_DIR /var/log/apache2 RUN echo &#39;Hello, vagrant docker provider&#39; &amp;gt; /var/www/index.</description>
    </item>
    
    <item>
      <title>DotenvではなくDirenvを使う</title>
      <link>https://deeeet.com/writing/2014/05/06/direnv/</link>
      <pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/06/direnv/</guid>
      <description>Dotenvは，.envファイルから環境変数を読み込むためのツール．他人には共有したくないパスワードやキーなどを.envに環境変数として記述しておき，実行時にそれを読み込むといった使い方をする．例えば自分は，vagrantからDigitalOceanを使う際に，CLIENT_IDやAPI_KEYを.envに記述してVagrantfileでそれを読み込むという使い方をしていた．
ただ，Dotenvは汎用性が低い．Dotenvを有効にするには，プログラム内から明示的にDotenv.loadを呼ぶ必要がある，もしくは，dotenvでプログラムを起動する必要がある．例えば，test-kitchenのdigitaloceanドライバーを使う際には，vagrantの場合と同様にCLIENT_IDやAPI_KEYが必要になる．しかし，test-kitchenでユーザが直接触るのは.kitchen.ymlであり，Dotenv.loadを記述する余地はない（直接test-kitchenのソースコードに記述することはできるが…）．
TL;DR Direnvを使うと，任意のディレクトリ以下で.envrcに記述した環境変数を明示的な読み込みなしで有効にすることができる．つまり，DotenvのようなDotenv.loadの記述なしで使える．
また，Direnvはgoで書かれているので，Rubyなしでも使える．
使用例 以下は，簡単な使用例．特定のディレクトリ内で$SECRET_KEYを利用する場合を考える．
$ pwd /User/tcnksm/test $ echo $SECRET_KEY $ direnv edit . # $Editorが開くので &amp;quot;export SECRET_KEY=thisisalsoanokaysecret&amp;quot;を記述する direnv: loading .envrc direnv: export +SECRET_KEY $ echo $SECRET_KEY thisisalsoanokaysecret $ cd .. direnv: unloading $ echo $SECRET_KEY  特定にディレクトリに移動するだけで，.envrcが読み込まれ$SECRET_KEYが有効になる．
使いどころ 少なくとも以下の2つの場面で利用できそう．
 他人に共有したくない設定を.envrcに環境変数として記述する bundle execしないように.evnrcに$PATHを追加する（参考）  インストール OSXの場合は，brewでインストールできる．
$ brew install direnv  以下を.zshrcに記述しておく．
eval &amp;quot;$(direnv hook zsh)&amp;quot;  その他のインストール方法はREADMEを参考に．
まとめ Direnvはkitchen-digitaloceanにDotenvのサポートのPull Requestを送った際のコメントで教えて頂いた．こういうことはどんどんやっていくべきだと思った．</description>
    </item>
    
    <item>
      <title>Dockerとは何か？どこで使うべきか？</title>
      <link>https://deeeet.com/writing/2014/05/01/what-is-docker/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/05/01/what-is-docker/</guid>
      <description>この記事はDockerに関する実験的な記事や，Buildpackを使ってHeroku AppをDocker Containerとして使えるようにする&amp;ldquo;building&amp;rdquo;の開発などで知られるCenturyLink Labsの &amp;ldquo;What is Docker and When To Use It&amp;rdquo;の翻訳です． Dockerとは何か？Dockerをどこで使うべきか？についてよく見かける記事とは違った視点から説明されています． 翻訳は許可をとった上で行っています．
Dockerとは何でないか Dockerとは何かを説明する前に，Dockerは何でないかについて述べる．Dockerの否定形は何か？Dockerの制限は何か？Dockerが得意でないことは何か？
 DockerはLXCのようなLinux Containerではない DockerはLXCだけのラッパーではない（理論的には仮想マシンも管理できる） DockerはChefやPuppet，SaltStackのようなConfiguration toolの代替ではない DockerはPaaSではない Dockerは異なるホスト間での連携が得意ではない DockerはLXC同士を隔離するのが得意ではない  Dockerとは何か では，Dockerは何ができるのか？メリットはなにか？
 Dockerはインフラを管理することができる Dockerはイメージのビルドや，Docker Indexを通じたイメージの共有ができる DockerはChefやPuppetといったConfiguration toolによりビルドされたサーバのテンプレートにとって，イメージ配布の良いモデルである DockerはCopy-on-wirteのファイルシステムであるbtrfsを使っており，Gitのようにファイルシステムの差分を管理することができる Dockerはイメージのリモートレポジトリをもっているため，簡単にそれらを様々なOS上で動かすことができる  Dockerの代替は何か AmazonのAWS MarketplaceはDocker Indexに近い．ただし，AMIはAWS上でしか動かすことができないのに対して，Dockerイメージは，Dockerが動いているLinuxサーバであればどこでも動かすことができる．
Cloud FoundryのWardenはLXCの管理ツールであり，Dockerに近い．ただし，Docker Indexのような他人とイメージを共有する仕組みを持っていない．
Dockerをいつ使うべきか DockerはGitやJavaのような基本的な開発ツールになりうるものであり，日々の開発やオペレーションに導入し始めるべきである．
例えば，
 インフラのバージョン管理システムとして使う チームにアプケーション用のインフラを配布したいときに使う 稼働中のサーバーと同様の環境をラップトップ上に再現して，コードを実行したいときに使う（例えばbuildingを使う） 複数の開発フェーズ（dev，stg，prod，QA）が必要なときに使う ChefのCookbookやPuppetのManifestと使う  DockerとJavaはどこが似ているのか Javaには&amp;rdquo;Write Once. Run Anywhere（一度書けばどこでも実行できる）&amp;rdquo;という文言がある．
Dockerはそれに似ている．Dockerは，一度イメージをビルドすると，Dockerが動いているLinuxサーバであれば全く同じようにそれを動かすことができる（&amp;ldquo;Build Once．Run Anywhere&amp;rdquo;）．
Javaの場合，例えば以下のようなJavaコードがあるとする．
// HelloWorld.java class HelloWorldApp { public static void main(String[] args) { System.</description>
    </item>
    
    <item>
      <title>Macのターミナルでビールが降る</title>
      <link>https://deeeet.com/writing/2014/04/30/beer-on-terminal/</link>
      <pubDate>Wed, 30 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/04/30/beer-on-terminal/</guid>
      <description> 辛いことがあったときに，どうぞ．
$ ruby -e &#39;C=`stty size`.scan(/\d+/)[1].to_i;S=&amp;quot;\xf0\x9f\x8d\xba&amp;quot;;a={};puts &amp;quot;\033[2J&amp;quot;;loop{a[rand(C)]=0;a.each{|x,o|;a[x]+=1;print &amp;quot;\033[#{o};#{x}H \033[#{a[x]};#{x}H#{S} \033[0;0H&amp;quot;};$stdout.flush;sleep 0.01}&#39;  Gifzo
参考  Macのターミナルで顔が降る Let it Snow in the Terminal of Mac OS X with This Command  </description>
    </item>
    
    <item>
      <title>Martini(&#43;Ginkgo)をWerckerでCIしてHerokuにデプロイ</title>
      <link>https://deeeet.com/writing/2014/04/23/martini/</link>
      <pubDate>Wed, 23 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/04/23/martini/</guid>
      <description>Martini Demo from Martini on Vimeo.
#117: Go, Martini and Gophercasts with Jeremy Saenz - The Changelog
を聴いていて，Sinatra風のGoの軽量WebフレームワークであるMartiniというのを知った．上に貼ったデモを見るとほとんどSinatraで良い感じ．Goはしばらく触ってなかったし，最近のGo事情を知るためにMartiniを触りつついろいろ試してみた．
あとCIサービスのWerckerも良さそうだなと思いつつ触ってなかったので，この機会に使ってみた．
やってみたのは，
 [Martini]()で簡単なGo Web Applicationの作成 Ginkgoを使ってBDDテスト [Wercker]()でCI Go Heroku buildpackでHerokuにデプロイ  今回のソースコードは全て以下にある
tcnksm/sample-martini
Martini パッケージをインストールしておく
$ go get github.com/go-martini/martini  例えば，以下のように書ける．ものすごくシンプル．
// server.go package main import &amp;quot;github.com/go-martini/martini&amp;quot; func main() { m := martini.Classic() m.Get(&amp;quot;/&amp;quot;, top) m.Run() } func top(params martini.Params) (int, string) { return 200, &amp;quot;Hello!&amp;quot; }  以下で起動する．</description>
    </item>
    
    <item>
      <title>OSXからAmazon Glacierに写真を自動バックアップ</title>
      <link>https://deeeet.com/writing/2014/04/20/lightroom-s3/</link>
      <pubDate>Sun, 20 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/04/20/lightroom-s3/</guid>
      <description>今まで惰性でiPhoto使って写真管理をしてきたが，そろそろ本格的な編集/加工をしたいなと思い，Lightroomに移行した（いずれVSCO Filmを使いたい）．その際，バックアップも外付けHDDからAmazon Glacierに移行した．
Amazon Glacierは，Amazonの提供するクラウドストレージで，1GBあたり1円/月で使える．S3と比べて値段は1/10だが，データをダウンロードするには解凍する時間が必要になる．データを頻繁に取り出さないバックアップなどの用途に向いている．また，AWS Command Line Interfaceでファイル同期ができるので，スクリプトを少し書いて自動バックアップの設定も簡単にできる．
launchctlを使ってLightroomにぶっ込んだ写真を自動でGlacierにバックアップをする仕組みをつくった．
まず，Bucketを作成する．
$ export AWS_CONFIG_FILE=... $ export BUCKET=... $ aws s3 mb s3://${BUCKET}  次に，作成したBucketにファイルのLifecycleルールを設定する．対象はBucket内の全てのファイルで，ファイルが同期され次第すぐにGlacierに移行するようにする．これを実現するため以下のjsonファイルを準備する．
# lifecycle.json { &amp;quot;Rules&amp;quot;: [ { &amp;quot;ID&amp;quot;: &amp;quot;Rule for backup&amp;quot;, &amp;quot;Status&amp;quot;: &amp;quot;Enabled&amp;quot;, &amp;quot;Prefix&amp;quot;: null, &amp;quot;Transition&amp;quot;: { &amp;quot;Days&amp;quot;: 0, &amp;quot;StorageClass&amp;quot;: &amp;quot;GLACIER&amp;quot; } } ] }  作成したルールをBucketに適用する．
$ aws s3api put-bucket-lifecycle --bucket ${BUCKET} --lifecycle file://lifecycle.json  ファイルの同期は以下のシェルスクリプトで行う．例えば~/Photo以下を同期する．
# backup.sh SRC=/Users/tcnksm/Photo BUCKET=... echo &amp;quot;[$(date +%Y-%m-%d-%H-%M)] Start backup to S3&amp;quot; export AWS_CONFIG_FILE=.</description>
    </item>
    
    <item>
      <title>Docker Meetup Tokyo #2 でLTしてきた &#43; DigitalOceanとGCEでもDocker Applicationを動かしてみた</title>
      <link>https://deeeet.com/writing/2014/04/13/docker-meetup-2/</link>
      <pubDate>Sun, 13 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/04/13/docker-meetup-2/</guid>
      <description>&amp;ldquo;Docker ApplicationをDaaSにデプロイ #dockerjp // Speaker Deck&amp;rdquo;
Docker Meetup Tokyo #2でLTをしてきた．Docker as a Service (DaaS) でDocker Application（Rails）を動かしてみたという内容で，基本は&amp;ldquo;OrchardにDockerアプリケーションをデプロイ&amp;rdquo;に書いたことをプレゼンにした．
発表時間は5分だけで，当日までにいろいろ試したことすべてを話すことができなかったので少し追記しておく．
LTではDockerコンテナ専用のホスティングサービスの話をしたが，それ以外のホスティングサービスでもDockerのサポートがされ始めている．例えば，DigitalOceanやGoogle Compute Engineなどがある．これらでもDocker Applicationを動かしてみた．まさに[@naoya_ito]()さんが話してたような，Build Once, Run Anywhereをやってみた感じ．
概要 動かすDocker ApplicationはLTで話したのと同様にRailsコンテナ（tcnksm/rails）と，DBにPostgresqlのコンテナ（[orchard/postgresql]()）で，それぞれ立ち上げて連携する．なおどちらもDocker Registryにあらかじめpushしておく．
これをローカル環境，Docker as a Service（Orchard），DigitalOcean，Google Compute Engingeで動かしてみる．
ローカル環境 ローカル環境（OSX）では，boot2dockerを使う．
あらかじめ，Port Forwardingした上で，VMを立ち上げる．
$ boot2docker init $ VBoxManage modifyvm &amp;quot;boot2docker-vm&amp;quot; --natpf1 &amp;quot;tcp-port3000,tcp,,3000,,3000&amp;quot; $ boot2docker up  後は，以下でコンテナを起動するだけ．
$ docker run -d -p 5432:5432 -e POSTGRESQL_USER=docker -e POSTGRESQL_PASS=docker --name pg orchardup/postgresql $ docker run -d -p 3000:3000 --link pg:db --name web -t tcnksm/rails &#39;rake db:create &amp;amp;&amp;amp; rake db:migrate &amp;amp;&amp;amp; rails s&#39;  [http://localhost:3000]()でアクセスできる．</description>
    </item>
    
    <item>
      <title>Docker&#43;Serf&#43;HAproxy (&#43;Supervisor)</title>
      <link>https://deeeet.com/writing/2014/04/08/docker-serf-haproxy/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/04/08/docker-serf-haproxy/</guid>
      <description>SerfでDockerコンテナのクラスタを形成する SerfでHAProxyの更新 on Vagrant  でやったことを融合した．つまり，HAProxy（ロードバランサ）コンテナとWebサーバコンテナを立てて，Serfでそれらのクラスタを形成する．そしてWebサーバコンテナの増減に応じてHAProxyコンテナの設定を書き換えるということをやってみた．
基本的には，上でやったことをDockerのコンテナに移行するだけだが，Dockerは1コンテナで1プロセスが普通であるため，複数プロセス（サービス）をどう扱うかが問題になる．
Dockerで複数プロセスを扱うときには，Supervisorという選択肢がある．この方法は，公式で紹介されていたり，Foot Fightの&amp;ldquo;Docker in Practice&amp;rdquo;で言及されてたり，CenturyLink Labsが試みていたりする．
ということで，SupervisordとSerf，HAproxyによるDockerコンテナのディスカバリをやってみた．
tcnksm/docker-serf-haproxy
準備 OSX+boot2dockerで行う．あらかじめboot2docker-vmのport forwardingの設定だけしておく．
$ boot2docker init $ VBoxManage modifyvm &amp;quot;boot2docker-vm&amp;quot; --natpf1 &amp;quot;tcp-port8080,tcp,,8080,,8080&amp;quot; $ boot2docker up  HAProxy+Serfコンテナ Dockerfileは以下．
FROM ubuntu # Install serf RUN apt-get install -y unzip wget RUN wget --no-check-certificat https://dl.bintray.com/mitchellh/serf/0.5.0_linux_amd64.zip -O serf.zip RUN unzip serf.zip RUN chmod +x serf RUN mv serf /usr/bin/serf # Install HAProxy RUN apt-get -y install haproxy RUN sed -i -e &#39;s/ENABLED=0/ENABLED=1/&#39; /etc/default/haproxy # Install supervisor RUN apt-get install -qy supervisor # Install basic packages for ruby RUN apt-get -y update RUN apt-get install -y build-essential git RUN apt-get install -y zlib1g-dev libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt-dev RUN apt-get clean # Install ruby-build RUN git clone https://github.</description>
    </item>
    
    <item>
      <title>SerfでHAProxyの更新 on Vagrant</title>
      <link>https://deeeet.com/writing/2014/04/01/serf-haproxy/</link>
      <pubDate>Tue, 01 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/04/01/serf-haproxy/</guid>
      <description>Serfの典型的な使い方として紹介されることの多い，HAProxyの登録/更新をやってみた．これは既に何人かの方が試みているし，SerfのGithubのdemoページでも紹介されている．
 hashicorp/serf/demo/web-load-balancer &amp;ldquo;Serf+HAProxyで作るAutomatic Load Balancer&amp;rdquo; &amp;ldquo;Synapse と Serf でサービスディスカバリ&amp;rdquo;  これらが何をやっているかを簡単に書くと，1つのProxyサーバ（ロードバランサ）と複数のWebサーバという構成において，Webサーバの増減に応じてロードバランサの設定を自動で書き換えるというもの．
これをVagrantで複数サーバを立ち上げて，自分で手を動しつつ触ってみた．
tcnksm/sample-serf-haproxy
Vagrantさえあれば誰でもすぐ試せるようになっている．
Vagrantの準備 以下のようなVagrantfileを準備する．
# Serfのインストール # すべてのホストで実行する $script = &amp;lt;&amp;lt;SCRIPT sudo apt-get install -y unzip cd /tmp/ wget https://dl.bintray.com/mitchellh/serf/0.5.0_linux_amd64.zip -O serf.zip unzip serf.zip chmod +x serf mv serf /usr/bin/serf SCRIPT Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; config.vm.provision :shell, inline: $script # proxyサーバのプロビジョニング # (1) HAProxyのインストール # (2) HAProxyの有効化 # (3) HAProxyの初期設定の書き出し # (4) HAProxyの起動 config.</description>
    </item>
    
    <item>
      <title>SerfでDockerコンテナのクラスタを形成する</title>
      <link>https://deeeet.com/writing/2014/03/31/docker-serf/</link>
      <pubDate>Mon, 31 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/31/docker-serf/</guid>
      <description>&amp;ldquo;Serf虎の巻&amp;rdquo;書いたし，Serf使っていろいろやってみるかということで，Dockerコンテナのクラスタ形成をやってみた．SerfとDockerの組み合わせについては，すでに[shiba_yu36]()さんが試みている（&amp;ldquo;serfとDockerでクラスタを組んでみる&amp;rdquo;）ので，もう少し踏み込んでクラスタへのjoinの仕方を模索してみた．
tcnksm/sample-docker-serf
やってみたのは，Dockerコンテナのみでのクラスタの形成．
準備 Vagrant上で実行する．Vagrantfileは以下．
$script1 = &amp;lt;&amp;lt;SCRIPT echo Installing depedencies... sudo apt-get install -y unzip echo Fetching Serf... cd /tmp/ wget https://dl.bintray.com/mitchellh/serf/0.5.0_linux_amd64.zip -O serf.zip echo Installing Serf... unzip serf.zip sudo chmod +x serf sudo mv serf /usr/bin/serf SCRIPT Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; config.vm.provision &amp;quot;shell&amp;quot;, inline: $script1 config.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;172.20.20.10&amp;quot; config.vm.provision :docker do |d| d.pull_images &amp;quot;ubuntu&amp;quot; end end  VagrantからもSerfを使いたいので事前にプロビジョニングでインストールしておく．またDockerプロビジョニングも有効にしておく．
次に，Dockerfileは以下．Serfをインストールするだけ．
FROM ubuntu RUN apt-get -y install unzip wget RUN cd /tmp/ RUN wget --no-check-certificat https://dl.</description>
    </item>
    
    <item>
      <title>Dockerを便利に使うためのaliasをつくった</title>
      <link>https://deeeet.com/writing/2014/03/30/docker-alias/</link>
      <pubDate>Sun, 30 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/30/docker-alias/</guid>
      <description>tcnksm/docker-alias
いろいろなひとのTipや，自分がやったやつの寄せ集めで作った．以下で使えるようになる．
$ curl -fsSL https://raw.github.com/tcnksm/docker-alias/master/zshrc &amp;gt;&amp;gt; ~/.zshrc &amp;amp;&amp;amp; source ~/.zshrc  コンテナの起動 インタラクティブモードでコンテナを起動する．
alias dki=&amp;quot;docker run -i -t -P&amp;quot;  $ dki base /bin/bash  デーモンモードでコンテナを起動する．
alias dkd=&amp;quot;docker run -d -P&amp;quot;  $ dkd base /bin/echo hello  コンテナの情報 最後に起動したコンテナのIDを取得する．
alias dl=&amp;quot;docker ps -l -q&amp;quot;  $ docker run -d ubuntu /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot; $ dl 4b9aa02548ae  コンテナのIPを取得する．
alias dip=&amp;quot;docker inspect --format &#39;{{ .</description>
    </item>
    
    <item>
      <title>Serf 虎の巻</title>
      <link>https://deeeet.com/writing/2014/03/23/serf-basic/</link>
      <pubDate>Sun, 23 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/23/serf-basic/</guid>
      <description>サービスディスカバリーとオーケストレーション用のツールであるSerfについてまとめた．基本的には公式のHPのGetting Startの抄訳．Vagrantで試験環境を立てて実際に触りつつSerfを使い始められるようにした．
目次  Serfとは Gossip protocolとは 試験環境の準備 クラスタの形成 クラスタからの離脱 イベントハンドラ カスタムイベント カスタムクエリ コマンド一覧 参考  Serfとは Serfはサービスディスカバリーやオーケストレーション，障害検出のためのツール．Vagrantの開発者であるMitchell Hashimoto氏により開発が進められている．SerfはImmutable Infrastructureの文脈で登場してきたツールであり，Immutableなシステムアーキテクチャー，デプロイを実現する上で必須のツールである．
Immutable Infrastructureを簡単に説明すると，上書き的にサーバーを更新するのではなく，デプロイの度に１からにサーバ，イメージを構築してしまおうという考え方．現段階では，ChefやPuppet，AnsibleのようなConfiguration toolでソフトウェア，サービスの設定を行いイメージを作成し，テストが完了した段階でロードバランサを切り替えるというワークフローが提唱されている（Blue Green Deployment）．もしくは，Dockerなどのコンテナベースであれば，そのポータビリティにより，ローカルでコンテナをつくって，それをそのままプロダクションデプロイする方法も考えられる．
このとき問題になるのは，ロードバランサへの追加や，Memcacheのクラスタ，MySQLのslave/masterなどの動的に変わるような設定．もちろんChefやPuppetがこれらの設定まで受け持つことは可能であるが，Immutableなデプロイを実現する上では複雑性が増す．
これを解決するのがSerf．ChefやPuppetで不変なサーバ，イメージが完成したあとに，それらのサーバ，イメージ間の紐付けやクラスタリングを行う．
Serfができること Serfは，大きく以下の3つのことを行うことができる．
 クラスタリング: Serfはクラスタを形成し，クラスタへメンバーの参加，離脱といったイベントを検出して，メンバーそれぞれにあらかじめ設定したスクリプトを実行させることができる．例えば，SerfはロードバランサのためのWebサーバのリストをもち，ノードの増減の度にロードバランサにそれを通知することができる． 障害の検出と回復: Serfはクラスタのメンバーが障害で落ちた場合にそれを検出し，残りのメンバーにそれを通知することができる．また，障害によりダウンしたメンバーを再びクラスタに参加させるように働く． イベントの伝搬: Serfはメンバーの参加，離脱といったイベント以外にオリジナルのカスタムイベントをメンバーに伝搬させることができる．これらは，デプロイやConfigurationのトリガーなどに使うことができる．  Serfの利用例 具体的なSerfの利用例には以下のようなものがある．
 Webサーバのロードバランサへの登録，解除 RedisやMemcachedのクラスタリング DNSの更新 デプロイのトリガー（カスタムイベント） シンプルなサービス監視（カスタムクエリ）  詳細は，公式のUse Casesを参照．
目次へ
Gossip Protocolとは Serfはクラスタのメンバーへのイベントの伝搬にGossip Protocolを用いている．Gossip Protocolは&amp;ldquo;SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol&amp;rdquo;を基にしており，SWIMのイベントの伝搬速度とカバレッジに改良を加えている．
SWIM Protocolの概要 Serfは新しいクラスタの形成，既存のクラスタへ参入，のどちらかで起動する．新しいクラスタが形成されると，そこには新しいノードが参入してくることが期待される．既存のクラスタに参入するには，既存クラスタのメンバーのIPアドレスが必要になる．新しいメンバーはTCPで既存クラスタのメンバーと状態が同期され，Gossiping（噂，情報のやりとり）が始まる．
GossipingはUDPで通信される．これにより，ネットワークの容量はノードの数に比例して一定になる．Gossipingよりも頻度は低いが，定期的にTCPによるランダムなノード間で完全な状態同期が行われる．
障害検出は，定期的にランダムなノードをチェックすることにより行われる．もし一定期間あるノードから反応がない場合は，直接そのノードに対してチェックが行われる．ネットワーク上問題でノードからの反応が得られていない可能性を考慮して，この直接のチェックは複数のノードから行われる．ランダムなチェックおよび，直接のチェックでも反応がない場合，そのノードは，_suspicious_と認定される．_suspicious_であってもそのノードはクラスタの一員として扱われる．それでも反応が慣れれば，そのノードは落ちたと認定され，それは他のノードにGossipされる．
GossipのSWIMからの改良点 Gossip ProtocolのSWIMからの変更点は大きく以下の3点</description>
    </item>
    
    <item>
      <title>OrchardにDockerアプリケーションをデプロイ</title>
      <link>https://deeeet.com/writing/2014/03/22/docker-orchard/</link>
      <pubDate>Sat, 22 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/22/docker-orchard/</guid>
      <description>Orchardは，Docker as a ServiceなDocker専用のホスティングサービス．[DigitalOcean]()のように時間単位の課金で利用できる．DigitalOceanより若干高いが，512MB RAM/20GB SSDであれば，1時間1円/月1000円程度で利用できる．
同様のサービスには，StackDockがある．またDockerをサポートしているプラットフォームとしては，Google Compute EngineやDigitalOceanなどがある．これと比較してOrchardがよいと感じた理由は以下．
 シンプル．専用のコマンドラインラッパーを使って，いつも通りのDockerコマンドをローカルから発行するだけでbuild/runが実行できる（StackDockはWebコンソールにDockerfileを書く）． Figのサポート/開発を行っており，将来的に複数のDockerのコンテナ間のリンクなどがやりやすくなりそう．  ということで，実際にサンプルアプリケーションをデプロイしてみた．サンプルコードは全て以下にある．
 tcnksm/sample-docker-orchard  準備 OSX上で行う．まず，Dockerのインストール．
$ brew update $ brew tap homebrew/binary $ brew install docker  次に，Dockerのデーモンを動かすために，VirtualBoxとboot2dockerのインストール．
$ brew tap phinze/homebrew-cask $ brew cask install virtualbox $ brew install boot2docker  boot2dockerを立ち上げて，Docker hostの環境変数を設定する．
$ boot2docker init $ boot2docker up $ export DOCKER_HOST=tcp://localhost:4243  ローカルで，アプリケーションの実行確認をする場合は，upする前に，boot2docker-vmのPort forwardingの設定をしておく．
$ VBoxManage modifyvm &amp;quot;boot2docker-vm&amp;quot; --natpf1 &amp;quot;tcp-port3000,tcp,,3000,,3000&amp;quot;  最後に，Orchardでアカウントを作成し，Orchardのコマンドラインツールをインストールする．
$ curl -L https://github.</description>
    </item>
    
    <item>
      <title>DockerfileのONBUILD</title>
      <link>https://deeeet.com/writing/2014/03/21/docker-onbuild/</link>
      <pubDate>Fri, 21 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/21/docker-onbuild/</guid>
      <description>Docker 0.8においてONBUILDというDockerfile用のコマンドが導入された．0.8ではOSXのdocker clientが脚光を浴びたが，このONBUILDはかなり強力な機能．リリースノートはこちら．ONBUILDの公式ドキュメントはこちら．
ONBUILDを使うと，次のビルドで実行するコマンドをイメージに仕込むことができるようになる．つまり，ベースイメージにONBUILDによるコマンドを仕込み，別のDockerfileでそのベースイメージを読み込みビルドした際に，そのコマンドを実行させるということが可能になる．要するに，親DockerfileのDockerfileコマンドを子Dockerfileのビルド時に実行させることができる機能．
これは，アプリケーション用のイメージを作るときや，ユーザ特有の設定を組み込んだデーモン用のイメージを作るときなどに有用になる．また，オリジナルのHerokuのBuildpack的なものを作ることもできる．
言葉では伝えられないので簡単に動作例を示す．例えば，以下のようなDockerfile.baseを準備する．
# Docekerfile.base FROM ubuntu ONBUILD RUN echo &amp;quot;See you later&amp;quot;  これをtcnksm/echo_baseという名前でビルドする．
$ docker build -t tcnksm/echo_base - &amp;lt; Dockerfile.base Step 0 : FROM ubuntu Pulling repository ubuntu ... f323cf34fd77: Download complete ---&amp;gt; 9cd978db300e Step 1 : ONBUILD RUN echo &amp;quot;See you later&amp;quot; ---&amp;gt; Running in 9e42ede94d60 ---&amp;gt; e18fdd8d9fa8  RUN echoは実行されていない．
次に，このtcnksm/echo_baseを基にした別のイメージを作成するDockerfileを準備する．
FROM tcnksm/echo_base  tcnksm/echoという名前でビルドする．
$ docker build -t tcnksm/echo . Uploading context 3.</description>
    </item>
    
    <item>
      <title>Dockerコンテナ間のlink，database.ymlの書き方</title>
      <link>https://deeeet.com/writing/2014/03/20/docker-link-container/</link>
      <pubDate>Thu, 20 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/20/docker-link-container/</guid>
      <description>DockerはLinksというコンテナ同士の連携を簡単に行う仕組みをもつ． これは，DB用のコンテナとアプリケーション用のコンテナの連携を行いたいときなどに有用になる．
例えば，1337ポートがEXPOSEされたcontainer1という名前のコンテナとの連携を行いたいとする． このとき以下のように，-link 連携したいコンテナ名:エイリアス名で新しいコンテナを起動すると， そのコンテナ内に連携したいコンテナのポート番号やIPをもった環境変数が現れる．
docker run -d -link container1:alias user/sample bash root@48408a38c9b2:/# env ALIAS_PORT_5432_TCP_ADDR=172.17.0.2 ALIAS_PORT=tcp://172.17.0.2:5432 ALIAS_PORT_5432_TCP=tcp://172.17.0.2:5432 ALIAS_PORT_5432_TCP_PORT=5432 ...  この環境変数を使えば，コンテナからコンテナへのデータの送信などの連携が可能になる．これがLinksの機能．
PostgresqlコンテナとRailsコンテナの連携 例として，postgresqlコンテナとRailsコンテナを連携させてみる． postgresqlのイメージには，dockerコンテナのホスティングサービスであるOrchardが提供する[orchardup/postgresql]()が使いやすいのでそれを利用する．
まず，postgresqlコンテナをpgという名前で起動する．
$ docker run -d -p 5432:5432 -e POSTGRESQL_USER=docker -e POSTGRESQL_PASS=docker -name pg orchardup/postgresql  Railsからこのコンテナのデータベースにアクセスするには，config/database.ymlを以下のようにしておく．
development: adapter: postgresql template: template0 encoding: unicode database: my_app_development pool: 5 username: docker password: docker host: &amp;lt;%= ENV.fetch(&#39;DB_PORT_5432_TCP_ADDR&#39;) %&amp;gt; port: &amp;lt;%= ENV.fetch(&#39;DB_PORT_5432_TCP_PORT&#39;) %&amp;gt;  あとは，エイリアス名をdbとして，Railsコンテナを起動する．
docker run -i -p 3000:3000 -link pg:db -name web -t tcnksm/rails &#39;rake db:create &amp;amp;&amp;amp; rake db:migrate &amp;amp;&amp;amp; rails s&#39;  参考</description>
    </item>
    
    <item>
      <title>RubyのコマンドラインツールのMan Pageをつくる</title>
      <link>https://deeeet.com/writing/2014/03/17/gem-man/</link>
      <pubDate>Mon, 17 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/17/gem-man/</guid>
      <description>コマンドラインツールでは，--helpオプションで簡単に使い方やオプションの説明を出力する．単純に使ってもらう分にはこれで足りる．ただ，さらにそのコマンドラインツールを使ってもらいたい場合には，詳細なドキュメントや使い方の例，簡単なチュートリアルをコマンドライン上で提供できるのがよい．単純にOptparseなどでこれをやろうとすると，スペースが足りないし，ちょっとヘルプを見たいだけのユーザには邪魔になる．
伝統的なUNIXコマンドは，manコマンドを通じてそのような詳細な情報を提供している．例えば，man lsと打てば，lsコマンドの詳細が見れる．RubyでつくったコマンドラインツールでもMan Pageを通じて，同様の情報を提供できるとよい．
しかし，Rubygems.orgを通してコマンドラインツールを配信する場合，標準のmanコマンドを通じてMan pageを提供するのは難しい．GithubのChris Wanstrath氏によるgem-manを使えば，gem manコマンドを通じて標準のmanと同様のMan Pageを簡単に提供できる．
さらに，Man Pageは，nroffという専用の言語で書かれており，わざわざ習得するのはめんどい．これも，rtomayko/ronnを使えば，MarkdownでMan pageを書いて，nroff形式に変換することができる．
TL;DR  gem manでrubyでつくったコマンドラインツールのMan Pageを提供する ronnでMarkdown形式をMan Pageのnroff形式を吐き出す  以下のようなものをつくる．
$ gem man your_app NAME your_app - Sample of gem-man and ronn SYNOPSIS your_app [options] DESCRIPTION your_app is a simple command-line tool for ... OPTIONS -h, --help Show help page. etc...  Install Gemとしてgem-manとronnをインストールする．
$ gem install gem-man ronn  Rubygemsとして配布する場合は，gemspecに以下を追記する．
# your_app.gemspec spec.add_dependency &amp;quot;gem-man&amp;quot; spec.add_development_dependency &amp;quot;ronn&amp;quot;  Man Pageのソースは，プロジェクトのルートのmanディレクトリに配置し，ファイル名はyour_app.</description>
    </item>
    
    <item>
      <title>Docker Share</title>
      <link>https://deeeet.com/writing/2014/03/12/docker-share/</link>
      <pubDate>Wed, 12 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/12/docker-share/</guid>
      <description>Vagrant Shareとngrok
Vagrant Share素晴らしい．外部ネットワークのマシンから，ローカルに立てた仮想マシンへのアクセスを実現している．
TL;DR ngrokを使えば，Dockerコンテナに対してVagrant Shareと同様のことができる．つまり，Dockerコンテナを外部ネットワークからアクセス可能にすることができる．
以下をやってみた．
 Apacheコンテナへのアクセス Railsコンテナへのアクセス  準備 OSX上で行った．dockerはboot2dockerで動かす．
$ brew install boot2docker  事前にboot2dockerにport forwardingの設定をしておく．
$ VBoxManage modifyvm &amp;quot;boot2docker-vm&amp;quot; --natpf1 &amp;quot;tcp-port8080,tcp,,8080,,8080&amp;quot;  設定が終わったらしたら，boot2dockerを起動しておく．
$ boot2docker start  また，ngrokをダウンロードして適切な場所に配置しておく．
Apacheコンテナ 以下のようなDockerfileを準備する．
FROM ubuntu:12.04 RUN apt-get update RUN apt-get install -y apache2 ENV APACHE_RUN_USER www-data ENV APACHE_RUN_GROUP www-data ENV APACHE_LOG_DIR /var/log/apache2 EXPOSE 80 ENTRYPOINT [&amp;quot;/usr/sbin/apache2&amp;quot;] CMD [&amp;quot;-D&amp;quot;, &amp;quot;FOREGROUND&amp;quot;]  イメージをビルドする．
$ docker build -t apache2 .</description>
    </item>
    
    <item>
      <title>Vagrant shareとngrokを使ってみた</title>
      <link>https://deeeet.com/writing/2014/03/11/vagrant-share/</link>
      <pubDate>Tue, 11 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/11/vagrant-share/</guid>
      <description>Vagrant 1.5 and Vagrant Cloud
Vagrant shareを使ってみた．今まではprivate_networkによるローカルマシンから仮想マシンへのアクセスや，public_networkによるLAN内のマシンから仮想マシンへのアクセスが可能だった．今回のアップデートで，外部ネットワークのマシンから，ローカルに立てた仮想マシンへのアクセスが可能になった．
主なアクセスは以下の2つ．
 仮想マシン内に立てたHTTPサーバーへのアクセス SSHによる仮想マシンへのログイン  試してみた．
準備 Vagrant Cloudでアカウントを作成し，ログインする．
$ vagrant login  また，例として以下のようなVagrantfileを準備し，仮想マシンを起動しておく．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; config.vm.network :forwarded_port, guest: 80, host: 8080 config.vm.provision :shell, :inline =&amp;gt; &amp;lt;&amp;lt;-PREPARE apt-get -y update apt-get install -y apache2 PREPARE end  $ vagarnt up  （やっているのは，8080-&amp;gt;80のport forwardとapacheのインストールのみ）
HTTP Access まず，PulicなHTTP URLを介して仮想マシン内のHTTPサーバにアクセスする方法．この場合は，共有相手のマシンにVagrantがインストールされている必要はない．
以下を実行する．
$ vagrant share ... ==&amp;gt; default: Your Vagrant Share is running!</description>
    </item>
    
    <item>
      <title>OSXのVagrant box</title>
      <link>https://deeeet.com/writing/2014/03/10/osx-vagrant-box/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/10/osx-vagrant-box/</guid>
      <description>OS X on OS X Virtualize OS X Mavericks with VirtualBox  VirtualBoxで動かせるならVagrantでも使えるなと思って探してみたらやってるひといた．
 Creating an OS X Vagrant base box for VMware Fusion Troubles creating OS X 10.9 box with VirtualBox  それならPackerでもできるなと思って探してみたらやってるひといた．
 Creating an OS X Base Box for Vagrant With Packer  面白いなと思ったけど，やる気は全くない．</description>
    </item>
    
    <item>
      <title>rbdockというRuby/Rails/Sinatra用のDockerfileを生成するgemをつくった</title>
      <link>https://deeeet.com/writing/2014/03/06/rbdock/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/06/rbdock/</guid>
      <description>tcnksm/rbdock rbdock | RubyGems.org | your community gem host  実験的に作ってみた．RubyやRails，Sinatraアプリケーションを動かすためのDockerfileを生成する．
これを作った理由は，今まで自分でRuby/Rails/Sinatraのコンテナを作ってみたり，Web上のRuby+Docker関連の記事などを見ていると，どれも同じようなDockerfileを書いていたため．
さらに，Dockerの流れを見ていると，
 コンテナは必要なものだけを入れるようになりそう．つまり，RedisならRedisの，nginxならnginxの，RailsならRailsのコンテナをそれぞれ作るようになりそう． コンテナの起動やコンテナ間の連携はFigなどが受け持ってくれそう．  な雰囲気なので，Ruby/Rails/Sinatra用のコンテナをつくるためのDockerfileをつくるところに特化したツールを作ってみようと考えた．ちなみにDocker.ioには，あらかじめRubyがビルドされたイメージが上げられつつある．だから，それをそのまま使うのもありだけど，編集可能なDockerfileが手元にある方がよい．
インストール gemでインストールする．
$ gem install rbdock  インストールが完了するとrbdockというコマンドが使えるようになる．
$ rbdock --version rbdock 0.1.0  使い方（Rubyのみ） 使い方は以下．使いたいバージョンのRubyを指定するだけで，そのバージョンのRubyが使えるDockerfileが生成される．
$ rbdock &amp;lt;ruby-versions&amp;gt; [&amp;lt;args&amp;gt;]  例えば，ruby 2.1.0が使えるDockerfileを生成したい場合は以下のようにする．
$ rbdock 2.1.0  生成結果は以下．
FROM ubuntu # Install basic packages RUN apt-get update RUN apt-get install -y build-essential wget curl git RUN apt-get install -y zlib1g-dev libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt-dev RUN apt-get install -y sqlite3 libsqlite3-dev RUN apt-get clean # Install ruby-build RUN git clone https://github.</description>
    </item>
    
    <item>
      <title>Rakeのtask名にaliasを設定する</title>
      <link>https://deeeet.com/writing/2014/03/05/rake-alias/</link>
      <pubDate>Wed, 05 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/05/rake-alias/</guid>
      <description>The alias of task name in rake
シンプルなやり方．
# Rakefile namespace :db do task :table do puts &amp;quot;table&amp;quot; end end task :t =&amp;gt; [&amp;quot;db:table&amp;quot;]  $ rake t # -&amp;gt; &amp;quot;table&amp;quot;  複数のタスクを一気に登録したい場合は，以下のようなメソッドを準備する．
# Rakefile def alias_tasks tasks tasks.each do |new, old| task new, [*Rake.application[old].arg_names] =&amp;gt; [old] end end namespace :db do task :table do puts &amp;quot;table&amp;quot; end task :schema do puts &amp;quot;schema&amp;quot; end end alias_tasks [ [:dt, &amp;quot;db:table&amp;quot;], [:ds, &amp;quot;db:schema&amp;quot;] ]  $ rake ds # -&amp;gt; schema  </description>
    </item>
    
    <item>
      <title>DockerイメージのビルドにPackerを使うべき理由</title>
      <link>https://deeeet.com/writing/2014/03/03/why-building-docker-by-packer/</link>
      <pubDate>Mon, 03 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/03/why-building-docker-by-packer/</guid>
      <description>&amp;ldquo;Ask HN: Do you bake AMIs for AWS deployments?&amp;rdquo;での，Mitchell Hashimoto氏のコメントより．簡単に抄訳．
 ソフトウェアのインストールや設定の知識は，依然としてShellscriptやChef，Puppetに残っている．Packerを使えば，Dockerのコンテナの作成に現時点で存在している経験やCIプロセスなどを利用できる． 共通のフォーマットの設定．Dockerfileの記述は特有である．それは良いが，現状様々なイメージ(AMIやDockerのコンテナ，Virtualboxなど)が存在する．Dockerが全てではないとき，イメージをビルドするために様々なツールをメンテするのは負担になる．Packerを使えば，一つの方法で，さまざまなプラットフォームに対応できる．たとえ企業がDockerのみに移行しても． 移植性．Packerは低リスクでDockerのコンテナに対応できる．DockerfileはDockerのためのものである．例えばDockerが気に入らない場合や，Dockerがある状況に対して適切ではない場合に，Dockerfileは別のフォーマットに移し替えられなければならない． 拡張性．Packerは簡単にプラグインを作ることができる．Dockerは特別なコマンドの追加をサボートしていないが，Packerなら可能（それが必要かは別にして）． プロセスがシンプル．Packerのイメージのビルドプロセスは，1..Nと順に進むだけで，余計なプロセスはない．DockerのコンテナやAminatorのAMIのビルドは異なるプロセスをもつ．新しいプロセスは，CIの特別な処理や，新人への教育，新たなメンテナンスを生む．  最後にPackerの今後についても言及している．現時点では，Packerには，Dockerのようにステップごとにスナップショットをとる機能はない．そのためbuildし直すと処理ははじめからになる．しかし，現時点でそのスナップショットの機能に取り組んでおり，将来サポートされる予定らしい．これができれば，docker buildのようにpacker buildの場合も，必要なステップから処理が再開されるようになる．クールだ．
Packerを使ったdockerイメージのビルドは，&amp;ldquo;Packerを使ってChef/Puppet/AnsibleでDockerのイメージをつくる&amp;rdquo;に書いた．とても簡単．</description>
    </item>
    
    <item>
      <title>Packerを使ってChef/Puppet/AnsibleでDockerのイメージをつくる</title>
      <link>https://deeeet.com/writing/2014/03/02/build-docker-image-by-packer/</link>
      <pubDate>Sun, 02 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/02/build-docker-image-by-packer/</guid>
      <description>Packerは，Vagrantの作者であるMitchell Hashimoto氏によって開発が進められているVirtualBoxやVMWare，Amazon EC2などの仮想マシンのテンプレートの作成を行うツール．VagrantのVirtualBox用のBoxを作るveeweeに置き換わるツールとして知られている．最近のアップデートでDockerのイメージのビルドをサポートした．
TL;DR Packerを使えばDockerのイメージをDockerfileを使わずビルドすることができる
つまり，Dockerfileの特有な記述を使わず，今まで慣れ親しんできたChefやPuppet，Ansibleのようなプロビジョニングツールを使ってDockerのイメージをビルドできる．
参考
 DockerイメージのビルドにPackerを使うべき理由  サンプル tcnksm/packer-docker
サンプルコードは全て上のレポジトリにある．ChefとPuppetとAnsibleで最小限で単純なサンプルを試した．
準備（Vagrantfile） 実行はすべてVagrantのVM上で行う．Vagrantの1.4以上がインストールされていれば，以下のVagrantfileを使えばすぐに試せる．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; # config.vm.provision :docker do |d| d.pull_images &amp;quot;ubuntu&amp;quot; end config.vm.provision :shell, :inline =&amp;gt; &amp;lt;&amp;lt;-PREPARE apt-get -y update apt-get install -y wget unzip curl mkdir /home/vagrant/packer cd /home/vagrant/packer wget https://dl.bintray.com/mitchellh/packer/0.5.2_linux_amd64.zip unzip 0.5.2_linux_amd64.zip echo &amp;quot;export PATH=$PATH:/home/vagrant/packer&amp;quot; &amp;gt; /home/vagrant/.bashrc PREPARE end  DockerとPackerの最新版のインストールしているだけ（注: PackerはOSX上でも動くが，OSXのDocker Clientのバグのためにうまく連携できなったので，PackerもVagrant上で実行している）．
準備（Chef） ここでは例として，apacheのインストールを行うクックブックとレシピを準備する．
$ knife cookbook create apache -o site-cookbooks  # site-cookbooks/apache/recipes/default.</description>
    </item>
    
    <item>
      <title>Packer雑感</title>
      <link>https://deeeet.com/writing/2014/03/02/packer/</link>
      <pubDate>Sun, 02 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/03/02/packer/</guid>
      <description>Packerを使ってChef/Puppet/AnsibleでDockerのイメージをつくる
で初めてPackerを使った．そのときの思ったことをざっと書き留めておく．
Packerは，Vagrantの作者であるMitchell Hashimoto氏によって開発が進められているVirtualBoxやVMWare，Amazon EC2などの仮想マシンのイメージの作成を行うツール．VagrantのVirtualBox用のBoxを作るveeweeに置き換わるツールとして知られている．
リリース時からPackerはVagrantのBoxを作る専用ツールとしてのイメージが強かった．実際，box作るときはベースboxを基にvagrantのプロビジョニング機能を使ってvagrant packageで済むし，ヘビーにVagrantを使うユーザのためのツールだと思っていた．また，ネット上にあるPacker関連の記事はこの用途を対象にしたものが多い．
だから，今回も基礎としてVagrantのboxの作成からやり始めた．が，正直途中で心が折れた．というのも，VirtualboxのVagrantのboxを作るときはisoイメージをベースに始めるため，KickStartとかPreseedの知識を要求される．その辺りはなじみがなくて辛かった．Redhat系のKickstartはまだ読めたが，Debian系のPreseedはしんどかった．Packerとは関係のないところで，理解が追いつかなかったために，Packerそのものの理解にたどり着くのが大変だった．
ただ，Dockerのイメージ作成やAmazon EC2のAMIの作成をやり始めると，VirtualBoxのbox作成と比べてシンプルであり，なじみもあるため理解が進んだ．そして，Packerはものすごくシンプルかつ強力なツールだということがわかった．Packerを自分なりの言葉でまとめてみると，
 builderで仮想マシンのテンプレート（ベース）の設定をする． ShellscriptやChef，Puppet，Ansibleで共通のプロビジョニングをする． post-processorで決められた形式（boxかdocker imageか）で仮想マシンを書き出す．  の一連の流れを自動化するツール．このとき1)と3)は基本的にはテンプレで，大体やることは決まっていると思う．自由度があるのは2)で，どのプロビジョニングツールを使って，どのようなセットアップをするかをユーザが自由に決めることができる．ちょっとしか触ってないが，2)ができることがPackerの強力な部分ではないかを思っている．
まとめると，Packerの入門をするときは，Vagrantの用のboxを作るツールという概念を捨てて，DockerイメージやAMIから始めると良いと思う．そうすれば，Packer特有の概念である，builderやpost-processorsが理解しやすい．
最後に参考文献．Packerの日本語の記事だとryuzeeさんが既に駆け抜けている．&amp;ldquo;実践Vagrant&amp;rdquo;にもPackerの付録があり，理解の助けになった．</description>
    </item>
    
    <item>
      <title>&#34;実践Vagrant&#34;を読んだ</title>
      <link>https://deeeet.com/writing/2014/02/25/vagrant-up-and-running/</link>
      <pubDate>Tue, 25 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/25/vagrant-up-and-running/</guid>
      <description>O&amp;rsquo;Reilly Japan - 実践 Vagrant
Vagrantは普通に問題なく使えているし，本をわざわざ読む必要もないと思ったが，以下のようなモチベーションで購入．
 Mitchell Hashimoto氏の設計思想的な部分を知りたかった プラグインをつくりたかった 落ち穂拾い  まず，設計思想．1章に&amp;rdquo;Vagrant道&amp;rdquo;という節があり，ユースケースというか，Vagrantを使った高レベルなワークフローが説明されている．開発者や運用技術者からみて，普段のプロジェクトの中でVagrantがどのような役割を果たすのかが簡単にまとめられている．Vagrantが近年の開発環境にあまりに自然に入り込んできたのは，このようなビジョンがあってからこそだと思う．誰もが理解できるビジョンを掲げ，それをコードに落とし込むところがMitchell氏のすごさなんだと改めて認識した．開発者としても，ビジョン-&amp;gt;コードの流れを参考にしたい．
プラグインの開発 次にプラグイン．これを読んでからいくつかプラグインを作ってみた．公開したのは以下．
 プロビジョニングの終了をiOS/Androidに通知するVagrantのpluginつくった 他人に共有したくない設定をVagrantfileに書くためのpluginつくった  プラグインを作るのはとても簡単．本書でカバーされているのは，以下．
 新しいサブコマンドの開発 (command) 新しい設定オプションの開発 (config) config.vm.provision :dockerのような新しいプロビジョナーの開発 (provisioner) vagrant upのような既存動作を変更 (hook)  プラグインはrubygemsとして開発する．rubygemを作ったことがあるひとであればすんなりと開発できる．詳しくは書かないが，基本はrubyのDSLによる簡単な記述し，そのDSLの戻り値を決められたメソッドを実装したクラスにするだけ．
すこしハマった部分．本書や公式ドキュメントだとVagrantfileにVagrant.require_plugin &amp;quot;my_plugin&amp;quot;を記述してプラグインのテストを行うように書かれているが，自分の環境だとうまく動かなかった．そのため，プラグインをビルドして，直接システムのVagrantに読み込ませてテストを行った．毎回コマンド打つのは億劫なので，以下のようなRakeタスクを作った．
require &amp;quot;vagrant-secret&amp;quot; require &amp;quot;bundler/gem_tasks&amp;quot; version = VagrantPlugins::Secret::VERSION desc &amp;quot;Install plugin to system vagrant.&amp;quot; task :install_plugin do system(&amp;quot;git add -u&amp;quot;) Rake::Task[:build].execute system(&amp;quot;/usr/bin/vagrant plugin install pkg/vagrant-secret-#{version}.gem&amp;quot;) end desc &amp;quot;Uninstall plugin&amp;quot; task :uninstall_plugin do system(&amp;quot;/usr/bin/vagrant plugin uninstall vagrant-secret&amp;quot;) end # alias task :i =&amp;gt; :install_plugin task :u =&amp;gt; :uninstall_plugin  ちょっとTips．プラグイン定義の先頭に以下を記述して，プラグインが実際にVagrantの中で動作しているかを確認するのがよい．これは，本書のコラムに書かれていた内容．他のVagrant pluginのプロジェクトにも採用されている．</description>
    </item>
    
    <item>
      <title>VagrantでOracle DB環境をつくる</title>
      <link>https://deeeet.com/writing/2014/02/24/vagrant-oracle-db/</link>
      <pubDate>Mon, 24 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/24/vagrant-oracle-db/</guid>
      <description>hilverd/vagrant-ubuntu-oracle-xe
を使う． VirtualBox Guest Additionsを使うため，事前にvagrant-vbguestをインストールしておく．あとは，hilverd/vagrant-ubuntu-oracle-xeをcloneしてvagrant upするだけ．</description>
    </item>
    
    <item>
      <title>他人に共有したくない設定をVagrantfileに書くためのpluginつくった</title>
      <link>https://deeeet.com/writing/2014/02/24/vagrant-secret/</link>
      <pubDate>Mon, 24 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/24/vagrant-secret/</guid>
      <description> （追記）dotenvというもっと便利なツールがありました．僕のは，pluginを作る際の参考にしてください．dotenvとvagrantの連携は，glidenoteさんの&amp;ldquo;dotenvを利用して環境ごとでVagrantfileの設定値を変更してみる&amp;rdquo;を参考にしてください．
vagrant-secret
例えば，VagrantでDigital Oceanを使う場合，以下のようにclient_idやapi_keyのような他人には共有したくない設定をVagrantfileに記述する．
Vagrant.configure(&#39;2&#39;) do |config| config.vm.provider :digital_ocean do |provider, override| provider.client_id = &#39;****&#39; provider.api_key = &#39;****&#39; end end  ローカルで自分だけが使う場合は問題ないが，Githubに上げて他人に共有した場合は面倒になる．vagrant-secretを使えば，専用のyamlファイルに設定を分けて記述することができる．
インストール Vagrantのpluginとしてインストールする．
$ vagrant plugin install vagrant-secret  使い方 まず，以下のコマンドで設定ファイルを書き出す．
$ vagrant secret-init  すると，.vagrant/secret.yamlが生成されるので，そこに公開したくない設定を記述する．例えば，以下のようにDigital Oceanで必要なclient_idとapi_keyを記述する．
client_id: &amp;quot;*******&amp;quot; api_key: &amp;quot;********&amp;quot;  後は，これらをVagrantfileで使うだけ．yamlのkeyがSecretという専用のクラスのクラス変数に割り当てられ，それを通してvalueを取り出すことができる．
Vagrant.configure(&#39;2&#39;) do |config| config.vm.provider :digital_ocean do |provider, override| provider.client_id = Secret.client_id provider.api_key = Secret.api_key end end  .vagrant/以下は普通はgitignoreするので，secret.yamlをわざわざgitignoreする必要はない．
バグなどはGithubのissueか@deeeetまでお願いします．
参考
 VagrantとSSDなVPS(Digital Ocean)で1時間1円の使い捨て高速サーバ環境を構築する プロビジョニングの終了をiOS/Androidに通知するVagrantのpluginつくった  </description>
    </item>
    
    <item>
      <title>プロビジョニングの終了をiOS/Androidに通知するVagrantのpluginつくった</title>
      <link>https://deeeet.com/writing/2014/02/19/vagrant-pushover/</link>
      <pubDate>Wed, 19 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/19/vagrant-pushover/</guid>
      <description>tcnksm/vagrant-pushover
Vagrantのプロビジョニングはものすごく時間がかかる．その時間を使って美味しい珈琲を淹れたい．でも，席を外したらいつプロビジョニングが終わったかわからない．プロビジョニングの終了を告げる通知が欲しい．
少し前からPushoverというiOS/Androidアプリで遊んでいる．シンプルなHTTP POSTを介してアプリに通知が送れる（詳しくは，&amp;ldquo;Pushover使ってみた&amp;rdquo;に書いた）．メールで通知でもよかったんだけど，せっかくなので，Pushoverに通知が送れるVagrantプラグインを作った．
以下のような通知を受け取ることができる．
インストール Vagrantのプラグインとしてインストールする．
$ vagrant plugin install vagrant-pushover  使い方 以下のようにVagrantfileに設定を記述するだけ．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.pushover.set do |p| p.user = &amp;quot;YOUR KEY&amp;quot; p.token = &amp;quot;YOUR APP TOKEN&amp;quot; end end  userはPushoverにサインアップ後にDashboardから，tokenはアプリケーションの登録をすると取得できる．
既存コマンドのフックとして記述してあるので，他に特別なことをする必要はなく，いつも通りにコマンドを実行するだけでよい．通知が行われるのは，プロビジョニングが行われる以下の場合のみ．
 vagrant up vagrant up --provision vagrant reload --provision vagrant provision  ただしvagrant upは，マシンの状態が:runningでない場合，プロビジョニングが一度も行われていない場合に通知が行われる．
他にも，通知のメッセージのタイトルや本文，通知音もVagrantfileの設定から行うことができる．API通りに実装してあるので，詳しくは公式ドキュメントを参照してください．
Vagrantfileを共有したい場合 tokenやuserがベタ書きされているのはよろしくないので，別ファイルとして記述できるようにもしてある．
以下のコマンドで設定ファイルを吐き出す．
$ vagrant pushover-init  設定ファイルは，.vagrant/pushover.rbとして吐き出されるので，中身を編集してtokenとuserを記述する．設定の読み込みを有効にするには，Vagrantfileを以下のように記述するだけ．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.pushover.read_key end  以上．
Vagrantのプラグインを作るのはとても簡単（実際2日もかかってない）．また，プラグインを作ることでVagrantの内部でどのようなことが行われているのか大分理解できる．作成には，&amp;ldquo;実践Vagrant&amp;rdquo;を参考にした．公式ドキュメントも充実している．すべてのVagrantのコマンドはプラグインとして実装されているので，作るときは参考になる．Mitchell Hashimotoさんが自らつくったプラグインvagrant-awsも参考になった．プラグインの作り方は，そのうちまとめる．
Pushoverを使ってるひとにしか使えないかなりニッチなプラグインだけど，プロビジョニング中に美味しい珈琲を淹れたいひとは是非使ってください．バグなどはGithubのissueか@deeeetまでお願いします．
参考</description>
    </item>
    
    <item>
      <title>シンプルにVagrantのprovisioningでchef-soloを使い始める</title>
      <link>https://deeeet.com/writing/2014/02/16/vagrant-chef-minimum/</link>
      <pubDate>Sun, 16 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/16/vagrant-chef-minimum/</guid>
      <description>とにかくシンプルに始めたい．cookbookの作成にはknife-soloを使う．例としてapacheのインストールをして，共有フォルダをホストのブラウザから閲覧できるようにする．
まずレシピの雛形を生成する．
$ knife cookbook create apache -o site-cookbook  次にレシピの編集する．
# recipes/default.rb # Install apache execute &amp;quot;apt-get update&amp;quot; package &amp;quot;apache2&amp;quot; do action :install end # Link to share folder execute &amp;quot;rm -fr /var/www&amp;quot; link &amp;quot;var/www&amp;quot; do to &amp;quot;/vagrant&amp;quot; end  Vagrantfileは以下のようにする．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; config.vm.network :forwarded_port, guest: 80, host: 8080 config.vm.provision :chef_solo do |chef| chef.cookbooks_path = &amp;quot;site-cookbooks&amp;quot; chef.run_list = [&amp;quot;apache::default&amp;quot;] end end  </description>
    </item>
    
    <item>
      <title>Rakefileに加えるべき7行</title>
      <link>https://deeeet.com/writing/2014/02/15/7lines-rakefile/</link>
      <pubDate>Sat, 15 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/15/7lines-rakefile/</guid>
      <description>7 Lines Every Gem&amp;rsquo;s Rakefile Should Have
必要なgemをrequireしてすぐに使えるようにしましょうと．自分はpryで．
task :console do require &#39;pry&#39; requrey &#39;my_gem&#39; ARGV.clear Pry.start end  OSSだとpry使ってないひともいる可能性があるからirbでやったほうがいいかも．</description>
    </item>
    
    <item>
      <title>Gitのcommitメッセージに定型文をぶっ込む</title>
      <link>https://deeeet.com/writing/2014/02/12/git-cmb/</link>
      <pubDate>Wed, 12 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/12/git-cmb/</guid>
      <description>Gitのcommitメッセージに定型文をぶっ込みたいときがある．例えば$defaultのような環境変数が設定されていて，それを自動でcommitメッセージに含めるようにしたいとする．
以下のようなaliasを設定する．
[alias] cmd = &amp;quot;!f () { git commit -m \&amp;quot;$1 ($default)\&amp;quot; $2;}; f&amp;quot;  $ export default=&amp;quot;default message&amp;quot; $ git cmd &amp;quot;Some changes&amp;quot; # -&amp;gt; git commit -m &amp;quot;Some changes (default message)&amp;quot;  f()という関数を定義してそれを最後に実行する．こんな回りくどいやり方をするのは，gitのaliasが最後に&amp;quot;$@&amp;quot;をつけてサブコマンド以外の引数をすべて展開するためで，単純に$1で第一引数を取得して&amp;hellip;では期待通りにはならない．関数定義して最後にそれを実行すれば，gitがつける&amp;quot;$@&amp;quot;をその関数の引数として使える．
自分のチームだとStashとJIRAを使っていて，JIRAのチケット名をcommitメッセージに含めるとStash上でJIRAのIssueとリンクされる．慣習としてbranch名とJIRAのチケット名を同じにしているので，ブランチ名をcommitメッセージに含めることができれば少し楽．
例えば，JIRAのissueの名前がxxx-1456とするとこんな感じのことをする．
$ git checkout -b xxx-1456 (Some changes) $ git commit -m &amp;quot;Some changes (xxx-1456)&amp;quot;  以下ようなaliasを設定すれば，自動でブランチ名を最後に含めたcommitメッセージが作られる．
cmb = &amp;quot;!f () { git commit -m \&amp;quot;$1 ($(git branch -a | grep &#39;^*&#39; | cut -b 3-))\&amp;quot; $2;}; f&amp;quot;  $ git checkout -b xxx-1456 (Some changes) $ git cmb &amp;quot;Some changes&amp;quot; # -&amp;gt; git commit -m &amp;quot;Some changes (xxx-1456)&amp;quot;  かなり希有なパターンだけど，他にも応用できるかなと．ちなみにexport GIT_TRACE=1すると，gitが何やってるのか細かく見れるので複雑なaliasを作るとき便利．</description>
    </item>
    
    <item>
      <title>gitshでgitのタイプ数を減らす</title>
      <link>https://deeeet.com/writing/2014/02/11/gitsh/</link>
      <pubDate>Tue, 11 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/11/gitsh/</guid>
      <description>gitはサブコマンドやオプションが多い．だからshellのaliasやらgitのaliasで頑張ってコマンドのタイプ数を減らす．Thoughbotの@georgebrockさんのgitshを使えばもっとコマンドのタイプ数を減らすことができる．
例えば以下はよく打つコマンド．gitって打ち過ぎ．
$ git status $ git add -u $ git commit -m &amp;quot;Commit message&amp;quot; $ git push  gitshを使うと専用のモードへのアタッチが始まり，gitを打たずサブコマンドだけを打てばよくなる．
$ gitsh gitsh@ status gitsh@ add -u gitsh@ commit -m &amp;quot;Commit message&amp;quot; gitsh@ push gitsh@ :exit  gitのaliasも引き継がれるので，例えば以下のようなaliasを設定してしておくと，さらにタイプ数を減らすことができる．
#.gitconfig [alias] au = add -u cm = commit -m p = push  $ gitsh gitsh@ ⏎ # status gitsh@ au gitsh@ cm &amp;quot;Commit message&amp;quot; gitsh@ p gitsh@ :exit  サブコマンドなしで⏎ (return)するとデフォルトではgit statusが表示される．デフォルのコマンドは以下で変更可能．</description>
    </item>
    
    <item>
      <title>Pushover使ってみた</title>
      <link>https://deeeet.com/writing/2014/02/09/pushover/</link>
      <pubDate>Sun, 09 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/09/pushover/</guid>
      <description>Pushover
PushoverはiOS/Androidの通知アプリ．提供されるシンプルなAPIを介して，アプリに通知を送れる．HTTP POSTさえできればよいので，Shell Scriptからでもrubyやperlからでも簡単に通知が送れる．
Rubyを使って遊んでみた．
require &#39;net/https&#39; url = URI.parse(&amp;quot;https://api.pushover.net/1/messages.json&amp;quot;) req = Net::HTTP::Post.new(url.path) req.set_form_data({ token: &amp;quot;****&amp;quot;, user: &amp;quot;****&amp;quot;, message: &amp;quot;Check this link, http://deeeet.com/&amp;quot;, title: &amp;quot;お知らせ&amp;quot;, device: &amp;quot;tcnksm_iphone&amp;quot;, url: &amp;quot;tel:117&amp;quot;, url_title: &amp;quot;Call now&amp;quot;, sound: &amp;quot;alien&amp;quot; }) res = Net::HTTP.new(url.host, url.port) res.use_ssl = true res.verify_mode = OpenSSL::SSL::VERIFY_PEER res.start {|http| http.request(req)}  userはsign upするともらえるUser Key．tokenはアプリを登録するともらえるAPI Token. messageは通知の本文． この3つが必須でこれだけでも通知は可能．
titleは通知のタイトル．指定しない場合は登録したアプリ名が使われる．deviceは通知したいデバイスを指定する．指定しない場合は登録されている全てのデバイスに通知される．
通知は以下のような感じ．
通知をタップすると，詳細が表示される．
通知本文内のURLは自動で判別されるため，ブラウザで開くことができる．urlとurl_titleパラメータを与えれば，追加でURLを送ることもできる．これで長いURLも送ることができるし，アプリのURLスキームも使えるので，例えば上のようにtel:117とすれば，タップでそのまま電話アプリを開いて117に電話するなんてこともできる（117はありえんが）．またsoundパラメータで通知音を変更することもできる．使える音はここにある．
priorityパラメータで通知のレベルも変更することができる．レベルは，Low(-1)，Normal(0)，High(1)，Emergency(2)の4つがある．
 Low: 通知は送られるが，通知音は鳴らない Normal: デフォルト．自分で設定するQuiet Hoursの間は通知音は鳴らない High: Quiet Hoursであっても通知音が鳴る Emergency: 通知内の確認ボタンをタップするまで通知音が鳴り続ける  例えば通知のレベルをHighにすると，通知は以下のように赤色になる．</description>
    </item>
    
    <item>
      <title>このプレゼン資料はさっき完成したばかりです</title>
      <link>https://deeeet.com/writing/2014/02/07/stupid-presentation/</link>
      <pubDate>Fri, 07 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/07/stupid-presentation/</guid>
      <description>と言われるとそのプレゼンを聞く気は一瞬で失せる．それは「このプレゼンの質は低いです」と宣言されているのと同じだからだ．そんなもの誰が聞きたいと思うだろうか．百戦錬磨の超一流のしゃべり手なら，それでも素晴らしいプレゼンをするだろう．でも大抵の人は違う．よほど聞きたい話題でないと，その低品質なプレゼンを苦労して読み解く気にはならない．そんなことを宣言するのはしゃべり手の傲慢，もしくは忙しいアピールにしか聞こえない．「やべーまだ明日のプレゼンの資料できてないわー」は，「明日の試験の勉強まだしてないわー」に似ている．
資料を作り上げるのをゴールであると思ってるひとも多いと思う．つまり，本番までに一度もその資料を使って「しゃべり」の練習をしていないひとのことだ．そんなもの一発でわかる．分かった瞬間聞く気は失せる．資料つくってそのまま発表するのは，コード書いて，テストなしに，リファクタリングなしに，本番にでプロイするのと同じだ．よほどの技術者でないとそんなことはできない．というか，素晴らしい技術者ほど，しっかりとしたテストを書き，リファクタリングをして，やっとデプロイするだろう．つまり，素晴らしいしゃべり手ほど，資料をつくるだけでなく，資料を作ったあとに実際にそれを使ってしゃべりの練習をし，しゃべりに詰まれば，しゃべりやすいように，よりわかりやすいように資料としゃべりを改善しているということ．資料の完成は，プレゼン準備のスタート地点にたった状態と思わないといけない．
聞き手のことを考えない傲慢なプレゼンがどれだけ多いことか．聞き手の貴重な時間を使っていることに自覚のないしゃべり手がどれだけ多いことか．クソなプレゼンを吐き出しつつけるならもう止めた方がいい，時間の無駄だ．その時間聞き手はみなtwitterを見て世界が平和であるかをチェックしてるのだから．
自分への戒めも込めて．</description>
    </item>
    
    <item>
      <title>Middlemanを使ってみた</title>
      <link>https://deeeet.com/writing/2014/02/05/middleman/</link>
      <pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/05/middleman/</guid>
      <description>deeeet.com
Middlemanは，HamlとSass, Compassがデフォルトで使えるため，簡単にいい感じのサイトをつくることができる．例えば，Packerの公式サイトなどMiddlemanで作られていてとてもいい感じだ（Githubのレポジトリみるかぎり，Mitchellさんデザインも自分でやっている&amp;hellip;?）．他にも個人のBlogをMiddlemanで作ってるひともいる．今後，簡単なサイト立てるときはまた使いそうなので，まとめておく．
HamlやSassを使うために特別に設定を書く必要はない．ただ拡張子を，Hamlの場合は，.html.haml（laytoutファイルは.haml）に，Sassの場合は，.scssにしておけばよい．buildの際は，HamlはhtmlにSassはcssとして生成される．Sassは:css_dirのアンダースコアで始まらないファイルがcssとして吐き出される（つまり@importされるpartialなどは無視される）．
設定ファイルであるconfig.rbでは，Liveloadを有効にしておくと，ソースを更新する度に，ブラウザで再読み込みをしてくれるので便利．
activate :livereload  デプロイなどはOctopressと同じように以下のようなRakeタスクを作っておくと楽になる．
ssh_user = &amp;quot;&amp;quot; ssh_port = &amp;quot;&amp;quot; document_root = &amp;quot;~/www/&amp;quot; rsync_args = &amp;quot;&amp;quot; source = &amp;quot;build&amp;quot; desc &amp;quot;Build middleman&amp;quot; task :build do system(&amp;quot;middleman build&amp;quot;) end desc &amp;quot;Deploy website via rsync&amp;quot; task :deploy do puts &amp;quot;## Deploying website via Rsync&amp;quot; system(&amp;quot;rsync -avze &#39;ssh -p #{ssh_port}&#39; #{exclude} #{rsync_args} #{&amp;quot;--delete&amp;quot; unless rsync_delete == false} #{source}/ #{ssh_user}:#{document_root}&amp;quot;) end desc &amp;quot;Build &amp;amp; deploy&amp;quot; task :gen_deploy do Rake::Task[&amp;quot;build&amp;quot;].invoke Rake::Task[&amp;quot;deploy&amp;quot;].</description>
    </item>
    
    <item>
      <title>初めてのOSSコミットはhomebrew-caskがいい感じ</title>
      <link>https://deeeet.com/writing/2014/02/05/first-oss/</link>
      <pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/02/05/first-oss/</guid>
      <description>BrewfileでHomebrewパッケージを管理するにも書いたが，homebrew-caskというのは，本家Homebrewではインストールできない，Google ChromeやVagrantといったdmg配布のアプリをBrewでインストール可能にするサイドプロジェクト．
先日koboのデスクトップアプリで本を読めるようになったが，まだhomebrew-caskではインストールできない状態だった．ということで，homebrew-cask，厳密にはhomebrew-caskで管理されるアプリの派生バージョンを管理しているhomebrew-cask-versionsに新しいCaskの追加のPull requestを送ってマージされた．この時，homebrew-caskは初めてのOSSコミットの経験にとても良いと感じた．
理由は以下の2点．
 Contributeの仕方がとても丁寧にまとめてあること（CONTRIBUTING.md） Caskの追加はとても単純であること  1つ目は読めばわかるけど，cloneからpull requestまでこうしてくださいってのがとても丁寧に書いてある．コミットメッセージはこう書くべきだというアドバイスまである．2つ目の理由として，新しいCask（追加したいアプリのインストール元のURLやバージョンなどを記載する，Chefのレシピのようなもの）の追加は，rubyの単純なDSLを書くだけでいい．今回だとこんな感じ．しかも個々のCaskは独立しているので，conflictしたらどうしようとか，余計なことを考える必要がない．
これらの理由から，gitとGithubを使ったOSSコミットの一通りのフロー，cloneから，commitして，pullして最新版に追従して，squashして，適切なコミットメッセージ書いて，pushして，pull requestして，issueでやりとりして(今回だとこんな感じ)，mergeされるまで，を経験するのにhomebrew-caskはとてもおすすめ．
自分が使ってるアプリでまだCaskに登録されてなければ，是非一度経験してみるといいと思う．
ちなみに，こんなことをtwitterでつぶやいていたらhomebrew-caskの作者にも同意された．
参考
 A Note About Git Commit Messages  </description>
    </item>
    
    <item>
      <title>OSXでboot2dockerを使う</title>
      <link>https://deeeet.com/writing/2014/01/28/boot2docker-osx/</link>
      <pubDate>Tue, 28 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/01/28/boot2docker-osx/</guid>
      <description>公式のDocker Client for OSXがリリースされて，OSXでDockerを使うのはちょっと楽になった．ただ，Docker自体はVritualBoxなどのVM上で実行する必要があり，VMの起動には時間がかかるので寿命が縮む．boot2dockerを使うと，他と比べて断然早くVMを起動でき，すぐにDockerが使える．
boot2dockerというのは，Tiny Core LinuxをベースにしたDocker実行のみに特化した軽量版のLinuxディストリビューション．特化しているため起動はとても速い．前からあるが，VirtualBoxをわざわざ起動する必要があったりなど，ちょっと使うのはめんどくさかった．
Vagrantの作者であるMitchell HashimotoさんがPackerを使ってboot2dockerのVagrant Boxを作ったため，Vagrant経由で簡単にboot2dockerを使うことができるようになった．
ということで，使ってみた．
tcnksm/boot2docker-osx
Vagrantfileは以下．
DOCKER_PORT = 5422 Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;boot2docker-0.4.0&amp;quot; config.vm.box_url = &amp;quot;https://github.com/mitchellh/boot2docker-vagrant-box/releases/download/v0.4.0/boot2docker.box&amp;quot; config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: DOCKER_PORT, host: DOCKER_PORT config.vm.provision :shell, :inline =&amp;gt; &amp;lt;&amp;lt;-PREPARE INITD=/usr/local/etc/init.d/docker sudo sed -i -e &#39;s/docker -d .* $EXPOSE_ALL/docker -d -H 0.0.0.0:#{DOCKER_PORT}/&#39; $INITD sudo $INITD restart PREPARE end  OSXのDockerクライアントを使うためにport fowardingの設定と，Docker deamonのバインドアドレスの変更のみをしている．
あとは起動（vagrant up）するだけ，大体20秒くらいで立ち上がる．例えば，Ubuntu precise 64 box+docker provisionと比べると，半分以下の時間で立ち上がる．
問題がないわけではなく，いくつかVagrantのprovisioningが使えない．例えば，自分が触った中では，private IPの設定やdocker provisioningなどが使えなかった．imageのビルドもちょっと遅い．それでも起動は速いので，とりあえず軽くコマンド叩きたいとか，で使うのが良さそう．がっつり開発するときは，普通のBox使ってる．
ちなみに，OSXからDockerを使うためのヘルパーはたくさん出てきている．例えば，docker-osxや，今回のboot2dockerを使ったdvmなどがある．でも，今回のように簡単なVagrantfileさえ準備できれば簡単に使えるから自分は使わないかなと．</description>
    </item>
    
    <item>
      <title>公式のDocker client for OSXがリリース</title>
      <link>https://deeeet.com/writing/2014/01/10/docker-from-osx/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/01/10/docker-from-osx/</guid>
      <description>2014.01.02にOSXのdocker clientがリリースされた．DockerはGoで書かれているので，OSX上で自分でビルドして使ってる人もいたが，今回は公式のバイナリリリース．さらに，Homebrewのhomebrew-binaryレポジトリにFormulaも追加され，すぐに使えるようになった．
clientなので，VMもしくはリモートに立てたDocker deamonに対してローカルからコマンドが叩けるようになったということ．とりあえず，ローカルにVM立てて触ってみた．
tcnksm/docker-osx
まず，dokcer clientのインストール．
$ brew update $ brew tap homebrew/binary $ brew install docker  Vagrantfileは以下のようにする（VagrantはDocker provisioningが有効な1.4以上を使うこと）．
#Vagrantfile DOCKER_URL = &amp;quot;192.168.50.4&amp;quot; DOCKER_PORT = 5422 Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; config.vm.network :private_network, ip: DOCKER_URL config.vm.provision :docker do |d| d.pull_images &amp;quot;base&amp;quot; end config.vm.provision :shell, :inline =&amp;gt; &amp;lt;&amp;lt;-PREPARE sudo sed -i -e &#39;s/DOCKER_OPTS=/DOCKER_OPTS=\&amp;quot;-H 0.0.0.0:#{DOCKER_PORT}\&amp;quot;/g&#39; /etc/init/docker.conf sudo service docker stop sudo service docker start PREPARE end  やってることは以下．</description>
    </item>
    
    <item>
      <title>serverspecとdocker-apiでDockerfileをTDD</title>
      <link>https://deeeet.com/writing/2014/01/06/tdd-dockerfile/</link>
      <pubDate>Mon, 06 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2014/01/06/tdd-dockerfile/</guid>
      <description>いくつかDockerfileを書いてきた．今書いているDockerfileは短くてシンプルなものばかりだが，もっと長く複雑化した時に不安になりそうだ．不安を解消するにはテストしかない．さらにテスト駆動的にDockerイメージを開発できたら素敵だ．つまり，
 テストを書く Dockerイメージを作成して，テストの実行 -&amp;gt; RED Dockerfileの編集 Dockerイメージを作成して，テストの実行 -&amp;gt; GREEN テストを&amp;hellip;  の流れができるとよい．
ということで，RSpecを使ってTDDでDockerfileを開発するというのをやってみた，tcnksm/docker-rspec．今回実現したのは以下．
 Docker Remote APIでDockerfile特有のコマンド(e.g, CMDやEXPOSE)のRSpecテスト serverspecでパッケージのインストールのRSpecテスト  これらをOSX上からやれるようにした．これでDockerfileの記述内容は網羅的にテストできると思う．
準備 (Vagrant) 今回は，VagrantのVM上でDockerを動かす．OSはUbuntu12.04．Vagrantfileは以下．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.box_url = &amp;quot;http://files.vagrantup.com/precise64.box&amp;quot; config.vm.network :private_network, ip: &amp;quot;192.168.50.4&amp;quot; config.vm.provision :docker do |d| d.pull_images &amp;quot;base&amp;quot; end end  やっていることは以下
 Vagrant VMにIPアドレス&amp;rdquo;192.168.50.4&amp;rdquo;を割り当て Dockerのbaseイメージをpull  Vagrant VMはあらかじめ起動しておく．
vagrant up  また，Vagrant VMへのsshの設定を書き出しておく．
vagrant ssh-config --host docker-vm &amp;gt;&amp;gt; ~/.ssh/config  Docker Remote APIによるテスト まず，Docker Remote APIを使って，Dockerfile特有のテストをする．例えば，イメージが存在しているか，外部に向けたポートが設定されているか(EXPOSE)など．これは，主にExperimenting with test driven development for dockerを参考にした．</description>
    </item>
    
    <item>
      <title>Best Music 2013</title>
      <link>https://deeeet.com/writing/2013/12/31/music-2013/</link>
      <pubDate>Tue, 31 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/31/music-2013/</guid>
      <description> This year, I listened many new relases. I selected best 10 musics from which I listened this year.
 Ryouji Ikeda &amp;ldquo;Supercodex&amp;rdquo; Oneothrix Point Never &amp;ldquo;R Plus Seven&amp;rdquo; Holden &amp;ldquo;the Inheritors&amp;rdquo; Forest Sword &amp;ldquo;Engravings&amp;rdquo; Washed Out &amp;ldquo;Paracosm&amp;rdquo; Shigeto &amp;ldquo;No Better Time Than Now&amp;rdquo; ATOM ™ &amp;ldquo;HD&amp;rdquo; ((さらうんど)) &amp;ldquo;New Age&amp;rdquo; Majical Cloudz &amp;ldquo;Impersonator&amp;rdquo; Darkstar &amp;ldquo;News From Nowhere&amp;rdquo;  Reference  Best Music 2012  </description>
    </item>
    
    <item>
      <title>Vagrant &#43; DockerでSinatraを動かす</title>
      <link>https://deeeet.com/writing/2013/12/27/sinatra-on-docker/</link>
      <pubDate>Fri, 27 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/27/sinatra-on-docker/</guid>
      <description>tcnksm/docker-sinatra
簡単なsinatraアプリケーションをDocker上で動かしてみた．
まずはsinatraアプリケーション．特別なことはなく，Procfileとconfig.ruを準備して，foremanで動かす．外部からのアクセスを有効にするため，ListenAddressを指定しておく．
#Procfile web: bundle exec rackup config.ru -p 4567 -s thin -o 0.0.0.0  次に，Vagrantの設定．VagrantはDockerのprovisioningが有効な1.4を利用する．vagrantのインストールは以下のBrewfileを準備して，brew bundleする．
tap phinze/homebrew-cask install brew-cask cask install virtualbox cask install vagrant  Vagrantfileは以下．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.network :forwarded_port, guest: 4567, host: 4567 config.vm.provision :docker do |d| d.pull_images &amp;quot;ubuntu&amp;quot; end end  Port fowardでホストからアクセス可能なポート番号を指定しておく．後は，dockerのprovisioningでubuntuイメージを取得しておく．
次に，Dockerイメージの作成．Dockerfileは以下．
FROM base # Install packages for building ruby RUN apt-get update RUN apt-get install -y --force-yes build-essential wget git RUN apt-get install -y --force-yes zlib1g-dev libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt-dev RUN apt-get clean # Install ruby RUN wget -P /root/src ftp://ftp.</description>
    </item>
    
    <item>
      <title>BrewfileでHomebrewパッケージを管理する</title>
      <link>https://deeeet.com/writing/2013/12/23/brewfile/</link>
      <pubDate>Mon, 23 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/23/brewfile/</guid>
      <description>この記事は1分で実現できる有用な技術 Advent Calendar 2013の24日目の記事です．
Brewfileを使えば，Bundlerでrubygemsを管理するようにHomebrewのパッケージを管理できる．Brewfileのあるディレクトリで
$ brew bundle  とすれば，Brewfileに書かれたパッケージがすべてインストールされる．これはHomebrew公式のコマンドであり，特別なインストール等は必要なく，最新版にアップデートすればすぐに使うことができる．
これを使えば，dotfilesに加えて自分のbrewパッケージを管理しておくこともできるし（tcnksm/dotfiles/Brewfile），imagemagickのようにプロジェクトで必要になるパッケージをBrewfileとして共有しておくこともできる．自分は，Boxenをやめてこちらに切り替えた．
Brewfileの基本の文法は，以下のようにinstall ...とインストールしたいパッケージを記述するだけ．brew updateやbrew cleanといったコマンドも記述できる．
# Homebrewを最新版にアップデート update # Formulaを更新 upgrade # パッケージのインストール install zsh install git install install coreutils # 不要なファイルを削除 clean  とするだけ．Brewfileに書かれたHomebrewパッケージがインストールされる．
公式以外のレポジトリを追加してインストールすることも可能．以下のように記述する．
tap homebrew/binary install packer  (追記) 以前は，同じレポジトリに対してbrew tapを実行すると，Errorとなり実行が停止してしまったため，原始的記述が必要だったが，@sonotsさんのpullreqでWarningに変更となった(https://github.com/Homebrew/homebrew/pull/25617)．ありがとうございます．
また，レポジトリに存在しないAppをダウンロードする際は，自分でFormulaを書いたレポジトリを準備して，それをtapすればよい．詳しくは，こちら．
Google ChromeやVagrantといったdmgでの配布Appも，homebrew-caskを使えば，Brewでインストールできるようになるが，それもBrewfileに記述することができる．
# homebrew-caskのインストール tap phinze/homebrew-cask install brew-cask # インストール cask install google-chrome cask install kobito cask install virtualbox cask install vagrant  参考
 Homebrew&amp;rsquo;s new feature: Brewfiles The Homebrew Brewfile - Rob and Lauren  </description>
    </item>
    
    <item>
      <title>HamlでAngular.js</title>
      <link>https://deeeet.com/writing/2013/12/23/haml-angular-js/</link>
      <pubDate>Mon, 23 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/23/haml-angular-js/</guid>
      <description>この記事はAngularJS Startup Advent Calendar 2013の23日目の記事です．
いきなりですが，参加させていただきました．文脈が全然違ったらすいません．とてもシンプルな記事です．
最近，Angular.jsを触っている．ただ，もう今更HTMLを0から打つのはやってられないと思う．Hamlで手軽に書く．
インストール
$ gem install haml  index.haml
!!! 5 %html{&amp;quot;ng-app&amp;quot; =&amp;gt; true} %head %title Sample %meta{charset: &#39;utf-8&#39;} %script{src: &amp;quot;http://ajax.googleapis.com/ajax/libs/angularjs/1.0.3/angular.js&amp;quot;} %body %input{type: &amp;quot;text&amp;quot;, &amp;quot;ng-model&amp;quot; =&amp;gt; &amp;quot;name&amp;quot;} %p Hello, {{ name }}  htmlに整形
$ haml index.haml index.html  index.html
&amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html ng-app&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Sample&amp;lt;/title&amp;gt; &amp;lt;meta charset=&#39;utf-8&#39;&amp;gt; &amp;lt;script src=&#39;http://ajax.googleapis.com/ajax/libs/angularjs/1.0.3/angular.js&#39;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;input ng-model=&#39;name&#39; type=&#39;text&#39;&amp;gt; &amp;lt;p&amp;gt;Hello, {{ name }}&amp;lt;/p&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt;  以上．</description>
    </item>
    
    <item>
      <title>Dockerのイメージはどこにある?</title>
      <link>https://deeeet.com/writing/2013/12/16/where-are-docker-images-storede/</link>
      <pubDate>Mon, 16 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/16/where-are-docker-images-storede/</guid>
      <description>Where are Docker images stored?
非常にわかりやすいまとめ．ただ，自分の環境とはディレクトリ構造などが若干異なった (バージョンが異なる?) ので，自分で手を動かしながらまとめなおしてみた．
今回用いるDockerのバージョンは以下．
$ docker -v Docker version 0.7.2  ubuntuレポジトリを取得する
$ docker pull ubuntu  $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE ubuntu 12.04 8dbd9e392a96 8 months ago 128 MB ubuntu latest 8dbd9e392a96 8 months ago 128 MB ubuntu precise 8dbd9e392a96 8 months ago 128 MB ubuntu 12.10 b750fe79269d 8 months ago 175.3 MB ubuntu quantal b750fe79269d 8 months ago 175.</description>
    </item>
    
    <item>
      <title>Dockerで継続的インテグレーション</title>
      <link>https://deeeet.com/writing/2013/12/13/ci-with-docker/</link>
      <pubDate>Fri, 13 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/13/ci-with-docker/</guid>
      <description>Dockerで複数バージョンのrubyがインストールされたイメージを作るを使って，ローカルでTravis CI的なビルドテストを実現する方法を書く．
準備 (OS X) Vagrantを使う．バージョン1.4からはDockerのprovisioningに対応してるのでそれを使う． Download Vagrant - Vagrantより.dmgをダウンロードしてきてインストール.
インストールしたら，rubyプロジェクトに移動して以下を実行する．
vagrant init precise64 http://files.vagrantup.com/precise64.box  Vagrantfileを以下のように編集する．ここでは，docker-rbenvで作成した，複数バージョンのruby (1.8,7と1.9.3，2.0.0)とそれぞれにbundlerがインストールされたDockerイメージtcnksm/rbenv-rubygemsを用いる．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.provision :docker do |d| d.pull_images &amp;quot;tcnksm/rbenv&amp;quot; end end  あらかじめ仮想サーバを起動しておく．
vagrant up  Dockerの実行は仮想サーバーへのssh経由で行う．
vagrant ssh-config --host docker-host &amp;gt;&amp;gt; ~/.ssh/config  (注: Vagrant1.4のバグでsshの設定以外の出力をすることがあるので，適宜~/.ssh/configを編集してそれを消す)
実行したいテストの記述 プロジェクトのルートに実行したいテストをシェルスクリプトで記述する．
# docker.sh for v in 1.8.7-p371 1.9.3-p392 2.0.0-p353 do rbenv global $v bundle rspec done  記述しているのはバージョンをそれぞれ1.8.7，1.9.3，2.0.0と切り替えて，それぞれに対してrubygemsをインストールして，rspecテストを実行している．(毎回bunldeを実行するのがたるい場合は，あらかじめbundleを実行してそれをコミットしてイメージを作ってしまえばよい，例えば，Using Docker and Vagrant on Mac OS X with a Ruby on Rails applicationも同様のことをしている)</description>
    </item>
    
    <item>
      <title>Docker image with multiple versions of ruby</title>
      <link>https://deeeet.com/writing/2013/12/12/docker-rbenv-en/</link>
      <pubDate>Thu, 12 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/12/docker-rbenv-en/</guid>
      <description>tcnksm/docker-rbenv
This can generate Docker image which is installed multiple versions of ruby by rbenv.
The image is pushed at docker.io, tcnksm/rbenv, so you can use it soon.
$ docker pull tcnksm/rbenv  or in Dockerfile,
FROM tcnksm/rbenv  Dockerfile I will describe this Dockerfile and how to edit it for your own image.
FROM base MAINTAINER tcnksm &amp;quot;https://github.com/tcnksm&amp;quot; # Install packages for building ruby RUN apt-get update RUN apt-get install -y --force-yes build-essential curl git RUN apt-get install -y --force-yes zlib1g-dev libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt-dev RUN apt-get clean # Install rbenv and ruby-build RUN git clone https://github.</description>
    </item>
    
    <item>
      <title>Dockerで複数バージョンのrubyがインストールされたイメージを作る</title>
      <link>https://deeeet.com/writing/2013/12/12/docker-rbenv/</link>
      <pubDate>Thu, 12 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/12/docker-rbenv/</guid>
      <description>tcnksm/docker-rbenv
これにより，rbenvにより複数バージョンのrubyがインストールされたイメージをつくることができる．
イメージはdocker.ioに置いてある（tcnksm/rbenv）ためすぐに使うことができる．
$ docker pull tcnksm/rbenv  もしくはDockerfileで
FROM tcnksm/rbenv  とするだけ．
具体的な使い方は，Dockerで継続的インテグレーションに書いた．例えば，guardと連携して，複数バージョンに対するrspecテストをローカルで実現するなど．
Dockerfile 以下では，このイメージを作成するためのDockerfileの詳細な説明とオリジナルのイメージを作成する方法について書く．Dockerfileは以下．
FROM base MAINTAINER tcnksm &amp;quot;https://github.com/tcnksm&amp;quot; # Install packages for building ruby RUN apt-get update RUN apt-get install -y --force-yes build-essential curl git RUN apt-get install -y --force-yes zlib1g-dev libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt-dev RUN apt-get clean # Install rbenv and ruby-build RUN git clone https://github.com/sstephenson/rbenv.git /root/.rbenv RUN git clone https://github.com/sstephenson/ruby-build.git /root/.rbenv/plugins/ruby-build RUN ./root/.rbenv/plugins/ruby-build/install.sh ENV PATH /root/.</description>
    </item>
    
    <item>
      <title>GoをWindows実行形式でコンパイル</title>
      <link>https://deeeet.com/writing/2013/12/12/go-build-for-windows/</link>
      <pubDate>Thu, 12 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/12/go-build-for-windows/</guid>
      <description>I don&amp;rsquo;t know why still we need to compile it for windows.
GOOS=windows GOARCH=386 go build -o hello.exe hello.go  参考:
 on Mac, Goはクロスコンパイルが簡単 - unknownplace.org on Linux, Building windows go programs on linux  </description>
    </item>
    
    <item>
      <title>10秒でDockerを起動する</title>
      <link>https://deeeet.com/writing/2013/12/09/fatest-docker-iso/</link>
      <pubDate>Mon, 09 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/09/fatest-docker-iso/</guid>
      <description>boot2docker
boot2dockerはDockerを実行することに特化した軽量版のLinuxディストリビューション．Tiny Core Linuxをベースにしている．isoイメージ自体もDockerから作られている．デモを観るとその起動速度がわかる．現在バージョン0.3．
wget https://github.com/steeve/boot2docker/releases/download/v0.3.0/boot2docker.iso  あとはVirtualboxなどから起動すればすぐにDockerを使える．
めちゃめちゃ軽量なので，普段使っているパッケージがなかったり，そもそもそのパッケージを入れるのが大変だったりする．今のところDockerのコマンドを試したりするに良さそう．
参考
 Boot Docker in 10 seconds on any VM or physical machine with this 30 MB ISO Tiny Core Linux, Micro Core Linux, 12MB Linux GUI Desktop, Live, Frugal, Extendable Install TinyCore Linux on VirtualBox  </description>
    </item>
    
    <item>
      <title>Docker cheat sheet with examples</title>
      <link>https://deeeet.com/writing/2013/12/08/docker-cheat-with-exmaple/</link>
      <pubDate>Sun, 08 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/08/docker-cheat-with-exmaple/</guid>
      <description>Docker Cheat Sheet is a nice documentation. It provides us Docker basic commands and system and It&amp;rsquo;s easy to understand. But there are less exaples, I reconstructed it with real examples. You should refer above document about installation.
Set up Pull a base image.
docker pull ubuntu  It&amp;rsquo;s annoy to restore Container ID, you may forget to restore. You can set below alias. With this, you can get the ID of the last-run Container (15 Docker tips in 5 minutes)</description>
    </item>
    
    <item>
      <title>すぐにDockerを試したい人のための基礎コマンド</title>
      <link>https://deeeet.com/writing/2013/12/08/docker-cheat/</link>
      <pubDate>Sun, 08 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/12/08/docker-cheat/</guid>
      <description>Docker 虎の巻
Dockerの基礎のまとめが良かったので翻訳してみた．原典は，Docker Cheat Sheet．このまとめは説明は十分にあるが，例がほとんどない．実例を使って，コンテナとイメージに関する基礎コマンドをまとめてみる．
OS X で試したい Vagrantを使う．バージョン1.4からはDockerのprovisioningに対応してるのでそれを使う． Download Vagrant - Vagrantより.dmgをダウンロードしてきてインストール.
vagrant init precise64 http://files.vagrantup.com/precise64.box  Vagrantfileを以下のようにすれば，すぐにDockerを使える．
Vagrant.configure(&amp;quot;2&amp;quot;) do |config| config.vm.box = &amp;quot;precise64&amp;quot; config.vm.provision :docker do |d| d.pull_images &amp;quot;ubuntu&amp;quot; end end  ログイン
vagrant ssh  準備 コンテナのIDをいちいち保持しておくのは面倒，忘れるので，以下のaliasを設定しておくと直近に起動したコンテナのIDを呼び出すことができるようになる（15 Docker tips in 5 minutes）．
alias dl=&#39;docker ps -l -q&#39;  コンテナ コンテナを作成する．-dオプションでバックグラウンドで実行する．
docker run -d ubuntu /bin/sh -c &amp;quot;while true; do echo hello world; sleep 1; done&amp;quot;  コンテナを停止する．</description>
    </item>
    
    <item>
      <title>Set up ruby test environment by Vagrant and Chef</title>
      <link>https://deeeet.com/writing/2013/11/20/vagrant-chef-ruby/</link>
      <pubDate>Wed, 20 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/11/20/vagrant-chef-ruby/</guid>
      <description>chefを使ってVagrantのVM上にrbenvによる複数バージョンのrubyがインストールされた環境をつくる，tcnksm/vagrant-chef-ruby．
まず，Vagrantによる試験環境の構築．今回はUbuntu Precise 12.04 (64 bit)を使う．
vagrant init precise64 http://files.vagrantup.com/precise64.box  Vagrantのプラグインであるvagrant-omnibusを使えば，VMを立ち上げるときに，chefがなければ自動でインストールをしてくれる．vagrant plugin install vagrant-omnibusでインストールし，Vagrantfileを以下のように記述．
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| ... # vagrant-omnibus (setup chef-environment in vm) config.omnibus.chef_version = &amp;quot;11.4.0&amp;quot; ... end  次に，chefによるrubyのインストール．rubyのインストールレシピは他でも使いたいのでcookbookを作成した，tcnksm/chef-rubies．Berksfileに今回作成したcookbookを指定する．
cookbook &#39;rubies&#39;, :git =&amp;gt; &#39;https://github.com/tcnksm/chef-rubies&#39;  後は，berks install --path cookbooksでこれをインストールして，以下でVMに対してchefを実行すれば，ruby1.8.7，1.9.3，2.0.0がインストールされる．
knife solo cook -c config/solo.rb host  </description>
    </item>
    
    <item>
      <title>How to test ActionMailer</title>
      <link>https://deeeet.com/writing/2013/11/07/rspec-actionmailer/</link>
      <pubDate>Thu, 07 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/11/07/rspec-actionmailer/</guid>
      <description>Railsに付属のactionmailer．Railsプロジェクトではなく，単独でも使うことができる．erbテンプレートが使えたり，htmlメールが送れるため，簡単なバッチをつくるときによく利用する．最小限の利用サンプルはこちら．
テストできるのは，メール送信数と送信先，送信元，件名，本文．まず，spec_helperの設定．
ActionMailer::Base.delivery_method = :test ActionMailer::Base.perform_deliveries = true  送信数．意図した数のメールが送られているか．
it &amp;quot;sends an mail&amp;quot; do expect(ActionMailer::Base.deliveries.count).to eq(1) end  送信先．意図したアドレスに配信されたか．
it &amp;quot;renders the receiver mail&amp;quot; do expect(ActionMailer::Base.deliveries.first.to).to eq([&amp;quot;test@mail.net&amp;quot;]) end  送信元．意図したアドレスから配信されているか．
it &amp;quot;renders the sender mail&amp;quot; do expect(ActionMailer::Base.deliveries.first.from).to eq([&amp;quot;sender@mail.net&amp;quot;]) end  件名．意図した件名で配信されているか．
it &amp;quot;set the success subject&amp;quot; do expect(ActionMailer::Base.deliveries.first.subject).to match(/[Success]/) end  本文．意図した本文で配信されているか．
it &amp;quot;sends the hello body&amp;quot; do expect(ActionMailer::Base.deliveries.first.body).to match(/Hello, #{user}./) end  今回のテストはすべてここにまとめてある．</description>
    </item>
    
    <item>
      <title>ChefでOS Xの環境セットアップする</title>
      <link>https://deeeet.com/writing/2013/10/16/chef-dev-env/</link>
      <pubDate>Wed, 16 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/10/16/chef-dev-env/</guid>
      <description>tcnksm/chef-dev-env をつくった．以下のことができる．
 Homebrewパッケージのインストール dmgパッケージのインストール rubygemsのインストール dotfilesのインストールとセットアップ  OS X特有のレシピだとDockの設定とかできそうだけど，得にデフォで問題ないからやってない． ここまできたら，AppStoreからのダウンロードもできたらなと思う．
インストール対象のパッケージはdata_bugsを使うといろいろ分けてすっきり書けた． 作業してて，これ使うなーと思ったらここにどんどん貯めていく感じで．
#data_bags/packages/homebrew.json { &amp;quot;id&amp;quot;: &amp;quot;homebrew&amp;quot;, &amp;quot;targets&amp;quot;: [ &amp;quot;tig&amp;quot;, &amp;quot;coreutils&amp;quot;, &amp;quot;go&amp;quot;, .... ] }  で，後は以下のようにrecipeから呼び出して一気にインストール．
item = data_bag_item(:packages, &amp;quot;homebrew&amp;quot;)  Ubuntuでの開発もやったりするからnode追加して，Ubuntu用も作る予定．
Chefの勉強のために始めた．こうやって自分の環境をコード的に作っていくのすごい楽しかった．
Chefの基礎と各ディレクトリの役割とかは&amp;ldquo;入門Chef Solo - Infrastructure as Code&amp;rdquo;を読んでほとんど理解できた．ChefをつかったOS Xのセットアップに関しては以下を参考にした．
 Managing My Workstations With Chef pivotal-sprout/sprout dann/chef-macbox  </description>
    </item>
    
    <item>
      <title>Octopressのデザインテーマの作り方</title>
      <link>https://deeeet.com/writing/2013/10/09/create-octpress-design/</link>
      <pubDate>Wed, 09 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2013/10/09/create-octpress-design/</guid>
      <description>ブログのデザインを一新した． 良い機会なので3rd party Octopressテーマとして公開した．
デザインテーマはrake install[THEME]でインストールできる． このrakeのタスクがやっているのは単純で, ただsassとsourceを置き換えているだけ．
task :install, :theme do |t, args| cp_r &amp;quot;#{themes_dir}/#{theme}/source/.&amp;quot;, source_dir cp_r &amp;quot;#{themes_dir}/#{theme}/sass/.&amp;quot;, &amp;quot;sass&amp;quot; end  作り方としては，
 デフォルトのclassicや3rd Party Octopress Themesから気に入ったものをインストール rake previewでローカルで表示しつつsassとsourceを編集 完成したら，.theme/YOUR_THEMEにsassと_postを抜いたsourceをコピー  とすればよい．簡単．</description>
    </item>
    
    <item>
      <title>Best Music 2012</title>
      <link>https://deeeet.com/writing/2012/12/31/music-2012/</link>
      <pubDate>Mon, 31 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://deeeet.com/writing/2012/12/31/music-2012/</guid>
      <description>I met many nice musics in this year, I selected my best in this year.
 Flying Lotus &amp;ldquo;Until the Quiet Comes&amp;rdquo; Chilly Gonzales &amp;ldquo;Solo Piano II&amp;rdquo; Lapalax &amp;ldquo;When you&amp;rsquo;re gone&amp;rdquo; Einar Stray &amp;ldquo;Chiaroscuro&amp;rdquo; Squarepusher &amp;ldquo;Ufabulum&amp;rdquo; Bibio &amp;ldquo;Ambivalence Avenue&amp;rdquo; Tropics &amp;ldquo;Parodia Flare&amp;rdquo; TOKiMONSTA &amp;ldquo;Creature Dreams&amp;rdquo; Brian Eno &amp;ldquo;Lux&amp;rdquo; 宮内優里 &amp;ldquo;ワーキングホリデー&amp;rdquo;  </description>
    </item>
    
  </channel>
</rss>